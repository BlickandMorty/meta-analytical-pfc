# Resume Content — Meta-Analytical PFC Project

Use whichever combination fits your layout. Options below range from a tight 3-liner to a full project block.

---

## OPTION A: Profile / Summary Section (top of resume)

> AI safety engineer and independent researcher building biomimetic cognitive architectures for self-correcting AI. Designed and implemented a 10-subsystem executive reasoning layer—integrating DerSimonian-Laird meta-analysis, persistent homology on activation manifolds, conjugate Bayesian updating, and Bradford Hill causal inference—that wraps large language models in a prefrontal-cortex-inspired orchestration pipeline. Work spans the intersection of AI safety (continuous allostatic threat modulation), research methodology (automated meta-analytical rigor with publication bias detection), and inference optimization (entropy-responsive depth control via continued-fraction scaling). Seeking roles in AI research and optimization where mathematical rigor and systems-level thinking drive real capability gains.

---

## OPTION B: Shorter Profile (2–3 lines)

> AI safety engineer and independent researcher. Designed a biomimetic cognitive architecture that instruments LLM inference with 10 mathematical frameworks—persistent homology, meta-analysis, Bayesian updating, causal DAGs, and prime-encoded concept harmonics—to produce calibrated, self-correcting, safety-aware reasoning. Bridging AI safety, research methodology, and inference optimization.

---

## OPTION C: Project Block (under "Projects" or "Research")

**Meta-Analytical Prefrontal Cortex (PFC)** — *Independent Research / Open Source*
Biomimetic cognitive architecture that wraps LLMs in an executive reasoning system modeled on the human prefrontal cortex.

- Designed and implemented a 10-stage executive pipeline integrating statistical analysis (Cohen's d, power analysis, MCID thresholds), causal inference (DAGs + Bradford Hill criteria), DerSimonian-Laird random-effects meta-analysis with Egger's publication bias detection, and conjugate Bayesian updating with prior sensitivity analysis
- Built a persistent homology pipeline (Ripser) that computes Betti numbers and persistence entropy on transformer activation manifolds, feeding topological signals into real-time inference control
- Engineered a continued-fraction focus controller that dynamically modulates temperature and token depth based on activation entropy and concept dissonance—an entropy valve that allocates compute proportional to problem complexity
- Implemented Leibnizian prime-encoded concept harmonics using the Fundamental Theorem of Arithmetic for guaranteed-unique, lossless concept combination tracking with dissonance detection
- Designed the Contextual Allostasis Engine (CAE): an embedding-based safety system using cosine similarity threat anchors with exponential smoothing, implementing graded autonomic response (GREEN/YELLOW/RED) rather than binary filtering
- Built a confidence calibrator that inverts the default LLM certainty assumption—starting from 0.5 (maximum ignorance) and earning confidence only through statistical power, causal strength, and surviving adversarial self-critique
- Implemented a meta-learning loop using TF-IDF vectorization and DBSCAN clustering to detect recurring skill gaps from executive reasoning traces, generating targeted training examples for self-improvement

---

## OPTION D: Bullet Points (pick 4–6 for a tighter layout)

- Designed a biomimetic AI orchestration layer implementing 10 mathematical frameworks (meta-analysis, Bayesian inference, persistent homology, causal inference, continued fractions) as an executive reasoning system for LLMs
- Built a persistent homology pipeline computing Betti numbers and persistence entropy on transformer activation manifolds via Ripser, using topological invariants as real-time inference control signals
- Engineered a continued-fraction focus controller—an entropy valve that dynamically scales temperature and reasoning depth based on activation complexity, achieving compute allocation proportional to problem difficulty
- Implemented DerSimonian-Laird random-effects meta-analysis with Egger's publication bias test, I² heterogeneity quantification, and leave-one-out sensitivity analysis for automated research-grade evidence synthesis
- Designed a graded safety system (Contextual Allostasis Engine) using embedding-based threat detection with exponential smoothing, modeling AI safety as continuous autonomic modulation rather than binary content filtering
- Built a confidence calibration system that inverts default LLM certainty—starting from maximum ignorance (0.5) and requiring statistical power, causal grounding, and surviving structured self-critique to increase
- Applied Leibniz's Fundamental Theorem of Arithmetic to encode concept combinations as unique prime products, enabling lossless constant-space dissonance detection across active reasoning concepts

---

## OPTION E: Skills / Technologies Line

**Frameworks & Methods:** Persistent Homology (Ripser), DerSimonian-Laird Meta-Analysis, Conjugate Bayesian Inference, Bradford Hill Causal Inference, TF-IDF/DBSCAN Clustering, Continued-Fraction Optimization, Topological Data Analysis, Exponential Smoothing, Cosine Similarity Embeddings

**Tools:** Python, NumPy, SciPy, scikit-learn, NetworkX, ChromaDB, SentenceTransformers, HuggingFace Transformers (4-bit quantization), Anthropic API, FastAPI, Ripser, PCA

---

## OPTION F: One-Liner (for tight resume headers or LinkedIn headline)

> AI Safety Engineer | Independent Researcher — Biomimetic cognitive architectures, persistent homology on activation manifolds, meta-analytical reasoning systems

---

## Talking Points for Interviews

If someone asks "tell me about this project," here's the arc:

1. **The problem:** LLMs are powerful but epistemically uncalibrated—they express high confidence on claims they can't ground, they don't detect contradictions in their own reasoning, and they allocate the same compute to trivial and complex problems.

2. **The insight:** The human prefrontal cortex already solves this. It triages complexity, modulates attention depth, detects dissonance, runs counterfactuals, and calibrates certainty against evidence strength. These aren't philosophical luxuries—they're computable operations.

3. **What I built:** An orchestration layer that implements these operations mathematically. Ten subsystems—persistent homology on activation manifolds, continued-fraction depth control, prime-encoded concept harmonics, DerSimonian-Laird meta-analysis, conjugate Bayesian updating, Bradford Hill causal inference, adversarial self-critique, exponential-smoothing safety, TF-IDF/DBSCAN meta-learning, and precision-weighted confidence calibration—composed into a single executive pipeline.

4. **The safety angle:** Most AI safety is binary—block or allow. The Contextual Allostasis Engine models safety as continuous autonomic modulation, inspired by biological allostasis. The system doesn't just filter harmful content; it *changes how it thinks* under threat—lowering temperature, increasing constraint, injecting caution—the way the human nervous system shifts from parasympathetic to sympathetic under stress.

5. **The research angle:** The system automates research-grade analytical practices that currently take systematic reviewers months—meta-analytical pooling, publication bias detection, heterogeneity quantification, causal scoring, sensitivity analysis—and applies them in real-time during inference.

6. **The optimization angle:** The continued-fraction focus controller is an entropy valve. It allocates compute proportional to problem complexity. Simple queries get fast, shallow processing. Complex queries get deep, constrained reasoning. The system *breathes*—expanding and contracting its cognitive effort exactly as human attention does.

7. **The big idea:** We've been scaling AI wrong—more parameters, same architecture. The transformer is the neocortex. I gave it a prefrontal cortex.
