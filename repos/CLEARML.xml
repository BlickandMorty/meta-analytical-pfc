This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
apiserver/__init__.py
apiserver/apierrors_generator/__init__.py
apiserver/apierrors_generator/__main__.py
apiserver/apierrors_generator/errors_generator.py
apiserver/apierrors_generator/generator.py
apiserver/apierrors_generator/templates/error.jinja2
apiserver/apierrors_generator/templates/init.jinja2
apiserver/apierrors_generator/templates/section.jinja2
apiserver/apierrors/__init__.py
apiserver/apierrors/apierror.py
apiserver/apierrors/base.py
apiserver/apierrors/errors.conf
apiserver/apimodels/__init__.py
apiserver/apimodels/auth.py
apiserver/apimodels/base.py
apiserver/apimodels/batch.py
apiserver/apimodels/custom_validators/__init__.py
apiserver/apimodels/events.py
apiserver/apimodels/login.py
apiserver/apimodels/metadata.py
apiserver/apimodels/models.py
apiserver/apimodels/organization.py
apiserver/apimodels/pipelines.py
apiserver/apimodels/projects.py
apiserver/apimodels/queues.py
apiserver/apimodels/reports.py
apiserver/apimodels/server.py
apiserver/apimodels/serving.py
apiserver/apimodels/storage.py
apiserver/apimodels/tasks.py
apiserver/apimodels/users.py
apiserver/apimodels/workers.py
apiserver/bll/__init__.py
apiserver/bll/auth/__init__.py
apiserver/bll/event/__init__.py
apiserver/bll/event/event_bll.py
apiserver/bll/event/event_common.py
apiserver/bll/event/event_metrics.py
apiserver/bll/event/events_iterator.py
apiserver/bll/event/history_debug_image_iterator.py
apiserver/bll/event/history_plots_iterator.py
apiserver/bll/event/metric_debug_images_iterator.py
apiserver/bll/event/metric_events_iterator.py
apiserver/bll/event/metric_plots_iterator.py
apiserver/bll/event/scalar_key.py
apiserver/bll/model/__init__.py
apiserver/bll/model/metadata.py
apiserver/bll/organization/__init__.py
apiserver/bll/organization/tags_cache.py
apiserver/bll/project/__init__.py
apiserver/bll/project/project_bll.py
apiserver/bll/project/project_cleanup.py
apiserver/bll/project/project_queries.py
apiserver/bll/project/project_usages.py
apiserver/bll/project/sub_projects.py
apiserver/bll/query/__init__.py
apiserver/bll/query/builder.py
apiserver/bll/queue/__init__.py
apiserver/bll/queue/queue_bll.py
apiserver/bll/queue/queue_metrics.py
apiserver/bll/redis_cache_manager.py
apiserver/bll/serving/__init__.py
apiserver/bll/serving/stats.py
apiserver/bll/statistics/resource_monitor.py
apiserver/bll/statistics/stats_reporter.py
apiserver/bll/storage/__init__.py
apiserver/bll/task/__init__.py
apiserver/bll/task/artifacts.py
apiserver/bll/task/hyperparams.py
apiserver/bll/task/non_responsive_tasks_watchdog.py
apiserver/bll/task/param_utils.py
apiserver/bll/task/task_bll.py
apiserver/bll/task/task_cleanup.py
apiserver/bll/task/task_operations.py
apiserver/bll/task/utils.py
apiserver/bll/user/__init__.py
apiserver/bll/util.py
apiserver/bll/workers/__init__.py
apiserver/bll/workers/stats.py
apiserver/config_repo.py
apiserver/config/__init__.py
apiserver/config/basic.py
apiserver/config/default/apiserver.conf
apiserver/config/default/hosts.conf
apiserver/config/default/logging.conf
apiserver/config/default/secure.conf
apiserver/config/default/services/_mongo.conf
apiserver/config/default/services/async_urls_delete.conf
apiserver/config/default/services/auth.conf
apiserver/config/default/services/events.conf
apiserver/config/default/services/models.conf
apiserver/config/default/services/organization.conf
apiserver/config/default/services/projects.conf
apiserver/config/default/services/queues.conf
apiserver/config/default/services/serving.conf
apiserver/config/default/services/storage_credentials.conf
apiserver/config/default/services/tasks.conf
apiserver/config/default/services/workers.conf
apiserver/config/info.py
apiserver/database/__init__.py
apiserver/database/defs.py
apiserver/database/errors.py
apiserver/database/fields.py
apiserver/database/model/__init__.py
apiserver/database/model/auth.py
apiserver/database/model/base.py
apiserver/database/model/company.py
apiserver/database/model/metadata.py
apiserver/database/model/model_labels.py
apiserver/database/model/model.py
apiserver/database/model/project.py
apiserver/database/model/queue.py
apiserver/database/model/settings.py
apiserver/database/model/storage_settings.py
apiserver/database/model/task/metrics.py
apiserver/database/model/task/output.py
apiserver/database/model/task/task.py
apiserver/database/model/url_to_delete.py
apiserver/database/model/user.py
apiserver/database/model/version.py
apiserver/database/projection.py
apiserver/database/props.py
apiserver/database/query.py
apiserver/database/utils.py
apiserver/documentation/api_versions.md
apiserver/elastic/apply_mappings.py
apiserver/elastic/index_templates/events/component_templates/events_common.json
apiserver/elastic/index_templates/events/events_log.json
apiserver/elastic/index_templates/events/events_plot.json
apiserver/elastic/index_templates/events/events_training_debug_image.json
apiserver/elastic/index_templates/events/events_training_stats_scalar.json
apiserver/elastic/index_templates/workers/queue_metrics.json
apiserver/elastic/index_templates/workers/serving_stats.json
apiserver/elastic/index_templates/workers/worker_stats.json
apiserver/elastic/initialize.py
apiserver/elastic/mappings/events/events_log.json
apiserver/elastic/mappings/events/events_plot.json
apiserver/elastic/mappings/events/events_training_debug_image.json
apiserver/elastic/mappings/events/events.json
apiserver/elastic/mappings/workers/queue_metrics.json
apiserver/elastic/mappings/workers/worker_stats.json
apiserver/elastic/requirements.txt
apiserver/es_factory.py
apiserver/fix_mongo_urls.py
apiserver/jobs/async_urls_delete.py
apiserver/LICENSE
apiserver/mongo/initialize/__init__.py
apiserver/mongo/initialize/migration.py
apiserver/mongo/initialize/pre_populate.py
apiserver/mongo/initialize/user.py
apiserver/mongo/initialize/util.py
apiserver/mongo/migrations/0_12_1.py
apiserver/mongo/migrations/0_13_0.py
apiserver/mongo/migrations/0_14_0.py
apiserver/mongo/migrations/0_15_0.py
apiserver/mongo/migrations/0_16_0.py
apiserver/mongo/migrations/0_16_1.py
apiserver/mongo/migrations/0_16_2.py
apiserver/mongo/migrations/0_17_0.py
apiserver/mongo/migrations/1_0_0.py
apiserver/mongo/migrations/1_0_2.py
apiserver/mongo/migrations/1_3_0.py
apiserver/mongo/migrations/1_6_0.py
apiserver/mongo/migrations/1_7_0.py
apiserver/mongo/migrations/1_9_0.py
apiserver/mongo/migrations/utils.py
apiserver/redis_manager.py
apiserver/requirements.txt
apiserver/schema/__init__.py
apiserver/schema/meta/__init__.py
apiserver/schema/meta/meta.conf
apiserver/schema/meta/validate.py
apiserver/schema/schema_reader.py
apiserver/schema/services/_api_defaults.conf
apiserver/schema/services/_common.conf
apiserver/schema/services/_events_common.conf
apiserver/schema/services/_tasks_common.conf
apiserver/schema/services/_workers_common.conf
apiserver/schema/services/auth.conf
apiserver/schema/services/debug.conf
apiserver/schema/services/events.conf
apiserver/schema/services/login.conf
apiserver/schema/services/models.conf
apiserver/schema/services/organization.conf
apiserver/schema/services/pipelines.conf
apiserver/schema/services/projects.conf
apiserver/schema/services/queues.conf
apiserver/schema/services/README.md
apiserver/schema/services/reports.conf
apiserver/schema/services/server.conf
apiserver/schema/services/serving.conf
apiserver/schema/services/storage.conf
apiserver/schema/services/tasks.conf
apiserver/schema/services/users.conf
apiserver/schema/services/workers.conf
apiserver/server_init/app_sequence.py
apiserver/server_init/request_handlers.py
apiserver/server.py
apiserver/service_repo/__init__.py
apiserver/service_repo/apicall.py
apiserver/service_repo/auth/__init__.py
apiserver/service_repo/auth/auth.py
apiserver/service_repo/auth/dictable.py
apiserver/service_repo/auth/fixed_user.py
apiserver/service_repo/auth/identity.py
apiserver/service_repo/auth/payload/__init__.py
apiserver/service_repo/auth/payload/auth_type.py
apiserver/service_repo/auth/payload/basic.py
apiserver/service_repo/auth/payload/payload.py
apiserver/service_repo/auth/payload/token.py
apiserver/service_repo/auth/utils.py
apiserver/service_repo/endpoint.py
apiserver/service_repo/errors.py
apiserver/service_repo/schema_validator.py
apiserver/service_repo/service_repo.py
apiserver/service_repo/util.py
apiserver/service_repo/validators.py
apiserver/services_schema.py
apiserver/services/__init__.py
apiserver/services/auth.py
apiserver/services/debug.py
apiserver/services/events.py
apiserver/services/login/__init__.py
apiserver/services/models.py
apiserver/services/organization.py
apiserver/services/pipelines.py
apiserver/services/projects.py
apiserver/services/queues.py
apiserver/services/reports.py
apiserver/services/server/__init__.py
apiserver/services/serving.py
apiserver/services/storage.py
apiserver/services/tasks.py
apiserver/services/users.py
apiserver/services/utils.py
apiserver/services/workers.py
apiserver/sync.py
apiserver/tests/__init__.py
apiserver/tests/api_client.py
apiserver/tests/automated/__init__.py
apiserver/tests/automated/test_batch_operations.py
apiserver/tests/automated/test_entity_ordering.py
apiserver/tests/automated/test_get_all_ex_filters.py
apiserver/tests/automated/test_models.py
apiserver/tests/automated/test_move_under_project.py
apiserver/tests/automated/test_organization.py
apiserver/tests/automated/test_paging_and_scrolling.py
apiserver/tests/automated/test_pipelines.py
apiserver/tests/automated/test_project_delete.py
apiserver/tests/automated/test_project_tags.py
apiserver/tests/automated/test_project_usages.py
apiserver/tests/automated/test_projection.py
apiserver/tests/automated/test_projects_edit.py
apiserver/tests/automated/test_queue_model_metadata.py
apiserver/tests/automated/test_queues.py
apiserver/tests/automated/test_reports.py
apiserver/tests/automated/test_serving.py
apiserver/tests/automated/test_subprojects.py
apiserver/tests/automated/test_tags.py
apiserver/tests/automated/test_task_artifacts.py
apiserver/tests/automated/test_task_debug_images.py
apiserver/tests/automated/test_task_events.py
apiserver/tests/automated/test_task_hyperparams.py
apiserver/tests/automated/test_task_models.py
apiserver/tests/automated/test_task_parents.py
apiserver/tests/automated/test_task_plots.py
apiserver/tests/automated/test_tasks_delete.py
apiserver/tests/automated/test_tasks_diff.py
apiserver/tests/automated/test_tasks_edit.py
apiserver/tests/automated/test_tasks_filtering.py
apiserver/tests/automated/test_tasks_running.py
apiserver/tests/automated/test_users.py
apiserver/tests/automated/test_workers.py
apiserver/tests/requirements.txt
apiserver/tools.py
apiserver/updates.py
apiserver/utilities/__init__.py
apiserver/utilities/attrs.py
apiserver/utilities/dicts.py
apiserver/utilities/env.py
apiserver/utilities/json.py
apiserver/utilities/parameter_key_escaper.py
apiserver/utilities/partial_version.py
apiserver/utilities/stringenum.py
apiserver/utilities/threads_manager.py
apiserver/version.py
docker/build/Dockerfile
docker/build/internal_files/build_webapp.sh
docker/build/internal_files/clearml_subpath.conf.template
docker/build/internal_files/clearml.conf.template
docker/build/internal_files/entrypoint.sh
docker/build/internal_files/final_image_preparation.sh
docker/build/internal_files/update_from_env.py
docker/docker-compose-win10.yml
docker/docker-compose.yml
docs/apiserver.conf
docs/ClearML_Server_Diagram.png
docs/clearml_server_logo.png
docs/faq.md
docs/install_aws.md
docs/install_gcp.md
docs/install_linux_mac.md
docs/install_win.md
docs/services.conf
fileserver/auth.py
fileserver/config/__init__.py
fileserver/config/basic.py
fileserver/config/default/fileserver.conf
fileserver/config/default/hosts.conf
fileserver/config/default/logging.conf
fileserver/config/default/secure.conf
fileserver/fileserver.py
fileserver/LICENSE
fileserver/redis_manager.py
fileserver/utils.py
LICENSE
README.md
upgrade/1_17_to_2_0/docker-compose-win10.yml
upgrade/1_17_to_2_0/docker-compose.yml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
syntax: glob
.idea
apierrors/errors
static/build.json
static/dashboard/node_modules
static/webapp/node_modules
static/webapp/.git
*.pyc
__pycache__
.ropeproject
test-reports
.pytest_cache
venv
*.noseids
*.egg-info
.cache
.mypy_cache
dist
code.tar.gz
server/schema/services/_cache.json
server/apierrors/errors/*
</file>

<file path="apiserver/__init__.py">

</file>

<file path="apiserver/apierrors_generator/__init__.py">

</file>

<file path="apiserver/apierrors_generator/__main__.py">

</file>

<file path="apiserver/apierrors_generator/errors_generator.py">
class ErrorsGenerator
⋮----
_apierrors_path = Path(__file__).parents[1] / "apierrors"
_files = [_apierrors_path / "errors.conf"]
⋮----
@classmethod
    def _get_codes(cls)
⋮----
@classmethod
    def add_errors_file(cls, path: Union[Path, str])
⋮----
@classmethod
    def generate_python_files(cls)
</file>

<file path="apiserver/apierrors_generator/generator.py">
env = jinja2.Environment(
⋮----
def env_filter(name=None)
⋮----
@env_filter()
def cls_name(name)
⋮----
delims = list(map(re.escape, (" ", "_")))
parts = re.split("|".join(delims), name)
⋮----
class Generator(object)
⋮----
_base_class_name = "BaseError"
_base_class_module = "apiserver.apierrors.base"
⋮----
def __init__(self, path, format_pep8=True, use_md5=True)
⋮----
def _make_init_file(self, path)
⋮----
def _do_render(self, file, template, context)
⋮----
result = template.render(
⋮----
result = autopep8.fix_code(
⋮----
def _make_section(self, name, code, subcodes)
⋮----
def _make_init(self, sections)
⋮----
def _key_to_str(self, data)
⋮----
def _calc_digest(self, data)
⋮----
data = json.dumps(self._key_to_str(data), sort_keys=True)
⋮----
def make_errors(self, errors)
⋮----
digest = None
digest_file = self._path / "digest.md5"
⋮----
digest = self._calc_digest(errors)
</file>

<file path="apiserver/apierrors_generator/templates/error.jinja2">
{% macro error_class(name, msg, code, subcode=0) %}
class {{ name }}({{ base_class_name }}):
    _default_code = {{ code }}
    _default_subcode = {{ subcode }}
    _default_msg = "{{ msg|capitalize }}"
{% endmacro -%}
</file>

<file path="apiserver/apierrors_generator/templates/init.jinja2">
{% from 'templates/error.jinja2' import error_class with context %}
{% if sections %}
from {{ base_class_module }} import {{ base_class_name }}
{% endif %}

{% for _, name in sections %}
from . import {{ name }}
{% endfor %}


{% for code, name in sections %}
{{ error_class(name|cls_name, name|replace('_', ' '), code) }}

{% endfor %}
</file>

<file path="apiserver/apierrors_generator/templates/section.jinja2">
{% from 'templates/error.jinja2' import error_class with context %}
{% if subcodes %}
from {{ base_class_module }} import {{ base_class_name }}
{% endif %}
{% for subcode, (name, msg) in subcodes %}


{{ error_class(name|cls_name, msg, code, subcode|int) -}}
{% endfor %}
</file>

<file path="apiserver/apierrors/__init__.py">

</file>

<file path="apiserver/apierrors/apierror.py">
class APIError(Exception)
⋮----
def __init__(self, msg, code=500, subcode=0, error_data=None, **_)
⋮----
@property
    def msg(self)
⋮----
@property
    def code(self)
⋮----
@property
    def subcode(self)
⋮----
@property
    def error_data(self)
⋮----
def __str__(self)
</file>

<file path="apiserver/apierrors/base.py">
jsonable_types = (dict, list, tuple, str, int, float, bool, type(None))
⋮----
class BaseError(APIError)
⋮----
_default_code = 500
_default_subcode = 0
_default_msg = ""
⋮----
def __init__(self, extra_msg=None, replacement_msg=None, **kwargs)
⋮----
message = replacement_msg or self._default_msg
⋮----
kwargs_msg = ", ".join(
⋮----
@staticmethod
    def _to_safe_json_types(data)
⋮----
def visit(_, k, v)
⋮----
v = str(v)
⋮----
@staticmethod
    def _format_kwarg(value)
⋮----
@classproperty
    def codes(self) -> Tuple[int, int]
</file>

<file path="apiserver/apierrors/errors.conf">
301 {
    _: "moved_permanently"
    1: ["not_supported", "this endpoint is no longer supported for the requested API version"]
}

400 {
    _: "bad_request"
    1: ["not_supported", "endpoint is not supported"]
    2: ["request_path_has_invalid_version", "request path has invalid version"]
    5: ["invalid_headers", "invalid headers"]
    6: ["impersonation_error", "impersonation error"]

    10: ["invalid_id", "invalid object id"]
    11: ["missing_required_fields", "missing required fields"]
    12: ["validation_error", "validation error"]
    13: ["fields_not_allowed_for_role", "fields not allowed for role"]
    14: ["invalid fields", "fields not defined for object"]
    15: ["fields_conflict", "conflicting fields"]
    16: ["fields_value_error", "invalid value for fields"]
    17: ["batch_contains_no_items", "batch request contains no items"]
    18: ["batch_validation_error", "batch request validation error"]
    19: ["invalid_lucene_syntax", "malformed lucene query"]
    20: ["fields_type_error", "invalid type for fields"]
    21: ["invalid_regex_error", "malformed regular expression"]
    22: ["invalid_email_address", "malformed email address"]
    23: ["invalid_domain_name", "malformed domain name"]
    24: ["not_public_object", "object is not public"]

    # Auth / Login
    75: ["invalid_access_key", "access key not found"]

    # Tasks
    100: ["task_error", "general task error"]
    101: ["invalid_task_id", "invalid task id"]
    102: ["task_validation_error", "task validation error"]
    110: ["invalid_task_status", "invalid task status"]
    111: ["task_not_started", "task not started (invalid task status)"]
    112: ["task_in_progress", "task in progress (invalid task status)"]
    113: ["task_published", "task published (invalid task status)"]
    114: ["task_status_unknown", "task unknown (invalid task status)"]
    120: ["invalid_task_execution_progress", "invalid task execution progress"]
    121: ["failed_changing_task_status", "failed changing task status. probably someone changed it before you"]
    122: ["missing_task_fields", "task is missing expected fields"]
    123: ["task_cannot_be_deleted", "task cannot be deleted"]
    125: ["task_has_jobs_running", "task has jobs that haven't completed yet"]
    126: ["invalid_task_type", "invalid task type for this operations"]
    127: ["invalid_task_input", "invalid task output"]
    128: ["invalid_task_output", "invalid task output"]
    129: ["task_publish_in_progress", "Task publish in progress"]
    130: ["task_not_found", "task not found"]
    131: ["events_not_added", "events not added"]

    # Reports
    150: ["operation_supported_on_reports_only", "passed task is not report"]

    # Pipelines
    160: ["cannot_remove_all_runs", "at least one pipeline run should be left"]

    # Models
    200: ["model_error", "general task error"]
    201: ["invalid_model_id", "invalid model id"]
    202: ["model_not_ready", "model is not ready"]
    203: ["model_is_ready", "model is ready"]
    204: ["invalid_model_uri", "invalid model URI"]
    205: ["model_in_use", "model is used by tasks"]
    206: ["model_creating_task_exists", "task that created this model exists"]

    # Users
    300: ["invalid_user", "invalid user"]
    301: ["invalid_user_id", "invalid user id"]
    302: ["user_id_exists", "user id already exists"]
    305: ["invalid_preferences_update", "Malformed key and/or value"]

    # Projects
    401: ["invalid_project_id", "invalid project id"]
    402: ["project_has_tasks", "project has associated tasks"]
    403: ["project_not_found", "project not found"]
    405: ["project_has_models", "project has associated models"]
    406: ["project_has_datasets", "project has associated non-empty datasets"]
    407: ["invalid_project_name", "invalid project name"]
    408: ["cannot_update_project_location", "Cannot update project location. Use projects.move instead"]
    409: ["project_path_exceeds_max", "Project path exceed the maximum allowed depth"]
    410: ["project_source_and_destination_are_the_same", "Project has the same source and destination paths"]
    411: ["project_cannot_be_moved_under_itself", "Project can not be moved under itself in the projects hierarchy"]
    412: ["project_cannot_be_merged_into_its_child", "Project can not be merged into its own child"]
    413: ["project_has_pipelines", "project has associated pipelines with active controllers"]
    414: ["public_project_exists", "Cannot create project. Public project with the same name already exists"]

    # Queues
    701: ["invalid_queue_id", "invalid queue id"]
    702: ["queue_not_empty", "queue is not empty"]
    703: ["invalid_queue_or_task_not_queued", "invalid queue id or task not in queue"]
    704: ["removed_during_reposition", "task was removed by another party during reposition"]
    705: ["failed_adding_during_reposition", "failed adding task back to queue during reposition"]
    706: ["task_already_queued", "failed adding task to queue since task is already queued"]
    707: ["no_default_queue", "no queue is tagged as the default queue for this company"]
    708: ["multiple_default_queues", "more than one queue is tagged as the default queue for this company"]

    # Database
    800: ["data_validation_error", "data validation error"]
    801: ["expected_unique_data", "value combination already exists (unique field already contains this value)"]

    # Workers
    1001: ["invalid_worker_id", "invalid worker id"]
    1002: ["worker_registration_failed", "worker registration failed"]
    1003: ["worker_registered", "worker is already registered"]
    1004: ["worker_not_registered", "worker is not registered"]
    1005: ["worker_stats_not_found", "worker stats not found"]

    # Serving
    1050: ["invalid_container_id", "invalid container id"]
    1051: ["container_not_registered", "container is not registered"]
    1052: ["no_containers_for_url", "no container instances found for service url"]

    1104: ["invalid_scroll_id", "Invalid scroll id"]
}

401 {
    _:  "unauthorized"
    1:  ["not_authorized", "unauthorized (not authorized for endpoint)"]
    2:  ["entity_not_allowed", "unauthorized (entity not allowed)"]
    10: ["bad_auth_type", "unauthorized (bad authentication header type)"]
    20: ["no_credentials", "unauthorized (missing credentials)"]
    21: ["bad_credentials", "unauthorized (malformed credentials)"]
    22: ["invalid_credentials", "unauthorized (invalid credentials)"]
    30: ["invalid_token", "invalid token"]
    31: ["blocked_token", "token is blocked"]
    40: ["invalid_fixed_user", "fixed user ID was not found"]
}

403: {
    _:  "forbidden"
    10: ["routing_error", "forbidden (routing error)"]
    12: ["blocked_internal_endpoint", "forbidden (blocked internal endpoint)"]
    20: ["role_not_allowed", "forbidden (not allowed for role)"]
    21: ["no_write_permission", "forbidden (modification not allowed)"]
}

410: {
    _:  "gone"
    1: ["not_supported", "thus endpoint is not supported any more"]
}

500 {
    _:   "server_error"
    0:   ["general_error", "general server error"]
    1:   ["internal_error", "internal server error"]
    2:   ["config_error", "configuration error"]
    3:   ["build_info_error", "build info unavailable or corrupted"]
    4:   ["low_disk_space", "Critical server error! Server reports low or insufficient disk space. Please resolve immediately by allocating additional disk space or freeing up storage space."]
    10:  ["transaction_error", "a transaction call has returned with an error"]
    # Database-related issues
    100: ["data_error", "general data error"]
    101: ["inconsistent_data", "inconsistent data encountered in document"]
    102: ["database_unavailable", "database is temporarily unavailable"]
    110: ["update_failed", "update failed"]

    # Index-related issues
    201: ["missing_index", "missing internal index"]

    9999: ["not_implemented", "action is not yet implemented"]
}
</file>

<file path="apiserver/apimodels/__init__.py">
class EmailField(fields.StringField)
⋮----
def validate(self, value)
⋮----
class DomainField(fields.StringField)
⋮----
def make_default(field_cls, default_value)
⋮----
class _FieldWithDefault(field_cls)
⋮----
def get_default_value(self)
⋮----
class OneOfEmbeddedField(EmbeddedField)
⋮----
model_types = tuple(set(self.discriminator_mapping.values()))
⋮----
def parse_value(self, value)
⋮----
"""Parse value to proper model type."""
⋮----
property_value = value.get(self.discriminator_property)
embed_type = self.discriminator_mapping.get(property_value)
⋮----
class ListField(fields.ListField)
⋮----
def __init__(self, items_types=None, *args, default=NotSet, **kwargs)
⋮----
default = default()
⋮----
def _cast_value(self, value)
⋮----
def validate_single_value(self, item)
⋮----
class ScalarField(fields.BaseField)
⋮----
"""String field."""
⋮----
types = (str, int, float, bool)
⋮----
class SafeStringField(fields.StringField)
⋮----
"""String field that can also accept numbers as input"""
⋮----
value = str(value)
⋮----
class DictField(fields.BaseField)
⋮----
types = (dict,)
⋮----
def __init__(self, value_types=None, *args, **kwargs)
⋮----
default = super(DictField, self).get_default_value()
⋮----
@staticmethod
    def _assign_types(value_types)
⋮----
value_types = tuple(value_types)
⋮----
value_types = (value_types,)
⋮----
value_types = tuple()
⋮----
def parse_value(self, values)
⋮----
"""Cast value to proper collection."""
result = self.get_default_value()
⋮----
tpl = 'Cannot decide which type to choose from "{types}".'
⋮----
def _elem_to_struct(self, value)
⋮----
def to_struct(self, values)
⋮----
class IntField(fields.IntField)
⋮----
class NullableEnumValidator(EnumValidator)
⋮----
"""Validator for enums that allows a None value."""
⋮----
class EnumField(fields.StringField)
⋮----
choices = list(map(self.parse_value, values_or_type))
validator_cls = EnumValidator if required else NullableEnumValidator
⋮----
class ActualEnumField(fields.StringField)
⋮----
# noinspection PyTypeChecker
choices = list(enum_class)
⋮----
validators = [*(validators or []), validator_cls(*choices)]
⋮----
# noinspection PyArgumentList
⋮----
def to_struct(self, value)
⋮----
class JsonSerializableMixin
⋮----
def to_json(self: ModelBase)
⋮----
@classmethod
    def from_json(cls: Type[ModelBase], s)
⋮----
def callable_default(cls: Type[fields.BaseField]) -> Type[fields.BaseField]
⋮----
class _Wrapped(cls)
⋮----
_callable_default = None
⋮----
def __init__(self, *args, default=None, **kwargs)
⋮----
class MongoengineFieldsDict(DictField)
⋮----
"""
    DictField representing mongoengine field names/value mapping.
    Used to convert mongoengine-style field/subfield notation to user-presentable syntax, including handling update
        operators.
    """
⋮----
mongoengine_update_operators = (
⋮----
@staticmethod
    def _normalize_mongo_value(value)
⋮----
@classmethod
    def _normalize_mongo_field_path(cls, path, value)
⋮----
parts = path.split("__")
⋮----
parts = parts[1:]
⋮----
value = None
⋮----
value = super(MongoengineFieldsDict, self).parse_value(value)
</file>

<file path="apiserver/apimodels/auth.py">
class GetTokenRequest(Base)
⋮----
""" User requests a token """
⋮----
expiration_sec = IntField(
""" Expiration time for token in seconds. """
⋮----
class GetTaskTokenRequest(GetTokenRequest)
⋮----
""" User requests a task token """
⋮----
task = StringField(required=True)
⋮----
class GetTokenForUserRequest(GetTokenRequest)
⋮----
""" System requests a token for a user """
⋮----
user = StringField(required=True)
company = StringField()
⋮----
class GetTaskTokenForUserRequest(GetTokenForUserRequest)
⋮----
""" System requests a token for a user, for a specific task """
⋮----
class GetTokenResponse(Base)
⋮----
token = StringField(required=True)
⋮----
class ValidateTokenRequest(Base)
⋮----
class ValidateUserRequest(Base)
⋮----
email = StringField(required=True)
⋮----
class ValidateResponse(Base)
⋮----
valid = BoolField(required=True)
msg = StringField()
user = StringField()
⋮----
class CreateUserRequest(Base)
⋮----
name = StringField(required=True)
company = StringField(required=True)
role = StringField(
⋮----
family_name = StringField()
given_name = StringField()
avatar = StringField()
⋮----
class CreateUserResponse(Base)
⋮----
id = StringField(required=True)
⋮----
class Credentials(Base)
⋮----
access_key = StringField(required=True)
secret_key = StringField(required=True)
label = StringField()
⋮----
class CredentialsResponse(Credentials)
⋮----
secret_key = StringField()
last_used = DateTimeField(default=None)
last_used_from = StringField()
⋮----
class CreateCredentialsRequest(Base)
⋮----
class CreateCredentialsResponse(Base)
⋮----
credentials = EmbeddedField(Credentials)
⋮----
class GetCredentialsResponse(Base)
⋮----
credentials = ListField(CredentialsResponse)
⋮----
class EditCredentialsRequest(Base)
⋮----
class RevokeCredentialsRequest(Base)
⋮----
class RevokeCredentialsResponse(Base)
⋮----
revoked = IntField(required=True)
⋮----
class AddUserRequest(CreateUserRequest)
⋮----
class AddUserResponse(CreateUserResponse)
⋮----
secret = StringField()
⋮----
class DeleteUserRequest(Base)
⋮----
class EditUserReq(Base)
⋮----
role = EnumField(Role.get_company_roles())
</file>

<file path="apiserver/apimodels/base.py">
class UpdateResponse(models.Base)
⋮----
updated = fields.IntField(required=True)
fields = MongoengineFieldsDict()
⋮----
class PagedRequest(models.Base)
⋮----
page = fields.IntField()
page_size = fields.IntField()
⋮----
class IdResponse(models.Base)
⋮----
id = fields.StringField(required=True)
⋮----
class MakePublicRequest(models.Base)
⋮----
ids = ListField(items_types=str, validators=[Length(minimum_value=1)])
⋮----
class MoveRequest(models.Base)
⋮----
ids = ListField([str], validators=Length(minimum_value=1))
project = fields.StringField()
project_name = fields.StringField()
</file>

<file path="apiserver/apimodels/batch.py">
class BatchRequest(Base)
⋮----
ids: Sequence[str] = ListField([str], validators=Length(minimum_value=1))
⋮----
class BatchResponse(Base)
⋮----
succeeded: Sequence[dict] = ListField([dict])
failed: Sequence[dict] = ListField([dict])
⋮----
class UpdateBatchItem(UpdateResponse)
⋮----
id: str = StringField()
⋮----
class UpdateBatchResponse(BatchResponse)
⋮----
succeeded: Sequence[UpdateBatchItem] = ListField(UpdateBatchItem)
</file>

<file path="apiserver/apimodels/custom_validators/__init__.py">
class ForEach(object)
⋮----
def __init__(self, validator)
⋮----
def validate(self, values)
⋮----
def modify_schema(self, field_schema)
⋮----
class Hostname(object)
⋮----
def validate(self, value)
⋮----
class Email(object)
</file>

<file path="apiserver/apimodels/events.py">
class TaskRequest(Base)
⋮----
task: str = StringField(required=True)
⋮----
class ModelRequest(Base)
⋮----
model: str = StringField(required=True)
⋮----
class HistogramRequestBase(Base)
⋮----
samples: int = IntField(default=2000, validators=[Min(1), Max(6000)])
key: ScalarKeyEnum = ActualEnumField(ScalarKeyEnum, default=ScalarKeyEnum.iter)
⋮----
class MetricVariants(Base)
⋮----
metric: str = StringField(required=True)
variants: Sequence[str] = ListField(items_types=str)
⋮----
class ScalarMetricsIterHistogramRequest(HistogramRequestBase)
⋮----
metrics: Sequence[MetricVariants] = ListField(items_types=MetricVariants)
model_events: bool = BoolField(default=False)
⋮----
class GetMetricsAndVariantsRequest(Base)
⋮----
class MultiTaskScalarMetricsIterHistogramRequest(HistogramRequestBase)
⋮----
tasks: Sequence[str] = ListField(
⋮----
class TaskMetric(Base)
⋮----
metric: str = StringField(default=None)
⋮----
class LegacyMetricEventsRequest(TaskRequest)
⋮----
iters: int = IntField(default=1, validators=validators.Min(1))
scroll_id: str = StringField()
⋮----
class MetricEventsRequest(Base)
⋮----
metrics: Sequence[TaskMetric] = ListField(
⋮----
navigate_earlier: bool = BoolField(default=True)
refresh: bool = BoolField(default=False)
⋮----
class VectorMetricsIterHistogramRequest(Base)
⋮----
variant: str = StringField(required=True)
⋮----
class GetVariantSampleRequest(Base)
⋮----
iteration: Optional[int] = IntField()
⋮----
scroll_id: Optional[str] = StringField()
navigate_current_metric: bool = BoolField(default=True)
⋮----
class GetMetricSamplesRequest(Base)
⋮----
class NextHistorySampleRequest(Base)
⋮----
next_iteration: bool = BoolField(default=False)
⋮----
class LogOrderEnum(StringEnum)
⋮----
asc = auto()
desc = auto()
⋮----
class TaskEventsRequestBase(Base)
⋮----
batch_size: int = IntField(default=500)
⋮----
class TaskEventsRequest(TaskEventsRequestBase)
⋮----
event_type: EventType = ActualEnumField(EventType, default=EventType.all)
order: Optional[str] = ActualEnumField(LogOrderEnum, default=LogOrderEnum.asc)
⋮----
count_total: bool = BoolField(default=True)
⋮----
class LegacyLogEventsRequest(TaskEventsRequestBase)
⋮----
order: Optional[str] = ActualEnumField(LogOrderEnum, default=LogOrderEnum.desc)
⋮----
class LogEventsRequest(TaskEventsRequestBase)
⋮----
batch_size: int = IntField(default=5000)
⋮----
from_timestamp: Optional[int] = IntField()
order: Optional[str] = ActualEnumField(LogOrderEnum)
⋮----
class ScalarMetricsIterRawRequest(TaskEventsRequestBase)
⋮----
batch_size: int = IntField()
⋮----
metric: MetricVariants = EmbeddedField(MetricVariants, required=True)
count_total: bool = BoolField(default=False)
⋮----
class IterationEvents(Base)
⋮----
iter: int = IntField()
events: Sequence[dict] = ListField(items_types=dict)
⋮----
class MetricEvents(Base)
⋮----
task: str = StringField()
iterations: Sequence[IterationEvents] = ListField(items_types=IterationEvents)
⋮----
class MetricEventsResponse(Base)
⋮----
metrics: Sequence[MetricEvents] = ListField(items_types=MetricEvents)
⋮----
class MultiTasksRequestBase(Base)
⋮----
class SingleValueMetricsRequest(MultiTasksRequestBase)
⋮----
class TaskMetricsRequest(MultiTasksRequestBase)
⋮----
event_type: EventType = ActualEnumField(EventType, required=True)
⋮----
class MultiTaskMetricsRequest(MultiTasksRequestBase)
⋮----
class LegacyMultiTaskEventsRequest(MultiTasksRequestBase)
⋮----
class MultiTaskPlotsRequest(MultiTasksRequestBase)
⋮----
iters: int = IntField(default=1)
⋮----
no_scroll: bool = BoolField(default=False)
last_iters_per_task_metric: bool = BoolField(default=True)
⋮----
class TaskPlotsRequest(Base)
⋮----
class GetScalarMetricDataRequest(Base)
⋮----
class ClearScrollRequest(Base)
⋮----
class ClearTaskLogRequest(Base)
⋮----
threshold_sec = IntField()
allow_locked = BoolField(default=False)
exclude_metrics = ListField(items_types=[str])
include_metrics = ListField(items_types=[str])
</file>

<file path="apiserver/apimodels/login.py">
class GetSupportedModesRequest(Base)
⋮----
# state = StringField(help_text="ASCII base64 encoded application state")
# callback_url_prefix = StringField()
⋮----
class BasicGuestMode(Base)
⋮----
enabled = BoolField(default=False)
name = StringField()
username = StringField()
password = StringField()
⋮----
class BasicMode(Base)
⋮----
guest = callable_default(EmbeddedField)(BasicGuestMode, default=BasicGuestMode)
⋮----
class ServerErrors(Base)
⋮----
missed_es_upgrade = BoolField(default=False)
es_connection_error = BoolField(default=False)
⋮----
class GetSupportedModesResponse(Base)
⋮----
basic = EmbeddedField(BasicMode)
server_errors = EmbeddedField(ServerErrors)
sso = DictField([str, type(None)])
sso_providers = ListField([dict])
authenticated = BoolField(default=False)
</file>

<file path="apiserver/apimodels/metadata.py">
class MetadataItem(Base)
⋮----
key = StringField(required=True)
type = StringField(required=True)
value = StringField(required=True)
⋮----
class DeleteMetadata(Base)
⋮----
keys: Sequence[str] = ListField(str, validators=validators.Length(minimum_value=1))
⋮----
class AddOrUpdateMetadata(Base)
⋮----
metadata: Sequence[MetadataItem] = ListField(
replace_metadata = BoolField(default=False)
</file>

<file path="apiserver/apimodels/models.py">
class GetFrameworksRequest(models.Base)
⋮----
projects = fields.ListField(items_types=[str])
⋮----
class CreateModelRequest(models.Base)
⋮----
name = fields.StringField(required=True)
uri = fields.StringField(required=True)
labels = DictField(value_types=string_types + (int,))
tags = ListField(items_types=string_types)
system_tags = ListField(items_types=string_types)
comment = fields.StringField()
public = fields.BoolField(default=False)
project = fields.StringField()
parent = fields.StringField()
framework = fields.StringField()
design = DictField()
ready = fields.BoolField(default=True)
ui_cache = DictField()
task = fields.StringField()
metadata = DictField(value_types=[MetadataItem])
⋮----
class CreateModelResponse(models.Base)
⋮----
id = fields.StringField(required=True)
created = fields.BoolField(required=True)
⋮----
class ModelRequest(models.Base)
⋮----
model = fields.StringField(required=True)
⋮----
class TaskRequest(models.Base)
⋮----
task = fields.StringField(required=True)
⋮----
class UpdateForTaskRequest(TaskRequest)
⋮----
uri = fields.StringField()
iteration = fields.IntField()
override_model_id = fields.StringField()
⋮----
class UpdateModelRequest(ModelRequest)
⋮----
class DeleteModelRequest(ModelRequest)
⋮----
force = fields.BoolField(default=False)
delete_external_artifacts = fields.BoolField(default=True)
⋮----
class ModelsDeleteManyRequest(BatchRequest)
⋮----
class PublishModelRequest(ModelRequest)
⋮----
force_publish_task = fields.BoolField(default=False)
publish_task = fields.BoolField(default=True)
⋮----
class ModelTaskPublishResponse(models.Base)
⋮----
data = fields.EmbeddedField(UpdateResponse)
⋮----
class PublishModelResponse(UpdateResponse)
⋮----
published_task = fields.EmbeddedField(ModelTaskPublishResponse)
⋮----
class ModelsPublishManyRequest(BatchRequest)
⋮----
class DeleteMetadataRequest(DeleteMetadata)
⋮----
class AddOrUpdateMetadataRequest(AddOrUpdateMetadata)
⋮----
class ModelsGetRequest(models.Base)
⋮----
include_stats = fields.BoolField(default=False)
allow_public = fields.BoolField(default=True)
</file>

<file path="apiserver/apimodels/organization.py">
class Filter(models.Base)
⋮----
tags = fields.ListField([str])
system_tags = fields.ListField([str])
⋮----
class TagsRequest(models.Base)
⋮----
include_system = fields.BoolField(default=False)
filter = fields.EmbeddedField(Filter)
⋮----
class EntitiesCountRequest(models.Base)
⋮----
projects = DictField()
tasks = DictField()
models = DictField()
pipelines = DictField()
datasets = DictField()
reports = DictField()
active_users = fields.ListField(str)
search_hidden = fields.BoolField(default=False)
allow_public = fields.BoolField(default=True)
limit = fields.IntField()
⋮----
class EntityType(StringEnum)
⋮----
task = auto()
model = auto()
⋮----
class ValueMapping(models.Base)
⋮----
key = ScalarField(nullable=True)
value = ScalarField(nullable=True)
⋮----
class FieldMapping(models.Base)
⋮----
field = fields.StringField(required=True)
name = fields.StringField()
values: Sequence[ValueMapping] = fields.ListField(items_types=[ValueMapping])
⋮----
class PrepareDownloadForGetAllRequest(models.Base)
⋮----
entity_type = ActualEnumField(EntityType)
⋮----
only_fields = fields.ListField(
field_mappings: Sequence[FieldMapping] = fields.ListField(
⋮----
class DownloadForGetAllRequest(models.Base)
⋮----
prepare_id = fields.StringField(required=True)
⋮----
class UsageAggFields(StringEnum)
⋮----
duration = auto()
cpu_usage = auto()
gpu_usage = auto()
⋮----
class UsageBreakdownKeys(StringEnum)
⋮----
project = auto()
user = auto()
queue = auto()
⋮----
class GetProjectUsagesRequest(models.Base)
⋮----
projects = fields.ListField(
from_date: str = fields.StringField(required=True)
to_date: str = fields.StringField(required=True)
include_development = fields.BoolField(default=False)
breakdown_keys: Sequence[str] = fields.ListField(
usage_fields: Sequence[str] = fields.ListField(
</file>

<file path="apiserver/apimodels/pipelines.py">
class Arg(models.Base)
⋮----
name = fields.StringField(required=True)
value = fields.StringField(required=True)
⋮----
class DeleteRunsRequest(models.Base)
⋮----
project = fields.StringField(required=True)
ids = ListField([str], required=True, validators=[Length(1)])
⋮----
class StartPipelineRequest(models.Base)
⋮----
task = fields.StringField(required=True)
queue = fields.StringField(required=True)
args = ListField(Arg)
verify_watched_queue = fields.BoolField(default=False)
</file>

<file path="apiserver/apimodels/projects.py">
class ProjectRequest(models.Base)
⋮----
project = fields.StringField(required=True)
⋮----
class MergeRequest(ProjectRequest)
⋮----
destination_project = fields.StringField()
⋮----
class MoveRequest(ProjectRequest)
⋮----
new_location = fields.StringField()
⋮----
class DeleteRequest(ProjectRequest)
⋮----
force = fields.BoolField(default=False)
delete_contents = fields.BoolField(default=False)
delete_external_artifacts = fields.BoolField(default=True)
⋮----
class ProjectOrNoneRequest(models.Base)
⋮----
project = fields.StringField()
include_subprojects = fields.BoolField(default=True)
⋮----
class GetUniqueMetricsRequest(ProjectOrNoneRequest)
⋮----
model_metrics = fields.BoolField(default=False)
ids = fields.ListField(str)
⋮----
class GetParamsRequest(ProjectOrNoneRequest)
⋮----
page = fields.IntField(default=0)
page_size = fields.IntField(default=500)
⋮----
class ProjectTagsRequest(TagsRequest)
⋮----
projects = ListField(str)
⋮----
class MultiProjectRequest(models.Base)
⋮----
projects = fields.ListField(items_types=[str, type(None)])
⋮----
class ProjectTaskParentsRequest(MultiProjectRequest)
⋮----
tasks_state = ActualEnumField(EntityVisibility)
task_name = fields.StringField()
⋮----
class EntityTypeEnum(StringEnum)
⋮----
task = auto()
model = auto()
⋮----
class ProjectUserNamesRequest(MultiProjectRequest)
⋮----
entity = ActualEnumField(EntityTypeEnum, default=EntityTypeEnum.task)
⋮----
class MultiProjectPagedRequest(MultiProjectRequest)
⋮----
allow_public = fields.BoolField(default=True)
⋮----
class ProjectHyperparamValuesRequest(MultiProjectPagedRequest)
⋮----
section = fields.StringField(required=True)
name = fields.StringField(required=True)
pattern = fields.StringField()
⋮----
class ProjectModelMetadataValuesRequest(MultiProjectPagedRequest)
⋮----
key = fields.StringField(required=True)
⋮----
class ProjectChildrenType(Enum)
⋮----
pipeline = "pipeline"
report = "report"
dataset = "dataset"
⋮----
class ProjectsGetRequest(models.Base)
⋮----
include_dataset_stats = fields.BoolField(default=False)
include_stats = fields.BoolField(default=False)
include_stats_filter = DictField()
stats_with_children = fields.BoolField(default=True)
stats_for_state = ActualEnumField(EntityVisibility, default=EntityVisibility.active)
non_public = fields.BoolField(default=False)  # legacy, use allow_public instead
active_users = fields.ListField(str)
check_own_contents = fields.BoolField(default=False)
shallow_search = fields.BoolField(default=False)
search_hidden = fields.BoolField(default=False)
⋮----
children_type = ActualEnumField(ProjectChildrenType)
children_tags = fields.ListField(str)
children_tags_filter = DictField()
</file>

<file path="apiserver/apimodels/queues.py">
class GetDefaultResp(Base)
⋮----
id = StringField(required=True)
name = StringField(required=True)
⋮----
class CreateRequest(Base)
⋮----
display_name = StringField()
tags = ListField(items_types=[str])
system_tags = ListField(items_types=[str])
metadata = DictField(value_types=[MetadataItem])
⋮----
class QueueRequest(Base)
⋮----
queue = StringField(required=True)
⋮----
class GetByIdRequest(QueueRequest)
⋮----
max_task_entries = IntField()
⋮----
class GetAllRequest(Base)
⋮----
search_hidden = BoolField(default=False)
⋮----
class GetNextTaskRequest(QueueRequest)
⋮----
get_task_info = BoolField(default=False)
task = StringField()
⋮----
class DeleteRequest(QueueRequest)
⋮----
force = BoolField(default=False)
⋮----
class UpdateRequest(QueueRequest)
⋮----
name = StringField()
⋮----
class TaskRequest(QueueRequest)
⋮----
task = StringField(required=True)
⋮----
class RemoveTaskRequest(TaskRequest)
⋮----
update_task_status = BoolField(default=False)
⋮----
class AddTaskRequest(TaskRequest)
⋮----
update_execution_queue = BoolField(default=True)
⋮----
class MoveTaskRequest(TaskRequest)
⋮----
count = IntField(default=1)
⋮----
class MoveTaskResponse(Base)
⋮----
position = IntField()
⋮----
class GetMetricsRequest(Base)
⋮----
queue_ids = ListField([str])
from_date = FloatField(required=True, validators=validators.Min(0))
to_date = FloatField(required=True, validators=validators.Min(0))
interval = IntField(required=True, validators=validators.Min(1))
refresh = BoolField(default=False)
⋮----
class QueueMetrics(Base)
⋮----
queue = StringField()
dates = ListField(int)
avg_waiting_times = ListField([float, int])
queue_lengths = ListField(int)
⋮----
class GetMetricsResponse(Base)
⋮----
queues = ListField(QueueMetrics)
⋮----
class DeleteMetadataRequest(DeleteMetadata)
⋮----
class AddOrUpdateMetadataRequest(AddOrUpdateMetadata)
</file>

<file path="apiserver/apimodels/reports.py">
class UpdateReportRequest(Base)
⋮----
task = StringField(required=True)
name = StringField(nullable=True, validators=Length(minimum_value=3))
tags = ListField(items_types=[str])
comment = StringField()
report = StringField()
report_assets = ListField(items_types=[str])
⋮----
class CreateReportRequest(Base)
⋮----
name = StringField(required=True, validators=Length(minimum_value=3))
⋮----
project = StringField()
⋮----
class PublishReportRequest(Base)
⋮----
message = StringField(default="")
⋮----
class ArchiveReportRequest(Base)
⋮----
class ShareReportRequest(Base)
⋮----
share = BoolField(default=True)
⋮----
class DeleteReportRequest(Base)
⋮----
force = BoolField(default=False)
⋮----
class MoveReportRequest(Base)
⋮----
project_name = StringField()
⋮----
class EventsRequest(Base)
⋮----
iters = IntField(default=1, validators=validators.Min(1))
metrics: Sequence[MetricVariants] = ListField(items_types=MetricVariants)
⋮----
class PlotEventsRequest(EventsRequest)
⋮----
last_iters_per_task_metric: bool = BoolField(default=True)
⋮----
class ScalarMetricsIterHistogram(HistogramRequestBase)
⋮----
class SingleValueMetrics(Base)
⋮----
class GetTasksDataRequest(Base)
⋮----
debug_images: EventsRequest = EmbeddedField(EventsRequest)
plots: PlotEventsRequest = EmbeddedField(PlotEventsRequest)
scalar_metrics_iter_histogram: ScalarMetricsIterHistogram = EmbeddedField(
single_value_metrics: SingleValueMetrics = EmbeddedField(SingleValueMetrics)
allow_public = BoolField(default=True)
model_events: bool = BoolField(default=False)
⋮----
class GetAllRequest(Base)
</file>

<file path="apiserver/apimodels/server.py">
class ReportStatsOptionRequest(Base)
⋮----
enabled = BoolField(default=None, nullable=True)
⋮----
class GetConfigRequest(Base)
⋮----
path = StringField()
⋮----
class ReportStatsOptionResponse(Base)
⋮----
supported = BoolField(default=True)
enabled = BoolField()
enabled_time = DateTimeField(nullable=True)
enabled_version = StringField(nullable=True)
enabled_user = StringField(nullable=True)
current_version = StringField()
</file>

<file path="apiserver/apimodels/serving.py">
class ReferenceItem(Base)
⋮----
type = StringField(
value = StringField(required=True)
⋮----
class ServingModel(Base)
⋮----
container_id = StringField(required=True)
endpoint_name = StringField(required=True)
endpoint_url = StringField()  # can be not existing yet at registration time
model_name = StringField(required=True)
model_source = StringField()
model_version = StringField()
preprocess_artifact = StringField()
input_type = StringField()
input_size = SafeStringField()
tags = ListField(str)
system_tags = ListField(str)
reference: Sequence[ReferenceItem] = ListField(ReferenceItem)
⋮----
class RegisterRequest(ServingModel)
⋮----
timeout = IntField(
""" registration timeout in seconds (default is 10min) """
⋮----
class UnregisterRequest(Base)
⋮----
class StatusReportRequest(ServingModel)
⋮----
uptime_sec = IntField()
requests_num = IntField()
requests_min = FloatField()
latency_ms = IntField()
machine_stats: MachineStats = EmbeddedField(MachineStats)
⋮----
class ServingContainerEntry(StatusReportRequest, JsonSerializableMixin)
⋮----
key = StringField(required=True)
company_id = StringField(required=True)
ip = StringField()
register_time = DateTimeField(required=True)
register_timeout = IntField(required=True)
last_activity_time = DateTimeField(required=True)
⋮----
class GetEndpointDetailsRequest(Base)
⋮----
endpoint_url = StringField(required=True)
⋮----
class MetricType(Enum)
⋮----
requests = "requests"
requests_min = "requests_min"
latency_ms = "latency_ms"
cpu_count = "cpu_count"
gpu_count = "gpu_count"
cpu_util = "cpu_util"
gpu_util = "gpu_util"
ram_total = "ram_total"
ram_used = "ram_used"
ram_free = "ram_free"
gpu_ram_total = "gpu_ram_total"
gpu_ram_used = "gpu_ram_used"
gpu_ram_free = "gpu_ram_free"
network_rx = "network_rx"
network_tx = "network_tx"
⋮----
class GetEndpointMetricsHistoryRequest(Base)
⋮----
from_date = FloatField(required=True, validators=Min(0))
to_date = FloatField(required=True, validators=Min(0))
interval = IntField(required=True, validators=Min(1))
⋮----
metric_type = ActualEnumField(MetricType, default=MetricType.requests)
instance_charts = BoolField(default=True)
</file>

<file path="apiserver/apimodels/storage.py">
class AWSBucketSettings(Base)
⋮----
bucket = StringField()
subdir = StringField()
host = StringField()
key = StringField()
secret = StringField()
token = StringField()
multipart = BoolField(default=True)
acl = StringField()
secure = BoolField(default=True)
region = StringField()
verify = BoolField(default=True)
use_credentials_chain = BoolField(default=False)
⋮----
class AWSSettings(Base)
⋮----
buckets = ListField(items_types=[AWSBucketSettings])
⋮----
class GoogleBucketSettings(Base)
⋮----
project = StringField()
credentials_json = StringField()
⋮----
class GoogleSettings(Base)
⋮----
buckets = ListField(items_types=[GoogleBucketSettings])
⋮----
class AzureContainerSettings(Base)
⋮----
account_name = StringField()
account_key = StringField()
container_name = StringField()
⋮----
class AzureSettings(Base)
⋮----
containers = ListField(items_types=[AzureContainerSettings])
⋮----
class SetSettingsRequest(Base)
⋮----
aws = EmbeddedField(AWSSettings)
google = EmbeddedField(GoogleSettings)
azure = EmbeddedField(AzureSettings)
⋮----
class ResetSettingsRequest(Base)
⋮----
keys = ListField([str], item_validators=[Enum("aws", "google", "azure")])
</file>

<file path="apiserver/apimodels/tasks.py">
class ArtifactTypeData(models.Base)
⋮----
preview = StringField()
content_type = StringField()
data_hash = StringField()
⋮----
class Artifact(models.Base)
⋮----
key = StringField(required=True)
type = StringField(required=True)
mode = StringField(
uri = StringField()
hash = StringField()
content_size = IntField()
timestamp = IntField()
type_data = EmbeddedField(ArtifactTypeData)
display_data = ListField([list])
⋮----
class StartedResponse(UpdateResponse)
⋮----
started = IntField()
⋮----
class EnqueueResponse(UpdateResponse)
⋮----
queued = IntField()
queue_watched = BoolField()
⋮----
class EnqueueBatchItem(UpdateBatchItem)
⋮----
queued: bool = BoolField()
⋮----
class EnqueueManyResponse(BatchResponse)
⋮----
succeeded: Sequence[EnqueueBatchItem] = ListField(EnqueueBatchItem)
⋮----
class DequeueResponse(UpdateResponse)
⋮----
dequeued = IntField()
⋮----
class DequeueBatchItem(UpdateBatchItem)
⋮----
dequeued: bool = BoolField()
⋮----
class DequeueManyResponse(BatchResponse)
⋮----
succeeded: Sequence[DequeueBatchItem] = ListField(DequeueBatchItem)
⋮----
class ResetResponse(UpdateResponse)
⋮----
dequeued = DictField()
events = DictField()
deleted_models = IntField()
urls = DictField()
⋮----
class ResetBatchItem(UpdateBatchItem)
⋮----
class ResetManyResponse(BatchResponse)
⋮----
succeeded: Sequence[ResetBatchItem] = ListField(ResetBatchItem)
⋮----
class TaskRequest(models.Base)
⋮----
task = StringField(required=True)
⋮----
class TaskUpdateRequest(TaskRequest)
⋮----
force = BoolField(default=False)
⋮----
class UpdateRequest(TaskUpdateRequest)
⋮----
status_reason = StringField(default="")
status_message = StringField(default="")
⋮----
class DequeueRequest(UpdateRequest)
⋮----
remove_from_all_queues = BoolField(default=False)
new_status = StringField()
⋮----
class StopRequest(UpdateRequest)
⋮----
include_pipeline_steps = BoolField(default=False)
⋮----
class EnqueueRequest(UpdateRequest)
⋮----
queue = StringField()
queue_name = StringField()
verify_watched_queue = BoolField(default=False)
update_execution_queue = BoolField(default=True)
⋮----
class DeleteRequest(UpdateRequest)
⋮----
move_to_trash = BoolField(default=False)
return_file_urls = BoolField(default=False)
delete_output_models = BoolField(default=True)
delete_external_artifacts = BoolField(default=True)
⋮----
class SetRequirementsRequest(TaskRequest)
⋮----
requirements = DictField(required=True)
⋮----
class CompletedRequest(UpdateRequest)
⋮----
publish = BoolField(default=False)
⋮----
class CompletedResponse(UpdateResponse)
⋮----
published = IntField(default=0)
⋮----
class PublishRequest(UpdateRequest)
⋮----
publish_model = BoolField(default=True)
⋮----
class TaskData(models.Base)
⋮----
"""
    This is a partial description of task can be updated incrementally
    """
⋮----
class CreateRequest(TaskData)
⋮----
name = StringField(required=True)
type = StringField(required=True, validators=Enum(*get_options(TaskType)))
⋮----
class PingRequest(TaskRequest)
⋮----
class EditRuntimeRequest(TaskUpdateRequest)
⋮----
add_or_update = DictField()
remove = ListField(items_types=[str])
⋮----
class GetTypesRequest(models.Base)
⋮----
projects = ListField(items_types=[str])
⋮----
class TaskInputModel(models.Base)
⋮----
name = StringField()
model = StringField()
⋮----
class CloneRequest(TaskRequest)
⋮----
new_task_name = StringField()
new_task_comment = StringField()
new_task_tags = ListField([str])
new_task_system_tags = ListField([str])
new_task_parent = StringField()
new_task_project = StringField()
new_task_hyperparams = DictField()
new_task_configuration = DictField()
new_task_container = DictField()
new_task_input_models = ListField([TaskInputModel])
execution_overrides = DictField()
validate_references = BoolField(default=False)
new_project_name = StringField()
⋮----
class AddOrUpdateArtifactsRequest(TaskUpdateRequest)
⋮----
artifacts = ListField([Artifact], validators=Length(minimum_value=1))
⋮----
class ArtifactId(models.Base)
⋮----
class DeleteArtifactsRequest(TaskUpdateRequest)
⋮----
artifacts = ListField([ArtifactId], validators=Length(minimum_value=1))
⋮----
class ResetRequest(UpdateRequest)
⋮----
clear_all = BoolField(default=False)
⋮----
class MultiTaskRequest(models.Base)
⋮----
tasks = ListField([str], validators=Length(minimum_value=1))
⋮----
class GetHyperParamsRequest(MultiTaskRequest)
⋮----
class HyperParamItem(models.Base)
⋮----
section = StringField(required=True, validators=Length(minimum_value=1))
name = StringField(required=True, validators=Length(minimum_value=1))
value = StringField(required=True)
type = StringField()
description = StringField()
⋮----
class ReplaceHyperparams(object)
⋮----
none = "none"
section = "section"
all = "all"
⋮----
class EditHyperParamsRequest(TaskUpdateRequest)
⋮----
hyperparams: Sequence[HyperParamItem] = ListField(
replace_hyperparams = StringField(
⋮----
class HyperParamKey(models.Base)
⋮----
name = StringField(nullable=True)
⋮----
class DeleteHyperParamsRequest(TaskUpdateRequest)
⋮----
hyperparams: Sequence[HyperParamKey] = ListField(
⋮----
class GetConfigurationsRequest(MultiTaskRequest)
⋮----
names = ListField([str])
⋮----
class GetConfigurationNamesRequest(MultiTaskRequest)
⋮----
skip_empty = BoolField(default=True)
⋮----
class Configuration(models.Base)
⋮----
class EditConfigurationRequest(TaskUpdateRequest)
⋮----
configuration: Sequence[Configuration] = ListField(
replace_configuration = BoolField(default=False)
⋮----
class DeleteConfigurationRequest(TaskUpdateRequest)
⋮----
configuration: Sequence[str] = ListField([str], validators=Length(minimum_value=1))
⋮----
class ArchiveRequest(MultiTaskRequest)
⋮----
class ArchiveResponse(models.Base)
⋮----
archived = IntField()
⋮----
class TaskBatchRequest(BatchRequest)
⋮----
class ArchiveManyRequest(TaskBatchRequest)
⋮----
class UnarchiveManyRequest(TaskBatchRequest)
⋮----
class StopManyRequest(TaskBatchRequest)
⋮----
class DequeueManyRequest(TaskBatchRequest)
⋮----
class EnqueueManyRequest(TaskBatchRequest)
⋮----
validate_tasks = BoolField(default=False)
⋮----
class DeleteManyRequest(TaskBatchRequest)
⋮----
class ResetManyRequest(TaskBatchRequest)
⋮----
class PublishManyRequest(TaskBatchRequest)
⋮----
class AddUpdateModelRequest(TaskRequest)
⋮----
model = StringField(required=True)
type = StringField(required=True, validators=Enum(*get_options(TaskModelTypes)))
iteration = IntField()
⋮----
class ModelItemKey(models.Base)
⋮----
class DeleteModelsRequest(TaskRequest)
⋮----
models: Sequence[ModelItemKey] = ListField(
⋮----
class GetAllReq(models.Base)
⋮----
allow_public = BoolField(default=True)
search_hidden = BoolField(default=False)
⋮----
class UpdateTagsRequest(BatchRequest)
⋮----
add_tags = ListField([str])
remove_tags = ListField([str])
</file>

<file path="apiserver/apimodels/users.py">
class UserRequest(Base)
⋮----
user = StringField(required=True)
⋮----
class CreateRequest(Base)
⋮----
id = StringField(required=True)
name = StringField(required=True)
company = StringField(required=True)
family_name = StringField()
given_name = StringField()
avatar = StringField()
⋮----
class SetPreferencesRequest(Base)
⋮----
preferences = DictField(required=True)
return_updated = BoolField(default=True)
</file>

<file path="apiserver/apimodels/workers.py">
class WorkerRequest(Base)
⋮----
worker = StringField(required=True)
tags = ListField(str)
system_tags = ListField(str)
⋮----
class Resources(Base)
⋮----
cpu_usage = FloatField()
gpu_usage = FloatField()
⋮----
class RegisterRequest(WorkerRequest)
⋮----
timeout = IntField(
""" registration timeout in seconds (default is 10min) """
queues = ListField(six.string_types)  # list of queues this worker listens to
resources = EmbeddedField(Resources)
⋮----
class MachineStats(Base)
⋮----
cpu_usage = ListField(six.integer_types + (float,))
cpu_temperature = ListField(six.integer_types + (float,))
gpu_usage = ListField(six.integer_types + (float,))
gpu_temperature = ListField(six.integer_types + (float,))
gpu_memory_free = ListField(six.integer_types + (float,))
gpu_memory_used = ListField(six.integer_types + (float,))
memory_used = FloatField()
memory_free = FloatField()
network_tx = FloatField()
network_rx = FloatField()
disk_free_home = FloatField()
disk_free_temp = FloatField()
disk_read = FloatField()
disk_write = FloatField()
⋮----
class StatusReportRequest(WorkerRequest)
⋮----
task = StringField()  # task the worker is running on
queue = StringField()  # queue from which task was taken
queues = ListField(
⋮----
)  # list of queues this worker listens to. if None, this will not update the worker's queues list.
timestamp = IntField(required=True)
machine_stats = EmbeddedField(MachineStats)
⋮----
class IdNameEntry(Base)
⋮----
id = StringField(required=True)
name = StringField()
⋮----
class WorkerEntry(Base, JsonSerializableMixin)
⋮----
key = StringField()  # not required due to migration issues
⋮----
user = EmbeddedField(IdNameEntry)
company = EmbeddedField(IdNameEntry)
ip = StringField()
task = EmbeddedField(IdNameEntry)
project = EmbeddedField(IdNameEntry)
queue = StringField()  # queue from which current task was taken
queues = ListField(str)  # list of queues this worker listens to
register_time = DateTimeField(required=True)
register_timeout = IntField(required=True)
last_activity_time = DateTimeField(required=True)
last_report_time = DateTimeField()
⋮----
class CurrentTaskEntry(IdNameEntry)
⋮----
running_time = IntField()
last_iteration = IntField()
⋮----
class QueueEntry(IdNameEntry)
⋮----
display_name = StringField()
next_task = EmbeddedField(IdNameEntry)
num_tasks = IntField()
⋮----
class WorkerResponseEntry(WorkerEntry)
⋮----
task = EmbeddedField(CurrentTaskEntry)
queue = EmbeddedField(QueueEntry)
queues = ListField(QueueEntry)
⋮----
class GetAllRequest(Base)
⋮----
last_seen = IntField(default=3600)
⋮----
worker_pattern = StringField()
⋮----
class GetAllResponse(Base)
⋮----
workers = ListField(WorkerResponseEntry)
⋮----
class GetCountRequest(GetAllRequest)
⋮----
last_seen = IntField(default=0)
⋮----
class StatsBase(Base)
⋮----
worker_ids = ListField(str)
⋮----
class StatsReportBase(StatsBase)
⋮----
from_date = FloatField(required=True, validators=validators.Min(0))
to_date = FloatField(required=True, validators=validators.Min(0))
interval = IntField(required=True, validators=validators.Min(1))
⋮----
class AggregationType(Enum)
⋮----
avg = "avg"
min = "min"
max = "max"
⋮----
class StatItem(Base)
⋮----
key = StringField(required=True)
aggregation = ActualEnumField(AggregationType, default=AggregationType.avg)
⋮----
class GetStatsRequest(StatsReportBase)
⋮----
items = ListField(
split_by_variant = BoolField(default=False)
split_by_resource = BoolField(default=False)
⋮----
class MetricResourceSeries(Base)
⋮----
values = ListField(float)
⋮----
class AggregationStats(Base)
⋮----
aggregation = EnumField(AggregationType)
dates = ListField(int)
⋮----
resource_series = ListField(MetricResourceSeries)
⋮----
class MetricStats(Base)
⋮----
metric = StringField()
variant = StringField()
stats = ListField(AggregationStats)
⋮----
class WorkerStatistics(Base)
⋮----
worker = StringField()
metrics = ListField(MetricStats)
⋮----
class GetStatsResponse(Base)
⋮----
workers = ListField(WorkerStatistics)
⋮----
class GetMetricKeysRequest(StatsBase)
⋮----
class MetricCategory(Base)
⋮----
metric_keys = ListField(str)
⋮----
class GetMetricKeysResponse(Base)
⋮----
categories = ListField(MetricCategory)
⋮----
class GetActivityReportRequest(StatsReportBase)
⋮----
class ActivityReportSeries(Base)
⋮----
counts = ListField(int)
⋮----
class GetActivityReportResponse(Base)
⋮----
total = EmbeddedField(ActivityReportSeries)
active = EmbeddedField(ActivityReportSeries)
</file>

<file path="apiserver/bll/__init__.py">

</file>

<file path="apiserver/bll/auth/__init__.py">
log = config.logger("AuthBLL")
⋮----
class AuthBLL
⋮----
query = dict(id=user_id)
⋮----
user = User.objects(**query).first()
⋮----
company_id = company_id or user.company
company = Company.objects(id=company_id).only("id", "name").first()
⋮----
identity = Identity(
⋮----
token = Token.create_encoded_token(
⋮----
@staticmethod
    def create_user(request: CreateUserRequest, call: APICall = None) -> str
⋮----
"""
        Create a new user in both the auth database and the backend database
        :param request: New user details
        :param call: API call that triggered this call. If not None, new backend user creation
        will be performed using a new call in the same transaction.
        :return: The new user's ID
        """
⋮----
user = User(
⋮----
users_create_request = Users_CreateRequest(
⋮----
"""
        Delete an existing user from both the auth database and the backend database
        :param identity: Calling user identity
        :param user_id: ID of user to delete
        :param company_id: Company of user to delete
        :param call: API call that triggered this call. If not None, backend user deletion
        will be performed using a new call in the same transaction.
        """
⋮----
company_id = identity.company
⋮----
query = dict(id=user_id, company=company_id)
res = User.objects(**query).delete()
⋮----
cred = CredModel(
</file>

<file path="apiserver/bll/event/__init__.py">

</file>

<file path="apiserver/bll/event/event_bll.py">
# noinspection PyTypeChecker
EVENT_TYPES: Set[str] = set(et.value for et in EventType if et != EventType.all)
LOCKED_TASK_STATUSES = (TaskStatus.publishing, TaskStatus.published)
MAX_LONG = 2**63 - 1
MIN_LONG = -(2**63)
⋮----
log = config.logger(__file__)
async_task_events_delete = config.get("services.tasks.async_events_delete", False)
async_delete_threshold = config.get(
⋮----
class EventBLL(object)
⋮----
event_id_fields = ("task", "iter", "metric", "variant", "key")
empty_scroll = "FFFF"
img_source_regex = re.compile(
_task_event_query = {
_model_event_query = {"term": {"model_event": True}}
⋮----
def __init__(self, events_es=None, redis=None)
⋮----
@property
    def metrics(self) -> EventMetrics
⋮----
"""Verify that task or model exists and can be updated"""
⋮----
allow_locked = {id_ for id_, allowed in ids.items() if allowed}
not_locked = {id_ for id_, allowed in ids.items() if not allowed}
res = set()
allow_locked_q = Q()
not_locked_q = (
⋮----
query = Q(id__in=requested_ids) & locked_q
⋮----
ids = Model.objects(query & Q(company=company_id)).scalar("id")
⋮----
ids = {
⋮----
user_id = identity.user
task_ids = {}
model_ids = {}
⋮----
model = event.pop("model", None)
⋮----
entity_ids = model_ids
⋮----
entity_ids = task_ids
⋮----
id_ = event.get("task")
allow_locked = event.pop("allow_locked", False)
⋮----
allowed_for_entity = entity_ids.get(id_)
⋮----
found_in_both = set(task_ids).intersection(set(model_ids))
⋮----
valid_models = self._get_valid_entities(
valid_tasks = self._get_valid_entities(
⋮----
actions: List[dict] = []
used_task_ids = set()
used_model_ids = set()
task_iteration = defaultdict(lambda: 0)
task_last_scalar_events = nested_dict(
⋮----
)  # task_id -> metric_hash -> variant_hash -> MetricEvent
task_last_events = nested_dict(
⋮----
)  # task_id -> metric_hash -> event_type -> MetricEvent
errors_per_type = defaultdict(int)
invalid_iteration_error = f"Iteration number should not exceed {MAX_LONG}"
⋮----
x_axis_label = event.pop("x_axis_label", None)
⋮----
# remove spaces from event type
event_type = event.get("type")
⋮----
event_type = event_type.replace(" ", "_")
⋮----
model_event = event["model_event"]
⋮----
task_or_model_id = event.get("task")
⋮----
# @timestamp indicates the time the event is written, not when it happened
⋮----
# for backward bomba-tavili-tea
⋮----
# set timestamp and worker if not sent
⋮----
# force iter to be a long int
iter = event.get("iter")
⋮----
iter = int(iter)
⋮----
# used to have "values" to indicate array. no need anymore
⋮----
index_name = get_index_name(company_id, event_type)
es_action = {
⋮----
"_op_type": "index",  # overwrite if exists with same ID
⋮----
# for "log" events, don't assign custom _id - whatever is sent, is written (not overwritten)
⋮----
plot_actions = [
⋮----
added = 0
⋮----
chunk_size = 500
# TODO: replace it with helpers.parallel_bulk in the future once the parallel pool leak is fixed
⋮----
# thread_count=8,
⋮----
now = datetime.utcnow()
⋮----
remaining_tasks = set()
⋮----
# Update related tasks. For reasons of performance, we prefer to update
# all of them and not only those who's events were successful
updated = self._update_task(
⋮----
# this is for backwards compatibility with streaming bulk throwing exception on those
invalid_iterations_count = errors_per_type.get(invalid_iteration_error)
⋮----
errors_count = sum(errors_per_type.values())
⋮----
validate = validate_json and not event.pop("skip_validation", False)
plot_str = event.get(PlotFields.plot_str)
⋮----
plot_len = len(plot_str)
⋮----
urls = {match for match in self.img_source_regex.findall(plot_str)}
⋮----
@parallel_chunked_decorator(chunk_size=10)
    def uncompress_plots(self, plot_events: Sequence[dict])
⋮----
@staticmethod
    def _is_valid_json(text: str) -> bool
⋮----
"""Check str for valid json"""
⋮----
def _update_last_scalar_events_for_task(self, last_events, event, x_axis_label=None)
⋮----
"""
        Update last_events structure with the provided event details if this event is more
        recent than the currently stored event for its metric/variant combination.

        last_events contains [hashed_metric_name -> hashed_variant_name -> event]. Keys are hashed to avoid mongodb
        key conflicts due to invalid characters and/or long field names.
        """
value = event.get("value")
⋮----
metric = event.get("metric") or ""
variant = event.get("variant") or ""
metric_hash = dbutils.hash_field_name(metric)
variant_hash = dbutils.hash_field_name(variant)
⋮----
last_event = last_events[metric_hash][variant_hash]
⋮----
event_iter = event.get("iter", 0)
event_timestamp = event.get("timestamp", 0)
⋮----
first_value_iter = last_event.get("first_value_iter")
⋮----
last_event_min_value = last_event.get("min_value")
⋮----
last_event_max_value = last_event.get("max_value")
⋮----
def _update_last_metric_events_for_task(self, last_events, event)
⋮----
"""
        Update last_events structure with the provided event details if this event is more
        recent than the currently stored event for its metric/event_type combination.
        last_events contains [metric_name -> event_type -> event]
        """
⋮----
timestamp = last_events[metric][event_type].get("timestamp", None)
⋮----
"""
        Update task information in DB with aggregated results after handling event(s) related to this task.

        This updates the task with the highest iteration value encountered during the last events update, as well
        as the latest metric/variant scalar values reported (according to the report timestamp) and the task's last
        update time.
        """
⋮----
def _get_event_id(self, event)
⋮----
id_values = (
⋮----
es_res = self.es.scroll(scroll_id=scroll_id, scroll="1h")
⋮----
size = min(batch_size, 10000)
⋮----
es_req = {
⋮----
es_res = search_company_events(
⋮----
event_type = EventType.metrics_plot
⋮----
plot_valid_condition = {
must = [plot_valid_condition, {"term": {"task": task_id}}]
⋮----
query = {"bool": {"must": must}}
search_args = dict(es=self.es, company_id=company_id, event_type=event_type)
⋮----
max_variants = int(max_variants // last_iterations_per_plot)
⋮----
es_response = search_company_events(body=es_req, ignore=404, **search_args)
⋮----
aggs_result = es_response.get("aggregations")
⋮----
events = [
⋮----
def _get_events_from_es_res(self, es_res: dict) -> Tuple[list, int, Optional[str]]
⋮----
"""
        Return events and next scroll id from the scrolled query
        Release the scroll once it is exhausted
        """
total_events = nested_get(es_res, ("hits", "total", "value"), default=0)
⋮----
next_scroll_id = es_res.get("_scroll_id")
⋮----
next_scroll_id = self.empty_scroll
⋮----
es_response = search_company_events(
res = nested_get(es_response, ("aggregations", "debug_images"))
⋮----
es_res = self.es.scroll(scroll_id=scroll_id, scroll="10m")
⋮----
company_ids = [company_id] if isinstance(company_id, str) else company_id
company_ids = [
⋮----
task_ids = [task_id] if isinstance(task_id, str) else task_id
⋮----
must = []
⋮----
task_metric_iters = self.get_last_iters_per_metric(
should = [
⋮----
tasks_iters = self.get_last_iters(
⋮----
sort = [{"timestamp": {"order": "asc"}}]
⋮----
query = {"bool": {"must": [{"term": {"task": task_id}}]}}
⋮----
es_res = search_company_events(body=es_req, **search_args)
⋮----
metrics = {}
⋮----
metric = metric_bucket["key"]
⋮----
event_type = EventType.metrics_scalar
⋮----
query = {
⋮----
max_variants = int(max_variants // 2)
⋮----
metrics = []
max_timestamp = 0
⋮----
metric_summary = dict(name=metric_bucket["key"], variants=[])
⋮----
variant_name = variant_bucket["key"]
last_value = variant_bucket["last_value"]["hits"]["hits"][0]["fields"][
last_10_value = variant_bucket["last_10_value"]["hits"]["hits"][0][
timestamp = variant_bucket["last_timestamp"]["value"]
max_timestamp = max(timestamp, max_timestamp)
⋮----
def get_vector_metrics_per_iter(self, company_id, task_id, metric, variant)
⋮----
event_type = EventType.metrics_vector
⋮----
vectors = []
iterations = []
⋮----
must = [{"terms": {"task": task_ids}}]
⋮----
max_tasks = min(len(task_ids), 1000)
max_metrics = 10_000 // (max_tasks * iters)
es_req: dict = {
⋮----
@staticmethod
    def _get_events_deletion_params(async_delete: bool) -> dict
⋮----
"""
        Delete task events. No check is done for tasks write access
        so it should be checked by the calling code
        """
⋮----
task_ids = [task_ids]
deleted = 0
⋮----
async_delete = async_task_events_delete and not wait_for_delete
⋮----
total = self.events_iterator.count_task_events(
⋮----
async_delete = False
⋮----
es_req = {"query": {"terms": {"task": tasks}}}
es_res = delete_company_events(
⋮----
must = [{"term": {"task": task_id}}]
sort = None
⋮----
timestamp_ms = int(threshold_sec * 1000)
⋮----
sort = {"timestamp": {"order": "desc"}}
⋮----
more_conditions = {}
⋮----
more_conditions = {"must_not": [{"terms": {"metric": exclude_metrics}}]}
⋮----
def clear_scroll(self, scroll_id: str)
⋮----
# noinspection PyBroadException
</file>

<file path="apiserver/bll/event/event_common.py">
class EventType(Enum)
⋮----
metrics_scalar = "training_stats_scalar"
metrics_vector = "training_stats_vector"
metrics_image = "training_debug_image"
metrics_plot = "plot"
task_log = "log"
all = "*"
⋮----
SINGLE_SCALAR_ITERATION = -(2 ** 31)
MetricVariants = Mapping[str, Sequence[str]]
TaskCompanies = Mapping[str, Sequence[Task]]
⋮----
class EventSettings
⋮----
_max_es_allowed_aggregation_buckets = 10000
⋮----
@classproperty
    def max_workers(self)
⋮----
@classproperty
    def state_expiration_sec(self)
⋮----
@classproperty
    def max_es_buckets(self)
⋮----
percentage = (
⋮----
def get_index_name(company_id: Union[str, Sequence[str]], event_type: str)
⋮----
event_type = event_type.lower().replace(" ", "_")
⋮----
company_id = [company_id]
⋮----
def check_empty_data(es: Elasticsearch, company_id: str, event_type: EventType) -> bool
⋮----
es_index = get_index_name(company_id, event_type.value)
⋮----
dynamic = config.get(
max_metrics_count = config.get(
max_variants_count = config.get(
⋮----
es_req: dict = {
⋮----
es_res = search_company_events(
⋮----
metrics_count = nested_get(
⋮----
def get_metric_variants_condition(metric_variants: MetricVariants,) -> Sequence
⋮----
conditions = [
⋮----
class PlotFields
⋮----
valid_plot = "valid_plot"
plot_len = "plot_len"
plot_str = "plot_str"
plot_data = "plot_data"
source_urls = "source_urls"
⋮----
def uncompress_plot(event: dict)
⋮----
plot_data = event.pop(PlotFields.plot_data, None)
</file>

<file path="apiserver/bll/event/event_metrics.py">
log = config.logger(__file__)
⋮----
class EventMetrics
⋮----
MAX_AGGS_ELEMENTS_COUNT = 50
MAX_SAMPLE_BUCKETS = 6000
⋮----
def __init__(self, es: Elasticsearch)
⋮----
"""
        Get scalar metric histogram per metric and variant
        The amount of points in each histogram should not exceed
        the requested samples
        """
event_type = EventType.metrics_scalar
⋮----
intervals = self._get_task_metric_intervals(
⋮----
interval_groups = self._group_task_metric_intervals(intervals)
⋮----
get_scalar_average = partial(
⋮----
metrics = itertools.chain.from_iterable(
⋮----
ret = defaultdict(dict)
⋮----
last_metrics = {}
cls_ = Model if model_events else Task
task = cls_.objects(id=task_id).only("last_metrics").first()
⋮----
last_metrics_data = last_metrics.get((metric_key, variant_key))
⋮----
"""
        Compare scalar metrics for different tasks per metric and variant
        The amount of points in each histogram should not exceed the requested samples
        """
⋮----
companies = {
⋮----
get_scalar_average_per_iter = partial(
⋮----
task_metrics = zip(
⋮----
task_names = {
res = defaultdict(lambda: defaultdict(dict))
⋮----
task_name = task_names[task_id]
⋮----
"""
        For the requested tasks return all the events delivered for the single iteration (-2**31)
        """
⋮----
task_events = list(
⋮----
def _get_value(event: dict)
⋮----
must = [
⋮----
es_req = {
⋮----
es_res = search_company_events(
⋮----
MetricInterval = Tuple[str, str, int, int]
MetricIntervalGroup = Tuple[int, Sequence[Tuple[str, str]]]
⋮----
"""
        Group task metric intervals so that the following conditions are meat:
            - All the metrics in the same group have the same interval (with 10% rounding)
            - The amount of metrics in the group does not exceed MAX_AGGS_ELEMENTS_COUNT
            - The total count of samples in the group does not exceed MAX_SAMPLE_BUCKETS
        """
metric_interval_groups = []
interval_group = []
group_interval_upper_bound = 0
group_max_interval = 0
group_samples = 0
⋮----
group_max_interval = interval
group_interval_upper_bound = interval + int(interval * 0.1)
⋮----
group_max_interval = max(group_max_interval, interval)
⋮----
"""
        Calculate interval per task metric variant so that the resulting
        amount of points does not exceed sample.
        Return the list og metric variant intervals as the following tuple:
        (metric, variant, interval, samples)
        """
must = self._task_conditions(task_id)
⋮----
query = {"bool": {"must": must}}
search_args = dict(es=self.es, company_id=company_id, event_type=event_type)
⋮----
max_variants = int(max_variants // 2)
⋮----
es_res = search_company_events(body=es_req, **search_args)
⋮----
aggs_result = es_res.get("aggregations")
⋮----
"""
        Calculate index interval per metric_variant variant so that the
        total amount of intervals does not exceeds the samples
        Return the interval and resulting amount of intervals
        """
count = nested_get(data, ("count", "value"), default=0)
⋮----
min_index = nested_get(data, ("min_index", "value"), default=0)
max_index = nested_get(data, ("max_index", "value"), default=min_index)
index_range = max_index - min_index + 1
interval = max(1, math.ceil(float(index_range) / samples))
max_samples = math.ceil(float(index_range) / interval)
⋮----
MetricData = Tuple[str, dict]
⋮----
"""
        Retrieve scalar histograms per several metric variants that share the same interval
        """
⋮----
aggregation = self._add_aggregation_average(key.get_aggregation(interval))
query = self._get_task_metrics_query(task_id=task_id, metrics=metrics)
⋮----
metrics = [
⋮----
@staticmethod
    def _add_aggregation_average(aggregation)
⋮----
average_agg = {"avg_val": {"avg": {"field": "value"}}}
⋮----
@staticmethod
    def _task_conditions(task_id: str) -> list
⋮----
must = cls._task_conditions(task_id)
⋮----
should = [
⋮----
def get_multi_task_metrics(self, companies: TaskCompanies, event_type: EventType) -> Mapping[str, list]
⋮----
"""
        For the requested tasks return reported metrics and variants
        """
tasks_ids = {
⋮----
companies_res: Sequence = list(
⋮----
res = defaultdict(set)
⋮----
search_args = dict(
query = QueryBuilder.terms("task", task_ids)
⋮----
"""
        For the requested tasks return reported metrics per task
        """
⋮----
res = pool.map(
</file>

<file path="apiserver/bll/event/events_iterator.py">
@attr.s(auto_attribs=True)
class TaskEventsResult
⋮----
total_events: int = 0
next_scroll_id: str = None
events: list = attr.Factory(list)
⋮----
class EventsIterator
⋮----
def __init__(self, es: Elasticsearch)
⋮----
from_key_value = kwargs.pop("from_timestamp", from_key_value)
⋮----
res = TaskEventsResult()
⋮----
es_req = {
⋮----
es_result = count_company_events(
⋮----
"""
        Return up to 'batch size' events starting from the previous key-field value (timestamp or iter) either in the
        direction of earlier events (navigate_earlier=True) or in the direction of later events.
        If from_key_field is not set then start either from latest or earliest.
        For the last key-field value all the events are brought (even if the resulting size exceeds batch_size)
        so that events with this value will not be lost between the calls.
        """
⋮----
# retrieve the next batch of events
⋮----
es_result = search_company_events(
hits = es_result["hits"]["hits"]
hits_total = es_result["hits"]["total"]["value"]
⋮----
events = [hit["_source"] for hit in hits]
⋮----
# retrieve the events that match the last event timestamp
# but did not make it into the previous call due to batch_size limitation
⋮----
last_second_hits = es_result["hits"]["hits"]
⋮----
# if only one element is returned for the last timestamp
# then it is already present in the events
⋮----
already_present_ids = set(hit["_id"] for hit in hits)
last_second_events = [
⋮----
# return the list merged from original query results +
# leftovers from the last timestamp
⋮----
query = {"terms": {"task": task_ids}}
must = [query]
⋮----
must = [
query = {"bool": {"must": must}}
⋮----
class Scroll(jsonmodels.models.Base)
⋮----
def get_scroll_id(self) -> str
⋮----
@classmethod
    def from_scroll_id(cls, scroll_id: str)
</file>

<file path="apiserver/bll/event/history_debug_image_iterator.py">
class VariantState(Base)
⋮----
name: str = StringField(required=True)
metric: str = StringField(default=None)
min_iteration: int = IntField()
max_iteration: int = IntField()
⋮----
class DebugImageSampleState(Base, JsonSerializableMixin)
⋮----
id: str = StringField(required=True)
iteration: int = IntField()
variant: str = StringField()
task: str = StringField()
metric: str = StringField()
variant_states: Sequence[VariantState] = ListField([VariantState])
warning: str = StringField()
navigate_current_metric = BoolField(default=True)
⋮----
@attr.s(auto_attribs=True)
class VariantSampleResult(object)
⋮----
scroll_id: str = None
event: dict = None
min_iteration: int = None
max_iteration: int = None
⋮----
class HistoryDebugImageIterator
⋮----
event_type = EventType.metrics_image
⋮----
def __init__(self, redis: StrictRedis, es: Elasticsearch)
⋮----
"""
        Get the sample for next/prev variant on the current iteration
        If does not exist then try getting sample for the first/last variant from next/prev iteration
        """
res = VariantSampleResult(scroll_id=state_id)
state = self.cache_manager.get_state(state_id)
⋮----
event = self._get_next_for_another_iteration(
⋮----
# noinspection PyArgumentList
event = first(
⋮----
var_state = first(
⋮----
@staticmethod
    def _get_metric_conditions(variants: Sequence[VariantState]) -> dict
⋮----
metrics = bucketize(variants, key=attrgetter("metric"))
⋮----
def _get_variants_conditions(metric_variants: Sequence[VariantState]) -> dict
⋮----
variants_conditions = [
⋮----
metrics_conditions = [
⋮----
"""
        Get the sample for next (if navigate_earlier is False) or previous variant sorted by name for the same iteration
        Only variants for which the iteration falls into their valid range are considered
        Return None if no such variant or sample is found
        """
⋮----
variants = [
⋮----
variants = state.variant_states
⋮----
cmp = operator.lt if navigate_earlier else operator.gt
⋮----
must_conditions = [
order = "desc" if navigate_earlier else "asc"
es_req = {
⋮----
es_res = search_company_events(
⋮----
hits = nested_get(es_res, ("hits", "hits"))
⋮----
"""
        Get the sample for the first variant for the next iteration (if navigate_earlier is set to False)
        or from the last variant for the previous iteration (otherwise)
        The variants for which the sample falls in invalid range are discarded
        If no suitable sample is found then None is returned
        """
⋮----
range_operator = "lt"
order = "desc"
⋮----
range_operator = "gt"
order = "asc"
variants = variants
⋮----
"""
        Get the sample for the requested iteration or the latest before it
        If the iteration is not passed then get the latest event
        """
res = VariantSampleResult()
⋮----
def init_state(state_: DebugImageSampleState)
⋮----
def validate_state(state_: DebugImageSampleState)
⋮----
# fix old variant states:
⋮----
state: DebugImageSampleState
⋮----
def _reset_variant_states(self, company_id: str, state: DebugImageSampleState)
⋮----
metrics = self._get_metric_variant_iterations(
⋮----
"""
        Return valid min and max iterations that the task reported events of the required type
        """
must = [
⋮----
query = {"bool": {"must": must}}
⋮----
search_args = dict(
⋮----
max_variants = int(max_variants // 2)
es_req: dict = {
⋮----
# group by urls and choose the minimal iteration
# from all the maximal iterations per url
⋮----
# find max iteration for each url
⋮----
es_res = search_company_events(body=es_req, **search_args)
⋮----
def get_variant_data(variant_bucket: dict) -> Tuple[str, int, int]
⋮----
variant = variant_bucket["key"]
urls = nested_get(variant_bucket, ("urls", "buckets"))
min_iter = int(urls[0]["max_iter"]["value"])
max_iter = int(variant_bucket["last_iter"]["value"])
</file>

<file path="apiserver/bll/event/history_plots_iterator.py">
class MetricState(Base)
⋮----
name: str = StringField(default=None)
min_iteration: int = IntField()
max_iteration: int = IntField()
⋮----
class PlotsSampleState(Base, JsonSerializableMixin)
⋮----
id: str = StringField(required=True)
iteration: int = IntField()
task: str = StringField()
metric: str = StringField()
metric_states: Sequence[MetricState] = ListField([MetricState])
warning: str = StringField()
navigate_current_metric = BoolField(default=True)
⋮----
@attr.s(auto_attribs=True)
class MetricSamplesResult(object)
⋮----
scroll_id: str = None
events: list = []
min_iteration: int = None
max_iteration: int = None
⋮----
class HistoryPlotsIterator
⋮----
event_type = EventType.metrics_plot
⋮----
def __init__(self, redis: StrictRedis, es: Elasticsearch)
⋮----
"""
        Get the samples for next/prev metric on the current iteration
        If does not exist then try getting sample for the first/last metric from next/prev iteration
        """
res = MetricSamplesResult(scroll_id=state_id)
state = self.cache_manager.get_state(state_id)
⋮----
range_operator = "lt"
order = "desc"
⋮----
range_operator = "gt"
order = "asc"
⋮----
must_conditions = [
⋮----
next_iteration_condition = {
⋮----
next_metric_condition = {
⋮----
events = self._get_metric_events_for_condition(
⋮----
"""
        Get the sample for the requested iteration or the latest before it
        If the iteration is not passed then get the latest event
        """
res = MetricSamplesResult()
⋮----
def init_state(state_: PlotsSampleState)
⋮----
def validate_state(state_: PlotsSampleState)
⋮----
state: PlotsSampleState
⋮----
metric_state = first(ms for ms in state.metric_states if ms.name == metric)
⋮----
def _reset_metric_states(self, company_id: str, state: PlotsSampleState)
⋮----
metrics = self._get_metric_iterations(
⋮----
"""
        Return valid min and max iterations that the task reported events of the required type
        """
must = [
⋮----
query = {"bool": {"must": must}}
⋮----
es_req: dict = {
⋮----
es_res = search_company_events(
⋮----
metric_state = first(
⋮----
es_req = {
⋮----
aggs_result = es_res.get("aggregations")
⋮----
level_data = aggs_result[level]["buckets"]
⋮----
aggs_result = level_data[0]
</file>

<file path="apiserver/bll/event/metric_debug_images_iterator.py">
class MetricDebugImagesIterator(MetricEventsIterator)
⋮----
def __init__(self, redis: StrictRedis, es: Elasticsearch)
⋮----
def _get_extra_conditions(self) -> Sequence[dict]
⋮----
def _get_variant_state_aggs(self) -> Tuple[dict, Callable[[dict, VariantState], None]]
⋮----
aggs = {
⋮----
"size": 1,  # we need only one url from the most recent iteration
⋮----
"size": 2,  # need two last iterations so that we can take
# the second one as invalid
⋮----
def fill_variant_state_data(variant_bucket: dict,  state: VariantState)
⋮----
"""If the image urls get recycled then fill the last_invalid_iteration field"""
top_iter_url = nested_get(variant_bucket, ("urls", "buckets"))[0]
iters = nested_get(top_iter_url, ("iters", "hits", "hits"))
⋮----
def _process_event(self, event: dict) -> dict
⋮----
def _get_same_variant_events_order(self) -> dict
</file>

<file path="apiserver/bll/event/metric_events_iterator.py">
class VariantState(Base)
⋮----
variant: str = StringField(required=True)
last_invalid_iteration: int = IntField()
⋮----
class MetricState(Base)
⋮----
metric: str = StringField(required=True)
variants: Sequence[VariantState] = ListField([VariantState], required=True)
timestamp: int = IntField(default=0)
⋮----
class TaskScrollState(Base)
⋮----
task: str = StringField(required=True)
metrics: Sequence[MetricState] = ListField([MetricState], required=True)
last_min_iter: Optional[int] = IntField()
last_max_iter: Optional[int] = IntField()
⋮----
def reset(self)
⋮----
"""Reset the scrolling state for the metric"""
⋮----
class MetricEventsScrollState(Base, JsonSerializableMixin)
⋮----
id: str = StringField(required=True)
tasks: Sequence[TaskScrollState] = ListField([TaskScrollState])
warning: str = StringField()
⋮----
@attr.s(auto_attribs=True)
class MetricEventsResult(object)
⋮----
metric_events: Sequence[tuple] = []
next_scroll_id: str = None
⋮----
class MetricEventsIterator
⋮----
def __init__(self, redis: StrictRedis, es: Elasticsearch, event_type: EventType)
⋮----
companies = {
⋮----
def init_state(state_: MetricEventsScrollState)
⋮----
def validate_state(state_: MetricEventsScrollState)
⋮----
"""
            Validate that the metrics stored in the state are the same
            as requested in the current call.
            Refresh the state if requested
            """
⋮----
res = MetricEventsResult(next_scroll_id=state.id)
specific_variants_requested = any(
⋮----
"""
        Determine the metrics for which new event_type events were added
        since their states were initialized and re-init these states
        """
tasks = Task.objects(id__in=list(task_metrics)).only("id", "metric_stats")
⋮----
"""For metrics that reported event_type events get mapping of the metric name to the last update times"""
metric_stats: Mapping[str, MetricEventStats] = task.metric_stats
⋮----
requested_metrics = task_metrics[task.id]
⋮----
update_times = {
task_metric_states = {
task_metrics_to_recalc = {}
⋮----
old_metric_states = task_metric_states[task]
metrics_to_recalc = {
⋮----
updated_task_states = self._init_task_states(companies, task_metrics_to_recalc)
⋮----
task = old_state.task
updated_state = first(uts for uts in updates if uts.task == task)
⋮----
updated_metrics = [m.metric for m in updated_state.metrics]
⋮----
"""
        Returned initialized metric scroll stated for the requested task metrics
        """
⋮----
task_metric_states = pool.map(
⋮----
@abc.abstractmethod
    def _get_extra_conditions(self) -> Sequence[dict]
⋮----
"""
        Return metric scroll states for the task filled with the variant states
        for the variants that reported any event_type events
        """
⋮----
company_id = companies[task]
must = [{"term": {"task": task}}, *self._get_extra_conditions()]
⋮----
query = {"bool": {"must": must}}
⋮----
search_args = dict(
⋮----
max_variants = int(max_variants // 2)
⋮----
es_req: dict = {
⋮----
es_res = search_company_events(body=es_req, **search_args)
⋮----
def init_variant_state(variant: dict)
⋮----
"""
            Return new variant state for the passed variant bucket
            """
state = VariantState(variant=variant["key"])
⋮----
@abc.abstractmethod
    def _process_event(self, event: dict) -> dict
⋮----
@abc.abstractmethod
    def _get_same_variant_events_order(self) -> dict
⋮----
"""
        Return task metric events grouped by iterations
        Update task scroll state
        """
⋮----
# the first fetch is always from the latest iteration to the earlier ones
navigate_earlier = True
⋮----
must_conditions = [
⋮----
range_condition = None
⋮----
range_condition = {"lt": task_state.last_min_iter}
⋮----
range_condition = {"gt": task_state.last_max_iter}
⋮----
metrics_count = len(task_state.metrics)
max_variants = int(EventSettings.max_es_buckets / (metrics_count * iter_count))
es_req = {
⋮----
es_res = search_company_events(
⋮----
invalid_iterations = {
allow_uninitialized = (
⋮----
def is_valid_event(event: dict) -> bool
⋮----
key = event.get("metric"), event.get("variant")
⋮----
max_invalid = invalid_iterations[key]
⋮----
def get_iteration_events(it_: dict) -> Sequence
⋮----
iterations = []
⋮----
events = get_iteration_events(it)
</file>

<file path="apiserver/bll/event/metric_plots_iterator.py">
class MetricPlotsIterator(MetricEventsIterator)
⋮----
def __init__(self, redis: StrictRedis, es: Elasticsearch)
⋮----
def _get_extra_conditions(self) -> Sequence[dict]
⋮----
def _get_variant_state_aggs(self)
⋮----
def _process_event(self, event: dict) -> dict
⋮----
def _get_same_variant_events_order(self) -> dict
</file>

<file path="apiserver/bll/event/scalar_key.py">
"""
Module for polymorphism over different types of X axes in scalar aggregations
"""
⋮----
log = config.logger(__file__)
⋮----
class ScalarKeyEnum(StringEnum)
⋮----
"""
    String enum representing X axes key
    """
⋮----
iter = auto()
timestamp = auto()
iso_time = auto()
⋮----
class ScalarKey(ABC)
⋮----
"""
    Abstract scalar key
    """
⋮----
_enum_to_key = {}
bucket_key_key = "key"
⋮----
@property
@abstractmethod
    def enum_value(self) -> ScalarKeyEnum
⋮----
"""
        Enum value accepted in API requests
        """
⋮----
@property
@abstractmethod
    def name(self) -> str
⋮----
"""
        Key name. Used as arbitrary internal key in elasticsearch queries
        """
⋮----
@property
@abstractmethod
    def field(self) -> str
⋮----
"""
        Event key to aggregate by
        """
⋮----
@abstractmethod
    def get_aggregation(self, interval: int) -> dict
⋮----
"""
        Get aggregation for this type of key
        :param interval: elasticsearch aggregation interval
        """
⋮----
def __init_subclass__(cls, **kwargs)
⋮----
"""
        Save a mapping from enum values to key class
        """
⋮----
@classmethod
    def resolve(cls, key: ScalarKeyEnum)
⋮----
"""
        Create a key instance from enum instance
        """
⋮----
def get_iterations_data(self, iter_buckets: dict) -> dict
⋮----
"""
        Convert a list of bucket entries to `x`s array and `y`s array
        """
⋮----
def _get_iterations_data_single(self, iter_data)
⋮----
"""
        Extract x value and y value from a single bucket item
        """
⋮----
def cast_value(self, value: Any) -> Any
⋮----
"""Cast value to appropriate type"""
⋮----
class TimestampKey(ScalarKey)
⋮----
"""
    Aggregate by timestamp in milliseconds since epoch
    """
⋮----
name = "timestamp"
field = "timestamp"
enum_value = ScalarKeyEnum.timestamp
⋮----
def get_aggregation(self, interval: int) -> dict
⋮----
def cast_value(self, value: Any) -> int
⋮----
class IterKey(ScalarKey)
⋮----
"""
    Aggregate by iteration number
    """
⋮----
name = "iters"
field = "iter"
enum_value = ScalarKeyEnum.iter
⋮----
class ISOTimeKey(ScalarKey)
⋮----
"""
    Aggregate by time formatted as ISO strings
    """
⋮----
name = "iso_time"
⋮----
enum_value = ScalarKeyEnum.iso_time
bucket_key_key = "key_as_string"
</file>

<file path="apiserver/bll/model/__init__.py">
class ModelBLL
⋮----
query = dict(company=company_id, id=model_id)
qs = Model.objects(**query)
⋮----
qs = qs.only(*only_fields)
model = qs.first()
⋮----
model_ids = [model_ids] if isinstance(model_ids, str) else model_ids
ids = set(model_ids)
query = Q(id__in=ids)
⋮----
q = Model.get_many(
⋮----
q = q.only(*only)
⋮----
model = cls.get_company_model_by_id(company_id=company_id, model_id=model_id)
⋮----
user_id = identity.user
published_task = None
⋮----
task = (
⋮----
task_publish_res = publish_task_func(
published_task = ModelTaskPublishResponse(
⋮----
now = datetime.utcnow()
updated = model.update(
⋮----
model = cls.get_company_model_by_id(
deleted_model_id = f"{deleted_prefix}{model_id}"
⋮----
using_tasks = Task.objects(models__input__model=model_id).only("id")
⋮----
# update deleted model id in using tasks
⋮----
task = Task.objects(id=model.task).first()
⋮----
del_count = Model.objects(id=model_id, company=company_id).delete()
⋮----
@classmethod
    def archive_model(cls, model_id: str, company_id: str, user_id: str)
⋮----
archived = Model.objects(company=company_id, id=model_id).update(
⋮----
@classmethod
    def unarchive_model(cls, model_id: str, company_id: str, user_id: str)
⋮----
unarchived = Model.objects(company=company_id, id=model_id).update(
⋮----
result = Model.aggregate(
⋮----
last_update = last_update or datetime.utcnow()
updates = {
⋮----
raw_updates = {}
⋮----
ret = Model.objects(id=model_id).update_one(**updates)
</file>

<file path="apiserver/bll/model/metadata.py">
log = config.logger(__file__)
⋮----
class Metadata
⋮----
update_cmds = dict()
metadata = cls.metadata_from_api(items)
⋮----
@classmethod
    def delete_metadata(cls, obj: Document, keys: Sequence[str], **more_updates) -> int
⋮----
@staticmethod
    def _process_path(path: str)
⋮----
"""
        Frontend does a partial escaping on the path so the all '.' in key names are escaped
        Need to unescape and apply a full mongo escaping
        """
parts = path.split(".")
⋮----
@classmethod
    def escape_paths(cls, paths: Sequence[str]) -> Sequence[str]
⋮----
paths = [
⋮----
@classmethod
    def escape_query_parameters(cls, call_data: dict) -> dict
⋮----
keys = list(call_data)
call_data = {
⋮----
projection = GetMixin.get_projection(call_data)
⋮----
ordering = GetMixin.get_ordering(call_data)
</file>

<file path="apiserver/bll/organization/__init__.py">
log = config.logger(__file__)
⋮----
class Tags(Enum)
⋮----
Task = "task"
Model = "model"
⋮----
class OrgBLL
⋮----
def __init__(self, redis=None)
⋮----
updated = 0
last_changed = {
⋮----
projects = entity_cls.objects(company=company_id, id__in=entity_ids).distinct(
⋮----
tags_cache = self._get_tags_cache_for_entity(entity)
⋮----
ret = defaultdict(set)
⋮----
project_tags = tags_cache.get_tags(
⋮----
def reset_tags(self, company_id: str, entity: Tags, projects: Sequence[str])
⋮----
def _get_tags_cache_for_entity(self, entity: Tags) -> _TagsCache
</file>

<file path="apiserver/bll/organization/tags_cache.py">
log = config.logger(__file__)
_settings_prefix = "services.organization"
⋮----
class _TagsCache
⋮----
_tags_field = "tags"
_system_tags_field = "system_tags"
_dummy_tag = "__dummy__"
# prepend our list in redis with this tag since empty lists are auto deleted
⋮----
def __init__(self, db_cls: Union[Type[Model], Type[Task]], redis: Redis)
⋮----
@property
    def _tags_cache_expiration_seconds(self)
⋮----
query = Q(company=company_id)
⋮----
# else:
#     query &= Q(system_tags__nin=[EntityVisibility.hidden.value])
⋮----
"""
        Project None means 'from all company projects'
        The key is built in the way that scanning company keys for 'all company projects'
        will not return the keys related to the particular company projects and vice versa.
        So that we can have a fine grain control on what redis keys to invalidate
        """
filter_str = None
⋮----
filter_str = "_".join(
key_parts = [field, company_id, project, self.db_cls.__name__, filter_str]
⋮----
"""
        Get tags and optionally system tags for the company
        Return the dictionary of tags per tags field name
        The function retrieves both cached values from Redis in one call
        and re calculates any of them if missing in Redis
        """
fields = [self._tags_field]
⋮----
ret = {}
⋮----
redis_key = self._get_tags_cache_key(
cached_tags = self.redis.lrange(redis_key, 0, -1)
⋮----
tags = [c.decode() for c in cached_tags[1:]]
⋮----
tags = list(
⋮----
def update_tags(self, company_id: str, projects: Sequence[str], tags=None, system_tags=None)
⋮----
"""
        Updates tags. If reset is set then both tags and system_tags
        are recalculated. Otherwise only those that are not 'None'
        """
fields = [
⋮----
def reset_tags(self, company_id: str, projects: Sequence[str])
⋮----
redis_keys = list(
</file>

<file path="apiserver/bll/project/__init__.py">

</file>

<file path="apiserver/bll/project/project_bll.py">
log = config.logger(__file__)
max_depth = config.get("services.projects.sub_projects.max_depth", 10)
reports_project_name = ".reports"
datasets_project_name = ".datasets"
pipelines_project_name = ".pipelines"
reports_tag = "reports"
dataset_tag = "dataset"
pipeline_tag = "pipeline"
⋮----
class ProjectBLL
⋮----
child_classes = (Task, Model)
⋮----
"""
        Move all the tasks and sub projects from the source project to the destination
        Remove the source project
        Return the amounts of moved entities and subprojects + set of all the affected project ids
        """
⋮----
source = Project.get(company, source_id)
⋮----
destination = Project.get(company, destination_id)
⋮----
destination = None
⋮----
children = _get_sub_projects(
⋮----
moved_entities = 0
⋮----
moved_sub_projects = 0
⋮----
affected = {source.id, *(source.path or [])}
⋮----
current_depth = len(current.path) + 1
⋮----
"""
        Move project with its sub projects from its current location to the target one.
        If the target location does not exist then it will be created. If it exists then
        it should be writable. The source location should be writable too.
        Return the number of moved projects + set of all the affected project ids
        """
project = Project.get(company, project_id)
old_parent_id = project.parent
old_parent = (
⋮----
children = _get_sub_projects([project.id], _only=("id", "name", "path"))[
⋮----
new_parent = _ensure_project(company=company, user=user, name=new_location)
new_parent_id = new_parent.id if new_parent else None
⋮----
moved = _reposition_project_with_children(
⋮----
now = datetime.utcnow()
affected = set()
p: Project
⋮----
@classmethod
    def update(cls, company: str, project_id: str, **fields)
⋮----
project = Project.get_for_writing(company=company, id=project_id)
⋮----
new_name = fields.pop("name", None)
⋮----
# noinspection PyTypeChecker
⋮----
updated = project.update(upsert=False, **fields)
⋮----
old_name = project.name
⋮----
"""
        Create a new project.
        Returns project ID
        """
⋮----
existing = _get_writable_project_from_name(
⋮----
project = Project(
parent = _ensure_project(
⋮----
"""
        Find a project named `project_name` or create a new one.
        Returns project ID
        """
⋮----
project = Project.objects(company=company, id=project_id).only("id").first()
⋮----
project = Project.objects(company=company, name=project_name).only("id").first()
⋮----
"""
        Move a batch of entities to `project` or a project named `project_name` (create if does not exist)
        """
⋮----
project = cls.find_or_create(
⋮----
extra = {}
⋮----
task_count_statuses = get_options(TaskStatus)
⋮----
def add_state_to_filter(f: Mapping[str, Any]) -> Mapping[str, Any]
⋮----
f = f or {}
new_f = {k: v for k, v in f.items() if k != "system_tags"}
system_tags = [
⋮----
def project_task_fields()
⋮----
def task_status_counts_subquery() -> dict
⋮----
def get_status_condition(status) -> dict
⋮----
def completed_after_subquery(time_thresh: datetime)
⋮----
# the sum of
⋮----
# for each task
⋮----
# if completed after the time_thresh
⋮----
def max_started_subquery()
⋮----
def runtime_subquery()
⋮----
# if completed and started and completed > started
⋮----
# then: floor((completed - started) / 1000)
⋮----
time_thresh = datetime.utcnow() - timedelta(hours=24)
runtime_pipeline = [
⋮----
# only count run time for these types of tasks
⋮----
# for each project
⋮----
T = TypeVar("T")
⋮----
"""
        Given a list of project ids and data collected over these projects and their subprojects
        For each project aggregates the data from all of its subprojects
        """
aggregated = {}
⋮----
relevant_projects = {p.id for p in child_projects.get(pid, [])} | {pid}
relevant_data = [data for p, data in data.items() if p in relevant_projects]
⋮----
task_runtime_pipeline = [
⋮----
child_projects = _get_sub_projects(
⋮----
child_projects = {}
project_ids_with_children = set(project_ids)
⋮----
pipeline = [
res = entity_class.aggregate(pipeline)
⋮----
project_stats = {
⋮----
def concat_dataset_stats(a: dict, b: dict) -> dict
⋮----
top_project_stats = cls.aggregate_project_data(
⋮----
empty_stats = {"count": 0, "tags": []}
stats = {
⋮----
filter_ = filter_ or {}
filter_system_tags = filter_.get("system_tags")
⋮----
filter_system_tags = []
⋮----
runtime_pipeline = cls.make_projects_get_all_pipeline(
⋮----
runtime = {
⋮----
ret = {}
⋮----
val = max(a.get(key) or datetime.min, b.get(key) or datetime.min)
⋮----
val = a.get(key, 0) + b.get(key, 0)
⋮----
runtime = cls.aggregate_project_data(
⋮----
def get_project_info(project_id)
⋮----
project_runtime = runtime.get(project_id, {})
⋮----
def get_time_or_none(value)
⋮----
status_counts = {
⋮----
"""
        Get the set of user ids that created tasks/models in the given projects
        If project_ids is empty then all projects are examined
        If user_ids are passed then only subset of these users is returned
        """
query = Q(company=company)
⋮----
projects_query = query
⋮----
project_ids = _ids_with_children(project_ids)
⋮----
res = set(Project.objects(projects_query).distinct(field="user"))
⋮----
query = Q(company=company_id)
⋮----
tags = Project.objects(query).distinct("tags")
system_tags = (
⋮----
"""
        Get the projects ids matching children_condition (if passed) or where the passed user created any tasks
        including all the parents of these projects
        If project ids are specified then filter the results by these project ids
        """
⋮----
query = (
⋮----
project_query = None
⋮----
child_query = query & GetMixin.get_list_filter_query(
⋮----
child_query = query & GetMixin.get_list_field_query("tags", children_tags)
⋮----
child_query = query
⋮----
child_queries = {
⋮----
child_queries = {Task: child_query & Q(system_tags__in=[reports_tag])}
⋮----
project_query = query
child_queries = {entity_cls: query for entity_cls in cls.child_classes}
⋮----
ids_with_children = _ids_with_children(project_ids)
⋮----
res = (
⋮----
res = list(res)
⋮----
selected_project_ids = _ids_with_parents(res)
filtered_ids = (
⋮----
query = get_company_or_none_constraint(company)
⋮----
projects = _ids_with_children(projects)
⋮----
# else:
#     query &= Q(system_tags__nin=[EntityVisibility.hidden.value])
⋮----
"""
        Get list of unique parent tasks sorted by task name for the passed company projects
        If projects is None or empty then get parents for all the company tasks
        """
query = cls._get_project_query(
⋮----
parent_ids = set(Task.objects(query).distinct("parent"))
⋮----
parents: Sequence[dict] = Task.get_many_with_join(
⋮----
user_ids = entity_cls.objects(query).distinct(field="user")
⋮----
users = User.objects(id__in=user_ids).only("id", "name")
⋮----
@classmethod
    def get_task_types(cls, company, project_ids: Optional[Sequence]) -> set
⋮----
"""
        Return the list of unique task types used by company and public tasks
        If project ids passed then only tasks from these projects are considered
        """
query = cls._get_project_query(company, project_ids)
res = Task.objects(query).distinct(field="type")
⋮----
@classmethod
    def get_model_frameworks(cls, company, project_ids: Optional[Sequence]) -> Sequence
⋮----
"""
        Return the list of unique frameworks used by company and public models
        If project ids passed then only models from these projects are considered
        """
⋮----
conditions = {
⋮----
or_conditions = []
⋮----
helper = GetMixin.NewListFieldBucketHelper(
op = helper.global_operator
db_query = {op: helper.actions}
⋮----
helper = GetMixin.ListQueryFilter.from_data(field, field_filter)
db_query = helper.db_query
⋮----
field_conditions = {}
⋮----
value = list(set(values)) if isinstance(values, list) else values
⋮----
value = {f"${key}": value}
⋮----
"""
        Returns the amount of hyper datasets per requested project
        """
⋮----
datasets = {
⋮----
"""
        Returns the amount of datasets per requested project
        """
⋮----
"""
        Returns the amount of task/models per requested project
        Use separate aggregation calls on Task/Model instead of lookup
        aggregation on projects in order not to hit memory limits on large tasks
        """
⋮----
system_tags_filter = filter_.get("system_tags", [])
archived = EntityVisibility.archived.value
non_archived = f"-{EntityVisibility.archived.value}"
⋮----
filter_ = {k: v for k, v in filter_.items()}
⋮----
def get_agrregate_res(cls_: Type[AttributedDocument]) -> dict
⋮----
tasks = get_agrregate_res(Task)
models = get_agrregate_res(Model)
</file>

<file path="apiserver/bll/project/project_cleanup.py">
log = config.logger(__file__)
event_bll = EventBLL()
⋮----
@attr.s(auto_attribs=True)
class DeleteProjectResult
⋮----
deleted: int = 0
disassociated_tasks: int = 0
deleted_models: int = 0
deleted_tasks: int = 0
urls: TaskUrls = None
⋮----
project_ids = _ids_with_children([project_id])
pipeline_ids = list(
dataset_ids = list(
⋮----
def validate_project_delete(company: str, project_id: str)
⋮----
project = Project.get_for_writing(
⋮----
ret = {}
⋮----
pipelines_with_active_controllers = Task.objects(
⋮----
datasets_with_data = Task.objects(
⋮----
project_ids = list(set(project_ids) - set(pipeline_ids) - set(dataset_ids))
⋮----
in_project_query = Q(project__in=project_ids)
⋮----
query = (
⋮----
delete_external_artifacts = delete_external_artifacts and config.get(
⋮----
active_controllers = Task.objects(
⋮----
regular_projects = list(set(project_ids) - set(pipeline_ids) - set(dataset_ids))
⋮----
non_archived = cls.objects(
⋮----
disassociated = defaultdict(int)
⋮----
res = DeleteProjectResult(disassociated_tasks=disassociated[Task])
⋮----
event_urls = task_event_urls | model_event_urls
⋮----
scheduled = schedule_for_delete(
⋮----
res = DeleteProjectResult(
⋮----
affected = {*project_ids, *(project.path or [])}
⋮----
"""
    Delete only the task themselves and their non published version.
    Child models under the same project are deleted separately.
    Children tasks should be deleted in the same api call.
    If any child entities are left in another projects then updated their parent task to None
    """
tasks = Task.objects(project__in=projects).only("id", "execution__artifacts")
⋮----
task_ids = list({t.id for t in tasks})
now = datetime.utcnow()
⋮----
artifact_urls = set()
⋮----
event_urls = delete_task_events_and_collect_urls(
deleted = tasks.delete()
⋮----
"""
    Delete project models and update the tasks from other projects
    that reference them to reference None.
    """
models = Model.objects(project__in=projects).only("task", "id", "uri")
⋮----
model_ids = list({m.id for m in models})
deleted = "__DELETED__"
⋮----
model_tasks = list({m.task for m in models if m.task})
⋮----
# update published tasks
⋮----
# update unpublished tasks
⋮----
model_urls = {m.uri for m in models if m.uri}
⋮----
deleted = models.delete()
</file>

<file path="apiserver/bll/project/project_queries.py">
log = config.logger(__file__)
⋮----
class ProjectQueries
⋮----
def __init__(self, redis=None)
⋮----
"""
        If passed projects is None means top level projects
        If passed projects is empty means no project filtering
        """
⋮----
project_ids = _ids_with_children(project_ids)
⋮----
project_ids = [None]
⋮----
@staticmethod
    def _get_company_constraint(company_id: str, allow_public: bool = True) -> dict
⋮----
page = max(0, page)
page_size = max(1, page_size)
pipeline = [
⋮----
result = next(Task.aggregate(pipeline), None)
⋮----
total = 0
remaining = 0
results = []
⋮----
total = int(result.get("total", -1))
results = [
remaining = max(0, total - (len(results) + page * page_size))
⋮----
ParamValues = Tuple[int, Sequence[str]]
⋮----
cached = self.redis.get(key)
⋮----
data = json.loads(cached)
cached_last_update = datetime.fromtimestamp(data["last_update"])
⋮----
company_constraint = self._get_company_constraint(company_id, allow_public)
project_constraint = self._get_project_constraint(
key_path = f"hyperparams.{ParameterKeyEscaper.escape(section)}.{ParameterKeyEscaper.escape(name)}"
last_updated_task = (
⋮----
redis_key = "_".join(
last_update = last_updated_task.last_update or datetime.utcnow()
cached_res = self._get_cached_param_values(
⋮----
match_condition = {
⋮----
result = next(Task.aggregate(pipeline, collation=Task._numeric_locale), None)
⋮----
total = int(result.get("total", 0))
values = result.get("results", [])
⋮----
ttl = config.get("services.tasks.hyperparam_values.cache_ttl_sec", 86400)
cached = dict(last_update=last_update.timestamp(), total=total, values=values)
⋮----
entity_cls = Model if model_metrics else Task
result = entity_cls.aggregate(pipeline)
⋮----
result = next(Model.aggregate(pipeline), None)
⋮----
key_path = f"metadata.{ParameterKeyEscaper.escape(key)}"
last_updated_model = (
⋮----
redis_key = f"modelmetadata_values_{company_id}_{'_'.join(project_ids)}_{key}_{allow_public}_{page}_{page_size}"
last_update = last_updated_model.last_update or datetime.utcnow()
⋮----
result = next(Model.aggregate(pipeline, collation=Model._numeric_locale), None)
⋮----
ttl = config.get("services.models.metadata_values.cache_ttl_sec", 86400)
</file>

<file path="apiserver/bll/project/project_usages.py">
log = config.logger(__file__)
⋮----
class ProjectUsages
⋮----
_default_cpu_usage = config.get("services.queues.resource_usages.cpu")
_default_gpu_usage = config.get("services.queues.resource_usages.gpu")
_conf = config.get("services.organization.project_usages")
_excluded_task_tags = _conf.get("excluded_task_tags", [])
_excluded_task_types = _conf.get("excluded_task_types", [])
_exclude_app_parent_tasks = _conf.get("exclude_app_parent_tasks", True)
_exclude_tasks_without_queue = _conf.get("exclude_tasks_without_queue", True)
⋮----
_field_to_cls = {
_other_q_id = "_other_"
⋮----
"""
        The tasks that belong to the passed projects, have started field set
        and are either in_progress or competed after the start date.
        For the tasks that are not running but do not have completed field (for example failed tasks)
        we check for the status_changed field
        Since mongo date time is always stored as utc make sure to convert the incoming dates to UTC
        """
start = start.astimezone(timezone.utc)
end = end.astimezone(timezone.utc)
optional_filters = {}
⋮----
excluded_tags = [] if include_development else ["development"]
⋮----
@staticmethod
    def _reduce_task_fields(start: datetime, end: datetime) -> dict
⋮----
"""
        Leve the minimal need fields.
        Set started to the latest from started field and start parameter
        If the task is still running then set stopped to the current date
        otherwise to completed (or status_changed is completed does not exist)
        Since mongo date time is always stored as utc make sure to convert the incoming dates to UTC
        """
⋮----
@staticmethod
    def _merge_queue_resources()
⋮----
"""
        Merge with the referenced queue and bring its resources
        """
⋮----
@classmethod
    def _add_dates_range_per_task(cls, tz_str: str)
⋮----
"""
        Set cpu and gpu usage from the merged queue resources. Put default values if queue resources not defined
        Generate array of dates between start and end dates.
        Each date points to beginning of the day in the passed time zone
        """
⋮----
# $eq does not work here since 'undefined' value is separate from null
# for aggregation equality comparison
⋮----
@staticmethod
    def _split_by_date()
⋮----
"""
        Generate a separate document for each task and date
        """
⋮----
@staticmethod
    def _calc_task_usages_per_date(tz_str: str)
⋮----
"""
        For each task date calculate its running time and resource usages
        Running time is calculated as amount of seconds that the task was running during the date
        If the task started before the date and stopped after the date then the usage on that date is 24h (86400 sec)
        Otherwise the exact running hours during the date are calculated
        """
⋮----
@staticmethod
    def _group_by_field(field: str)
⋮----
"""
        Group task date documents by the passed field and date
        For each date calculates the total amount of seconds
        """
⋮----
@staticmethod
    def _flatten_group_key()
⋮----
@classmethod
    def _group_by_dates(cls, breakdown_keys: Sequence[str])
⋮----
@classmethod
    def _get_names(cls, ids, cls_: Type[Document])
⋮----
q_names = {cls._other_q_id: "other"}
⋮----
@staticmethod
    def _get_date_from_iso_str(iso_str: str) -> date
⋮----
"""
        This api does not check permissions and intended for internal use only
        If to_date and from_date contain timezones then the calculation will be done
        for that timezone otherwise for UTC
        """
unsupported_breakdown_keys = set(breakdown_keys) - set(cls._field_to_cls)
⋮----
from_date = from_date.replace(tzinfo=timezone.utc)
⋮----
to_date = to_date.replace(tzinfo=timezone.utc)
request_tz_str = from_date.strftime("%z")
request_tz = from_date.tzinfo
start = datetime.combine(from_date, time.min, tzinfo=request_tz)
end = datetime.combine(to_date, time.max, tzinfo=request_tz)
⋮----
pipeline = [
res = list(Task.aggregate(pipeline))
⋮----
def get_parent_project_id(item: dict)
⋮----
p_id = item.get("key")
⋮----
allowed_queues = list(
⋮----
def get_queue_id(item: dict)
⋮----
"""
            If task is not enqueued or queue_id in allowed_queues then return queue_id
            Otherwise return 'other' queue id
            """
q_id = item.get("key")
⋮----
ret = {}
key_funcs = {
⋮----
cls_ = cls._field_to_cls[field]
data = res[0][f"{field}s"]
key_func = key_funcs.get(cls_, lambda x: x.get("key", None))
data_by_key = bucketize(data, key=key_func)
key_values = set(data_by_key)
⋮----
names = cls._get_names(key_values, cls_)
⋮----
field_ret = {
⋮----
key_data = data_by_key.get(key)
⋮----
name = names.get(key, key)
total_usages = defaultdict(int)
dates = []
date_usages = defaultdict(list)
next_date = start
⋮----
# the mongo aggregation dates are always returned as UTC
# need to convert them to the requested timezone
current_date = (
⋮----
timestamp = current_date.timestamp()
⋮----
# since data from several queues or projects can collapse into the same one
# it is possible that we have several records with the same date for the same queue/project
# then need to sum the usages under the same date
⋮----
next_date = current_date + timedelta(days=1)
⋮----
more_params = {}
⋮----
def_field = f"def_{usage_type}"
⋮----
@staticmethod
    def fromisoformat(date_str: str) -> datetime
⋮----
"""
        Overcome python fromisoformat function limitations
        """
⋮----
date_str = date_str[:-1]
⋮----
"""
        Return project usage per day grouped by project, queue and user
        If several project ids were passed then group by those projects
        If no project ids passed then group by top level projects
        If only one project was passed then group by its top level children
        In the last 2 cases also calculate the usage for the tasks sitting directly under the root or passed project
        to_day_str and from_date_str should contain date-times in ISO format with the timezone
        """
breakdown_keys = breakdown_keys or (
usage_fields = usage_fields or (
from_date = cls.fromisoformat(from_date_str)
to_date = cls.fromisoformat(to_date_str)
⋮----
empty = {
⋮----
add_root = False
project_root = None
⋮----
add_root = True
project_root = project_ids[0]
project_ids = list(Project.objects(parent=project_root).scalar("id"))
⋮----
child_projects = _get_sub_projects(
project_ids_with_children = list(
⋮----
child_to_parent = {}
⋮----
child_to_parent = {
</file>

<file path="apiserver/bll/project/sub_projects.py">
name_separator = "/"
⋮----
def _get_project_depth(project_name: str) -> int
⋮----
def _validate_project_name(project_name: str, raise_if_empty=True) -> Tuple[str, str]
⋮----
"""
    Remove redundant '/' characters. Ensure that the project name is not empty
    Return the cleaned up project name and location
    """
name_parts = [p.strip() for p in project_name.split(name_separator) if p]
⋮----
def _get_basename_from_name(name: str, raise_on_blank: bool = True) -> str
⋮----
basename = name.split("/")[-1].strip()
⋮----
"""
    Makes sure that the project with the given name exists
    If needed auto-create the project and all the missing projects in the path to it
    Return the project
    """
⋮----
project = _get_writable_project_from_name(company, name)
⋮----
now = datetime.utcnow()
project = Project(
parent = _ensure_project(company, user, location, creation_params=creation_params)
⋮----
def _save_under_parent(project: Project, parent: Optional[Project])
⋮----
"""
    Save the project under the given parent project or top level (parent=None)
    Check that the project location matches the parent name
    """
⋮----
"""
    Return a project from name. If the project not found then return None
    """
qs = Project.objects(company__in=[company, ""], name=name)
⋮----
_only = ["company", *_only]
qs = qs.only(*_only)
projects = list(qs)
⋮----
project = first(p for p in projects if p.company == company)
⋮----
ProjectsChildren = Mapping[str, Sequence[Project]]
⋮----
"""
    Return the list of child projects of all the levels for the parent project ids
    """
query = dict(path__in=project_ids)
⋮----
qs = Project.objects(**query)
⋮----
_only = set(_only) | {"path"}
⋮----
subprojects = list(qs)
⋮----
def _ids_with_parents(project_ids: Sequence[str]) -> Sequence[str]
⋮----
"""
    Return project ids with all the parent projects
    """
projects = Project.objects(id__in=project_ids).only("id", "path")
parent_ids = set(itertools.chain.from_iterable(p.path for p in projects if p.path))
⋮----
def _ids_with_children(project_ids: Sequence[str]) -> Sequence[str]
⋮----
"""
    Return project ids with the ids of all the subprojects
    """
children_ids = Project.objects(path__in=project_ids).scalar("id")
⋮----
"""
    Update sub project names when the base project name changes
    Optionally update the paths
    """
updated = 0
⋮----
child_suffix = name_separator.join(
updates = {
⋮----
new_location = parent.name if parent else None
old_name = project.name
old_path = project.path
⋮----
moved = 1 + _update_subproject_names(
</file>

<file path="apiserver/bll/query/__init__.py">

</file>

<file path="apiserver/bll/query/builder.py">
log = config.logger(__file__)
⋮----
RANGE_IGNORE_VALUE = -1
⋮----
class Builder
⋮----
conditions = {}
⋮----
@staticmethod
    def terms(field: str, values: Iterable) -> dict
⋮----
@staticmethod
    def term(field: str, value) -> dict
</file>

<file path="apiserver/bll/queue/__init__.py">

</file>

<file path="apiserver/bll/queue/queue_bll.py">
log = config.logger(__file__)
MOVE_FIRST = "first"
MOVE_LAST = "last"
⋮----
class QueueBLL(object)
⋮----
def __init__(self, worker_bll: WorkerBLL = None, es: Elasticsearch = None)
⋮----
@property
    def metrics(self) -> QueueMetrics
⋮----
"""Creates a queue"""
⋮----
now = datetime.utcnow()
queue = Queue(
⋮----
qs = Queue.objects(name=queue_name, company=company_id)
⋮----
qs = qs.only(*only)
⋮----
@staticmethod
    def _get_task_entries_projection(max_task_entries: int) -> dict
⋮----
"""
        Get queue by id
        :raise errors.bad_request.InvalidQueueId: if the queue is not found
        """
⋮----
query = dict(id=queue_id, company=company_id)
qs = Queue.objects(**query)
⋮----
qs = qs.fields(**self._get_task_entries_projection(max_task_entries))
queue = qs.first()
⋮----
@classmethod
    def get_queue_with_task(cls, company_id: str, queue_id: str, task_id: str) -> Queue
⋮----
queue = Queue.objects(entries__task=task_id, **query).first()
⋮----
def get_default(self, company_id: str) -> Queue
⋮----
"""
        Get the default queue
        :raise errors.bad_request.NoDefaultQueue: if the default queue not found
        :raise errors.bad_request.MultipleDefaultQueues: if more than one default queue is found
        """
⋮----
res = Queue.objects(company=company_id, system_tags="default").only(
⋮----
"""
        Partial update of the queue from update_fields
        :raise errors.bad_request.InvalidQueueId: if the queue is not found
        :return: number of updated objects and updated fields dictionary
        """
⋮----
# validate the queue exists
⋮----
tasks = []
⋮----
task = Task.get(
⋮----
def delete(self, company_id: str, user_id: str, queue_id: str, force: bool) -> Sequence[str]
⋮----
"""
        Delete the queue
        :raise errors.bad_request.InvalidQueueId: if the queue is not found
        :raise errors.bad_request.QueueNotEmpty: if the queue is not empty and 'force' not set
        """
queue = self.get_by_id(company_id=company_id, queue_id=queue_id)
⋮----
tasks = self._update_task_status_on_removal_from_queue(
⋮----
"""Get all the queues according to the query"""
⋮----
def check_for_workers(self, company_id: str, queue_id: str) -> bool
⋮----
"""
        Get infos on all the company queues, including queue tasks and workers
        """
projection = Queue.get_extra_projection("entries.task.name")
⋮----
res = Queue.get_many_with_join(
⋮----
queue_workers = defaultdict(list)
⋮----
def add_task(self, company_id: str, queue_id: str, task_id: str) -> dict
⋮----
"""
        Add the task to the queue and return the queue update results
        :raise errors.bad_request.TaskAlreadyQueued: if the task is already in the queue
        :raise errors.bad_request.InvalidQueueOrTaskNotQueued: if the queue update operation failed
        """
⋮----
entry = Entry(added=datetime.utcnow(), task=task_id)
⋮----
res = Queue.objects(entries__task__ne=task_id, **query).update_one(
⋮----
"""
        Atomically pop and return the first task from the queue (or None)
        :raise errors.bad_request.InvalidQueueId: if the queue does not exist
        """
⋮----
queue = Queue.objects(
⋮----
queue = Queue.objects(company=company_id, id=queue_id).first()
⋮----
def remove_task(self, company_id: str, user_id: str, queue_id: str, task_id: str, update_task_status: bool = False) -> int
⋮----
"""
        Removes the task from the queue and returns the number of removed items
        :raise errors.bad_request.InvalidQueueOrTaskNotQueued: if the task is not found in the queue
        """
⋮----
queue = self.get_queue_with_task(
⋮----
entries_to_remove = [e for e in queue.entries if e.task == task_id]
⋮----
res = Queue.objects(entries__task=task_id, **query).update_one(
⋮----
"""
        Moves the task in the queue to the position calculated by pos_func
        Returns the updated task position in the queue
        """
⋮----
def get_queue_and_task_position()
⋮----
q = self.get_queue_with_task(
⋮----
new_position = 0
⋮----
new_position = len(queue.entries) - 1
⋮----
new_position = position + move_count
⋮----
without_entry = {
task_entry = {
⋮----
operations = [
⋮----
updated = Queue.objects(
⋮----
def count_entries(self, company: str, queue_id: str) -> Optional[int]
⋮----
res = next(
</file>

<file path="apiserver/bll/queue/queue_metrics.py">
log = config.logger(__file__)
_conf = config.get("services.queues")
_queue_metrics_key_pattern = "queue_metrics_{queue}"
redis = redman.connection("apiserver")
⋮----
class EsKeys
⋮----
WAITING_TIME_FIELD = "average_waiting_time"
QUEUE_LENGTH_FIELD = "queue_length"
TIMESTAMP_FIELD = "timestamp"
QUEUE_FIELD = "queue"
⋮----
class QueueMetrics
⋮----
def __init__(self, es: Elasticsearch)
⋮----
@staticmethod
    def _queue_metrics_prefix_for_company(company_id: str) -> str
⋮----
"""Returns the es index prefix for the company"""
⋮----
@staticmethod
    def _get_es_index_suffix()
⋮----
"""Get the index name suffix for storing current month data"""
⋮----
@staticmethod
    def _calc_avg_waiting_time(entries: Sequence[Entry]) -> float
⋮----
"""
        Calculate avg waiting time for the given tasks.
        Return 0 if the list is empty
        """
⋮----
now = datetime.utcnow()
total_waiting_in_secs = sum((now - e.added).total_seconds() for e in entries)
⋮----
def log_queue_metrics_to_es(self, company_id: str, queues: Sequence[Queue]) -> int
⋮----
"""
        Calculate and write queue statistics (avg waiting time and queue length) to Elastic
        :return: True if the write to es was successful, false otherwise
        """
es_index = (
⋮----
timestamp = es_factory.get_timestamp_millis()
⋮----
def make_doc(queue: Queue) -> dict
⋮----
entries = [e for e in queue.entries if e.added]
⋮----
logged = 0
⋮----
queue_doc = make_doc(q)
⋮----
redis_key = _queue_metrics_key_pattern.format(queue=q.id)
⋮----
def _log_current_metrics(self, company_id: str, queue_ids: Sequence[str])
⋮----
query = dict(company=company_id)
⋮----
# noinspection PyTypeChecker
⋮----
queues = Queue.objects(**query)
⋮----
def _search_company_metrics(self, company_id: str, es_req: dict) -> dict
⋮----
@classmethod
    def _get_dates_agg(cls, interval) -> dict
⋮----
"""
        Aggregation for building date histogram with internal grouping per queue.
        We are grouping by queue inside date histogram and not vice versa so that
        it will be easy to average between queue metrics inside each date bucket.
        Ignore empty buckets.
        """
⋮----
@classmethod
    def _get_top_waiting_agg(cls) -> dict
⋮----
"""
        Aggregation for getting max waiting time and the corresponding queue length
        inside each date->queue bucket
        """
⋮----
"""
        Get the company queue metrics in the specified time range.
        Returned as date histograms of average values per queue and metric type.
        The from_date is extended by 'metrics_before_from_date' seconds from
        queues.conf due to possibly small amount of points. The default extension is 3600s
        In case no queue ids are specified the avg across all the
        company queues is calculated for each metric
        """
⋮----
seconds_before = config.get("services.queues.metrics_before_from_date", 3600)
must_terms = [QueryBuilder.dates_range(from_date - seconds_before, to_date)]
⋮----
es_req = {
⋮----
res = self._search_company_metrics(company_id, es_req)
⋮----
date_metrics = [
chart_start = (from_date - interval) * 1000
⋮----
@classmethod
    def _datetime_histogram_per_queue(cls, date_metrics: Sequence[dict], chart_start=None) -> dict
⋮----
"""
        Build datetime histogram per queue from datetime histogram where every
        bucket contains all the queues metrics
        """
queues_data = defaultdict(list)
⋮----
timestamp = date_data["timestamp"]
⋮----
@classmethod
    def _average_datetime_histogram(cls, date_metrics: Sequence[dict], chart_start=None) -> dict
⋮----
"""
        Calculate weighted averages and total count for each bucket of date_metrics histogram.
        If for any queue the data is missing then take it from the previous bucket
        The result is returned as a dictionary with one key 'total'
        """
queues_total = []
last_values = {}
⋮----
date_metrics = date_data["queue_metrics"]
queue_metrics = {
⋮----
total_length = sum(m["queue_length"] for m in queue_metrics.values())
⋮----
total_average = sum(
⋮----
total_average = 0
⋮----
@classmethod
    def _extract_queue_metrics(cls, queue_buckets: Sequence[dict]) -> dict
⋮----
"""
        Extract ES data for single date and queue bucket
        """
queue_metrics = dict()
⋮----
res = queue_data["top_avg_waiting"]["hits"]["hits"][0]["_source"]
⋮----
class MetricsRefresher
⋮----
threads = ThreadsManager()
⋮----
@classproperty
    def watch_interval_sec(self)
⋮----
@classmethod
@threads.register("queue_metrics_refresh_watchdog", daemon=True)
    def start(cls, queue_metrics: QueueMetrics = None)
⋮----
queue_metrics = QueueBLL().metrics
⋮----
doc_time = 0
⋮----
redis_key = _queue_metrics_key_pattern.format(queue=queue.id)
data = redis.get(redis_key)
⋮----
queue_doc = json.loads(data)
doc_time = int(queue_doc.get(EsKeys.TIMESTAMP_FIELD))
</file>

<file path="apiserver/bll/redis_cache_manager.py">
T = TypeVar("T")
⋮----
def _do_nothing(_: T)
⋮----
class RedisCacheManager(Generic[T])
⋮----
"""
    Class for store/retrieve of state objects from redis

    self.state_class - class of the state
    self.redis - instance of redis
    self.expiration_interval - expiration interval in seconds
    """
⋮----
def set_state(self, state: T) -> None
⋮----
redis_key = self._get_redis_key(state.id)
⋮----
def get_state(self, state_id) -> Optional[T]
⋮----
redis_key = self._get_redis_key(state_id)
response = self.redis.get(redis_key)
⋮----
def delete_state(self, state_id) -> None
⋮----
def _get_redis_key(self, state_id)
⋮----
state = self.get_state(state_id) if state_id else None
⋮----
state = self.state_class(id=database.utils.id())
⋮----
"""
        Try to retrieve state with the given id from the Redis cache if yes then validates it
        If no then create a new one with randomly generated id
        Yield the state and write it back to redis once the user code block exits
        :param state_id: id of the state to retrieve
        :param init_state: user callback to init the newly created state
        If not passed then no init except for the id generation is done
        :param validate_state: user callback to validate the state if retrieved from cache
        Should throw an exception if the state is not valid. If not passed then no validation is done
        """
state = self.get_or_create_state_core(
</file>

<file path="apiserver/bll/serving/__init__.py">
log = config.logger(__file__)
⋮----
class ServingBLL
⋮----
def __init__(self, redis=None)
⋮----
@staticmethod
    def _get_url_key(company: str, url: str)
⋮----
@staticmethod
    def _get_container_key(company: str, container_id: str) -> str
⋮----
"""Build redis key from company and container_id"""
⋮----
def _save_serving_container_entry(self, entry: ServingContainerEntry)
⋮----
url_key = self._get_url_key(entry.company_id, entry.endpoint_url)
expiration = int(time()) + entry.register_timeout
container_item = {entry.key: expiration}
⋮----
# make sure that url set will not get stuck in redis
# indefinitely in case no more containers report to it
⋮----
"""
        Get a container entry for the provided container ID.
        """
key = self._get_container_key(company_id, container_id)
data = self.redis.get(key)
⋮----
entry = ServingContainerEntry.from_json(data)
⋮----
msg = "Failed parsing container entry"
⋮----
"""
        Register a serving container
        """
now = datetime.now(timezone.utc)
key = self._get_container_key(company_id, request.container_id)
entry = ServingContainerEntry(
⋮----
"""
        Unregister a serving container
        """
entry = self._get_serving_container_entry(company_id, container_id)
⋮----
res = self.redis.delete(key)
⋮----
"""
        Serving container status report
        """
container_id = report.container_id
⋮----
ip = ip or entry.ip
register_time = entry.register_time
register_timeout = entry.register_timeout
⋮----
ip = ip
register_time = now
register_timeout = int(
⋮----
keys = list(self.redis.scan_iter(self._get_container_key(company_id, "*")))
entries = []
⋮----
data = self.redis.mget(keys)
⋮----
@attr.s(auto_attribs=True)
    class Counter
⋮----
class AggType(Enum)
⋮----
avg = auto()
max = auto()
total = auto()
count = auto()
⋮----
name: str
field: str
agg_type: AggType
float_precision: int = None
⋮----
_max: Union[int, float, datetime] = attr.field(init=False, default=None)
_total: Union[int, float] = attr.field(init=False, default=0)
_count: int = attr.field(init=False, default=0)
⋮----
def add(self, entry: ServingContainerEntry)
⋮----
value = getattr(entry, self.field, None)
⋮----
def __call__(self)
⋮----
avg = self._total / self._count
⋮----
def _get_summary(self, entries: Sequence[ServingContainerEntry]) -> dict
⋮----
counters = [
⋮----
first_entry = entries[0]
ret = {
⋮----
def get_endpoints(self, company_id: str)
⋮----
"""
        Group instances by urls and return a summary for each url
        Do not return data for "loading" instances that have no url
        """
entries = self._get_all(company_id)
by_url = bucketize(entries, key=attrgetter("endpoint_url"))
⋮----
url_key = self._get_url_key(company_id, endpoint_url)
timestamp = int(time())
⋮----
container_keys = {key.decode() for key in self.redis.zrange(url_key, 0, -1)}
⋮----
found_keys = set()
data = self.redis.mget(container_keys) or []
⋮----
entry = ServingContainerEntry.from_json(d)
⋮----
missing_keys = container_keys - found_keys
⋮----
def get_loading_instances(self, company_id: str)
⋮----
entries = self._get_endpoint_entries(company_id, None)
⋮----
def get_endpoint_details(self, company_id, endpoint_url: str) -> dict
⋮----
entries = self._get_endpoint_entries(company_id, endpoint_url)
⋮----
instances = []
entry: ServingContainerEntry
⋮----
def get_machine_stats_data(machine_stats: MachineStats) -> dict
⋮----
ret = {"cpu_count": 0, "gpu_count": 0}
</file>

<file path="apiserver/bll/serving/stats.py">
class _AggregationType(Enum)
⋮----
avg = "avg"
sum = "sum"
⋮----
class ServingStats
⋮----
min_chart_interval = config.get("services.serving.min_chart_interval_sec", 40)
es: Elasticsearch = es_factory.connect("workers")
⋮----
@classmethod
    def _serving_stats_prefix(cls, company_id: str) -> str
⋮----
"""Returns the es index prefix for the company"""
⋮----
@staticmethod
    def _get_es_index_suffix()
⋮----
"""Get the index name suffix for storing current month data"""
⋮----
@staticmethod
    def _get_average_value(value) -> Tuple[Optional[float], Optional[int]]
⋮----
count = len(value)
⋮----
"""
        Actually writing the worker statistics to Elastic
        :return: The amount of logged documents
        """
company_id = entry.company_id
es_index = (
⋮----
entry_data = entry.to_struct()
doc = {
⋮----
stats = entry_data.get("machine_stats")
⋮----
@staticmethod
    def round_series(values: Sequence, koeff) -> list
⋮----
_mb_to_gb = 1 / 1024
agg_fields = {
⋮----
from_date = metrics_request.from_date
to_date = metrics_request.to_date
⋮----
metric_type = metrics_request.metric_type
agg_data = cls.agg_fields.get(metric_type)
⋮----
instance_sum_type = "sum_bucket"
⋮----
instance_sum_type = "avg_bucket"
⋮----
interval = max(metrics_request.interval, cls.min_chart_interval)
endpoint_url = metrics_request.endpoint_url
hist_ret = {
must_conditions = [
query = {"bool": {"must": must_conditions}}
es_index = f"{cls._serving_stats_prefix(company_id)}*"
res = cls.es.search(
instance_buckets = nested_get(res, ("aggregations", "instances", "buckets"))
⋮----
instance_keys = {ib["key"] for ib in instance_buckets}
⋮----
sample_func = "avg" if metric_type != MetricType.requests else "max"
aggs = {
hist_params = {}
⋮----
filter_path = None
⋮----
filter_path = "aggregations.dates.buckets.total_instances"
⋮----
data = cls.es.search(
agg_res = data.get("aggregations")
⋮----
dates_ = []
total = []
instances = defaultdict(list)
# remove last interval if it's incomplete. Allow 10% tolerance
last_valid_timestamp = (to_date - 0.9 * interval) * 1000
⋮----
date_ = point["key"]
⋮----
found_keys = set()
⋮----
koeff = multiplier if multiplier else 1.0
</file>

<file path="apiserver/bll/statistics/resource_monitor.py">
stat_threads = ThreadsManager("Statistics")
⋮----
@attr.s(auto_attribs=True)
class Sample
⋮----
cpu_usage: float = 0.0
mem_used_gb: float = 0
mem_free_gb: float = 0
⋮----
@classmethod
    def _apply(cls, op, *samples)
⋮----
def min(self, sample)
⋮----
def max(self, sample)
⋮----
def avg(self, sample, count)
⋮----
res = self._apply(lambda x: x * count, self)
res = self._apply(operator.add, res, sample)
res = self._apply(lambda x: x / (count + 1), res)
⋮----
@classmethod
    def get_current_sample(cls) -> "Sample"
⋮----
class ResourceMonitor
⋮----
class Accumulator
⋮----
def __init__(self)
⋮----
sample = Sample.get_current_sample()
⋮----
def add_sample(self, sample: Sample)
⋮----
sample_interval_sec = 5
_lock = Lock()
accumulator = Accumulator()
⋮----
@classmethod
@stat_threads.register("resource_monitor", daemon=True)
    def start(cls)
⋮----
@classmethod
    def get_stats(cls) -> dict
⋮----
""" Returns current resource statistics and clears internal resource statistics """
⋮----
min_ = attr.asdict(cls.accumulator.min)
max_ = attr.asdict(cls.accumulator.max)
avg = attr.asdict(cls.accumulator.avg)
interval = datetime.utcnow() - cls.accumulator.time
</file>

<file path="apiserver/bll/statistics/stats_reporter.py">
log = config.logger(__file__)
⋮----
worker_bll = WorkerBLL()
⋮----
class StatisticsReporter
⋮----
send_queue = queue.Queue()
supported = config.get("apiserver.statistics.supported", True)
⋮----
@classmethod
    def start(cls)
⋮----
@classmethod
@stat_threads.register("reporter", daemon=True)
    def start_reporter(cls)
⋮----
"""
        Periodically send statistics reports for companies who have opted in.
        Note: in clearml we usually have only a single company
        """
⋮----
report_interval = timedelta(
⋮----
stats = cls.get_statistics(company.id)
⋮----
@classmethod
@stat_threads.register("sender", daemon=True)
    def start_sender(cls)
⋮----
url = config.get("apiserver.statistics.url")
⋮----
retries = config.get("apiserver.statistics.max_retries", 5)
max_backoff = config.get("apiserver.statistics.max_backoff_sec", 5)
session = requests.Session()
adapter = HTTPAdapter(max_retries=Retry(retries))
⋮----
report = cls.send_queue.get()
⋮----
# Set a random backoff factor each time we send a report
⋮----
@classmethod
    def get_statistics(cls, company_id: str) -> dict
⋮----
"""
        Returns a statistics report per company
        """
⋮----
@classmethod
    def _get_agents_statistics(cls, company_id: str) -> Sequence[dict]
⋮----
result = cls._get_resource_stats_per_agent(company_id, key="resources")
⋮----
@classmethod
    def _get_resource_stats_per_agent(cls, company_id: str, key: str) -> dict
⋮----
agent_resource_threshold_sec = timedelta(
to_timestamp = int(time.time())
from_timestamp = to_timestamp - int(agent_resource_threshold_sec)
es_req = {
res = cls._run_worker_stats_query(company_id, es_req)
⋮----
def _get_cardinality_fields(categories: Sequence[dict]) -> dict
⋮----
names = {"cpu": "num_cores"}
⋮----
def _get_metric_fields(metrics: Sequence[dict]) -> dict
⋮----
names = {
⋮----
buckets = nested_get(res, ("aggregations", "workers", "buckets"), default=[])
⋮----
@classmethod
    def _get_experiments_stats_per_agent(cls, company_id: str, key: str) -> dict
⋮----
agent_relevant_threshold = timedelta(
⋮----
from_timestamp = to_timestamp - int(agent_relevant_threshold.total_seconds())
workers = cls._get_active_workers(company_id, from_timestamp, to_timestamp)
⋮----
stats = cls._get_experiments_stats(company_id, list(workers.keys()))
⋮----
@classmethod
    def _run_worker_stats_query(cls, company_id, es_req) -> dict
⋮----
pipeline = [
⋮----
class WarningFilter(logging.Filter)
⋮----
@classmethod
    def attach(cls)
⋮----
)  # required to make sure the logger is created
⋮----
assert ConnectionPool  # make sure import is not optimized out
⋮----
def filter(self, record)
</file>

<file path="apiserver/bll/storage/__init__.py">
log = config.logger(__file__)
⋮----
class StorageBLL
⋮----
default_aws_configs: S3BucketConfigurations = None
conf = config.get("services.storage_credentials")
⋮----
@cachedproperty
    def _default_aws_configs(self) -> S3BucketConfigurations
⋮----
@cachedproperty
    def _default_azure_configs(self) -> AzureContainerConfigurations
⋮----
@cachedproperty
    def _default_gs_configs(self) -> GSBucketConfigurations
⋮----
db_settings = (
⋮----
azure = db_settings.azure
⋮----
buckets = [
⋮----
buckets = self._default_gs_configs._buckets
⋮----
def get_bucket_config(bc: GoogleBucketSettings) -> GSBucketConfig
⋮----
data = bc.to_proper_dict()
⋮----
google = db_settings.google
buckets_configs = [get_bucket_config(b) for b in (google.buckets or [])]
⋮----
aws = db_settings.aws
buckets_configs = S3BucketConfig.from_list(
⋮----
def _assure_json_file(self, name_or_content: str) -> str
⋮----
def _assure_json_string(self, name_or_content: str) -> Optional[str]
⋮----
def get_company_settings(self, company_id: str) -> dict
⋮----
db_settings = StorageSettings.objects(company=company_id).first()
aws = self.get_aws_settings_for_company(company_id, db_settings, query_db=False)
aws_dict = {
⋮----
gs = self.get_gs_settings_for_company(
gs_dict = {
⋮----
azure = self.get_azure_settings_for_company(company_id, db_settings)
azure_dict = {
⋮----
update_dict = {}
⋮----
cred_json = update_dict["google"].get("credentials_json")
⋮----
settings = StorageSettings.objects(company=company_id).only("id").first()
settings_id = settings.id if settings else db_id()
⋮----
def reset_company_settings(self, company_id: str, keys: Sequence[str]) -> int
</file>

<file path="apiserver/bll/task/__init__.py">

</file>

<file path="apiserver/bll/task/artifacts.py">
def get_artifact_id(artifact: dict)
⋮----
"""
    Calculate id from 'key' and 'mode' fields
    Return hash on on the id so that it will not contain mongo illegal characters
    """
key_hash: str = hash_field_name(artifact["key"])
mode: str = artifact.get("mode", DEFAULT_ARTIFACT_MODE)
⋮----
def artifacts_prepare_for_save(fields: dict)
⋮----
artifacts_field = ("execution", "artifacts")
artifacts = nested_get(fields, artifacts_field)
⋮----
def artifacts_unprepare_from_saved(fields)
⋮----
class Artifacts
⋮----
task = get_task_for_update(
⋮----
artifacts = {
⋮----
update_cmds = {
⋮----
artifact_ids = [
delete_cmds = {
</file>

<file path="apiserver/bll/task/hyperparams.py">
log = config.logger(__file__)
task_bll = TaskBLL()
⋮----
class HyperParams
⋮----
_properties_section = "properties"
⋮----
@classmethod
    def get_params(cls, company_id: str, task_ids: Sequence[str]) -> Dict[str, dict]
⋮----
only = ("id", "hyperparams")
tasks = task_bll.assert_exists(
⋮----
ret = list(chain.from_iterable(v.values() for v in items.values()))
⋮----
@classmethod
    def _normalize_params(cls, params: Sequence) -> bool
⋮----
"""
        Lower case properties section and return True if it is the only section
        """
⋮----
properties_only = cls._normalize_params(hyperparams)
task = get_task_for_update(
⋮----
sections_to_delete = {p.section for p in without_param}
delete_cmds = {
⋮----
section = ParameterKeyEscaper.escape(item.section)
⋮----
name = ParameterKeyEscaper.escape(item.name)
⋮----
update_cmds = dict()
hyperparams = cls._db_dicts_from_list(hyperparams)
⋮----
@classmethod
    def _db_dicts_from_list(cls, items: Sequence[HyperParamItem]) -> Dict[str, dict]
⋮----
sections = iterutils.bucketize(items, key=attrgetter("section"))
⋮----
only = ["id"]
⋮----
skip_empty_condition = {"$match": {"items.v.value": {"$nin": [None, ""]}}}
pipeline = [
⋮----
tasks = Task.aggregate(pipeline)
⋮----
configuration = {
</file>

<file path="apiserver/bll/task/non_responsive_tasks_watchdog.py">
log = config.logger(__file__)
⋮----
class NonResponsiveTasksWatchdog
⋮----
threads = ThreadsManager()
⋮----
class _Settings
⋮----
"""
        Retrieves watchdog settings from the config file
        The properties are not cached so that the updates in
        the config file are reflected
        """
⋮----
_prefix = "services.tasks.non_responsive_tasks_watchdog"
⋮----
@property
        def enabled(self)
⋮----
@property
        def watch_interval_sec(self)
⋮----
@property
        def threshold_sec(self)
⋮----
settings = _Settings()
⋮----
@classmethod
@threads.register("non_responsive_tasks_watchdog", daemon=True)
    def start(cls)
⋮----
watch_interval = cls.settings.watch_interval_sec
⋮----
stopped = cls.cleanup_tasks(
⋮----
@classmethod
    def cleanup_tasks(cls, threshold_sec)
⋮----
relevant_status = (TaskStatus.in_progress,)
threshold = timedelta(seconds=threshold_sec)
ref_time = datetime.utcnow() - threshold
⋮----
tasks = list(
⋮----
err_count = 0
project_ids = set()
now = datetime.utcnow()
⋮----
# noinspection PyBroadException
⋮----
updated = Task.objects(id=task.id, status=task.status).update(
</file>

<file path="apiserver/bll/task/param_utils.py">
hyperparams_default_section = "Args"
hyperparams_legacy_type = "legacy"
tf_define_section = "TF_DEFINE"
⋮----
def split_param_name(full_name: str, default_section: str) -> Tuple[Optional[str], str]
⋮----
"""
    Return parameter section and name. The section is either TF_DEFINE or the default one
    """
⋮----
def _get_full_param_name(param: dict) -> str
⋮----
section = param.get("section")
⋮----
def _remove_legacy_params(data: dict, with_sections: bool = False) -> int
⋮----
"""
    Remove the legacy params from the data dict and return the number of removed params
    If the path not found then return 0
    """
removed = 0
⋮----
"""If section is empty after removing legacy params then delete it"""
⋮----
def _get_legacy_params(data: dict, with_sections: bool = False) -> Sequence[dict]
⋮----
def params_prepare_for_save(fields: dict, previous_task: Task = None)
⋮----
"""
    If legacy hyper params or configuration is passed then replace the corresponding section in the new structure
    Escape all the section and param names for hyper params and configuration to make it mongo sage
    """
⋮----
legacy_params = nested_get(fields, old_params_field)
⋮----
previous_data = previous_task.to_proper_dict().get(new_params_field)
removed = _remove_legacy_params(
⋮----
# if we only need to delete legacy fields from the db
# but they are not there then there is no point to proceed
⋮----
fields_update = {new_params_field: previous_data}
⋮----
new_path = list(filter(None, (new_params_field, section, name)))
new_param = dict(name=name, type=hyperparams_legacy_type, value=str(value))
⋮----
def ensure_non_empty(k: str, desc: str) -> str
⋮----
params = fields.get("hyperparams")
⋮----
escaped_params = {
⋮----
params = fields.get("configuration")
⋮----
def params_unprepare_from_saved(fields, copy_to_legacy=False)
⋮----
"""
    Unescape all section and param names for hyper params and configuration
    If copy_to_legacy is set then copy hyperparams and configuration data to the legacy location for the old clients
    """
⋮----
params = fields.get(param_field)
⋮----
unescaped_params = {
⋮----
legacy_params = _get_legacy_params(
⋮----
def _process_path(path: str)
⋮----
"""
    Frontend does a partial escaping on the path so the all '.' in section and key names are escaped
    Need to unescape and apply a full mongo escaping
    """
parts = path.split(".")
⋮----
def escape_paths(paths: Sequence[str]) -> Sequence[str]
⋮----
path: str
paths = [path.replace(old_prefix, new_prefix) for path in paths]
⋮----
paths = [
</file>

<file path="apiserver/bll/task/task_bll.py">
log = config.logger(__file__)
org_bll = OrgBLL()
queue_bll = QueueBLL()
project_bll = ProjectBLL()
⋮----
class TaskBLL
⋮----
def __init__(self, events_es=None, redis=None)
⋮----
only_fields = [only_fields]
⋮----
only_fields = list(only_fields)
only_fields = only_fields + ["status"]
⋮----
tasks = Task.get_many(
task = None if not tasks else tasks[0]
⋮----
task = get_task_for_update(
⋮----
update_cmds = {
⋮----
task_ids = [task_ids] if isinstance(task_ids, six.string_types) else task_ids
⋮----
ids = set(task_ids)
q = Task.get_many(
⋮----
# Make sure to reset fields filters (some fields are excluded by default) since this
# is an internal call and specific fields were requested.
q = q.all_fields().only(*only)
⋮----
@staticmethod
    def create(company: str, user: str, fields: dict)
⋮----
now = datetime.now(timezone.utc)
⋮----
@staticmethod
    def validate_input_models(task, allow_only_public=False)
⋮----
company = None if allow_only_public else task.company
model_ids = set(m.model for m in task.models.input)
models = Model.objects(
missing = model_ids - {m.id for m in models}
⋮----
task: Task = cls.get_by_id(
⋮----
params_dict = {}
⋮----
updated_hyperparams = {
⋮----
updated_configuration = {
⋮----
input_models = [
⋮----
execution_dict = task.execution.to_proper_dict() if task.execution else {}
⋮----
execution_model = execution_overrides.pop("model", None)
⋮----
docker_cmd = execution_overrides.pop("docker_cmd", None)
⋮----
container = {"image": image, "arguments": arguments}
⋮----
legacy_value = execution_overrides.pop(legacy_param, None)
⋮----
artifacts = execution_dict.get("artifacts")
⋮----
new_project_data = None
⋮----
# Use a project with the provided name, or create a new project
project = ProjectBLL.find_or_create(
new_project_data = {"id": project, "name": new_project_name}
⋮----
def clean_system_tags(input_tags: Sequence[str]) -> Sequence[str]
⋮----
def ensure_int_labels(execution: dict) -> dict
⋮----
model_labels = execution.get("model_labels")
⋮----
parent_task = (
new_task = Task(
⋮----
updated_tags = tags
updated_system_tags = system_tags
⋮----
updated_tags = new_task.tags
updated_system_tags = new_task.system_tags
⋮----
"""
        Validate task properties according to the flag
        Task project is always checked for being writable
        in order to disable the modification of public projects
        """
⋮----
project = Project.get_for_writing(company=task.company, id=task.project)
⋮----
tasks = Task.objects(id__in=task_ids, company=company_id).only(
count = 0
⋮----
updates = extra_updates
⋮----
updates = {
⋮----
"""
        Update task statistics
        :param task_id: Task's ID.
        :param company_id: Task's company ID.
        :param last_update: Last update time. If not provided, defaults to datetime.utcnow().
        :param last_iteration: Last reported iteration. Use this to set a value regardless of current
            task's last iteration value.
        :param last_iteration_max: Last reported iteration. Use this to conditionally set a value only
            if the current task's last iteration value is smaller than the provided value.
        :param last_scalar_events: Last reported metrics summary for scalar events (value, metric, variant).
        :param last_events: Last reported metrics summary (value, metric, event type).
        :param extra_updates: Extra task updates to include in this update call.
        :return:
        """
last_update = last_update or datetime.now(timezone.utc)
⋮----
raw_updates = {}
⋮----
def events_per_type(metric_data_: Dict[str, dict]) -> Dict[str, EventStats]
⋮----
metric_stats = {
⋮----
ret = TaskBLL.set_last_update(
⋮----
more = {}
⋮----
# dequeue may fail if the queue was deleted
⋮----
new_status = new_status_for_aborted_task
⋮----
@classmethod
    def dequeue(cls, task: Task, company_id: str, user_id: str, silent_fail=False)
⋮----
"""
        Dequeue the task from the queue
        :param task: task to dequeue
        :param company_id: task's company ID.
        :param silent_fail: do not throw exceptions. APIError is still thrown
        :raise errors.bad_request.InvalidTaskId: if the task's status is not queued
        :raise errors.bad_request.MissingRequiredFields: if the task is not queued
        :raise APIError or errors.server_error.TransactionError: if internal call to queues.remove_task fails
        :return: the result of queues.remove_task call. None in case of silent failure
        """
</file>

<file path="apiserver/bll/task/task_cleanup.py">
log = config.logger(__file__)
event_bll = EventBLL()
⋮----
@attr.s(auto_attribs=True)
class TaskUrls
⋮----
model_urls: Sequence[str]
artifact_urls: Sequence[str]
event_urls: Sequence[str] = []  # left here is in order not to break the api
⋮----
def __add__(self, other: "TaskUrls")
⋮----
@attr.s(auto_attribs=True)
class CleanupResult
⋮----
"""
    Counts of objects modified in task cleanup operation
    """
⋮----
updated_children: int
updated_models: int
deleted_models: int
deleted_model_ids: Set[str]
urls: TaskUrls = None
⋮----
def to_res_dict(self, return_file_urls: bool) -> dict
⋮----
remove_fields = ["deleted_model_ids"]
⋮----
# noinspection PyTypeChecker
res = attr.asdict(
⋮----
def __add__(self, other: "CleanupResult")
⋮----
@staticmethod
    def empty()
⋮----
urls = set()
task_ids = task_or_model if isinstance(task_or_model, list) else [task_or_model]
⋮----
next_scroll_id = None
⋮----
event_urls = event.get(PlotFields.source_urls)
⋮----
"""
    Return the set of unique image urls
    Uses DebugImagesIterator to make sure that we do not retrieve recycled urls
    """
⋮----
after_key = None
⋮----
supported_storage_types = {
⋮----
urls_per_storage = bucketize(
⋮----
processed_urls = set()
⋮----
delete_folders = (storage_type == StorageType.fileserver) and can_delete_folders
scheduled_to_delete = set()
⋮----
folder = None
⋮----
parsed = furl(url)
⋮----
folder = parsed.remove(
⋮----
to_delete = folder or url
⋮----
existing = UrlToDelete.objects(company=company, url=to_delete).first()
⋮----
event_urls = collect_debug_image_urls(company, task_ids) | collect_plot_image_urls(
⋮----
"""
    Validate task deletion and delete/modify all its output.
    :param task: task object
    :param force: whether to delete task with published outputs
    :return: count of delete and modified items
    """
⋮----
artifact_urls = (
model_urls = {m.uri for m in draft_models if m.uri and m.id not in in_use_model_ids}
⋮----
deleted_task_id = f"{deleted_prefix}{task.id}"
updated_children = 0
now = datetime.utcnow()
⋮----
updated_children = Task.objects(parent=task.id).update(
⋮----
deleted_models = 0
updated_models = 0
deleted_model_ids = set()
⋮----
model_ids = list({m.id for m in models if m.id not in in_use_model_ids})
⋮----
published_children_count = Task.objects(
⋮----
model_fields = ["id", "ready", "uri"]
⋮----
model_ids = [m.model for m in task.models.output]
⋮----
in_use_model_ids = {}
⋮----
model_ids = {m.id for m in draft_models}
dependent_tasks = Task.objects(models__input__model__in=list(model_ids)).only(
in_use_model_ids = model_ids & {
</file>

<file path="apiserver/bll/task/task_operations.py">
log = config.logger(__file__)
queue_bll = QueueBLL()
⋮----
query = Task.objects(company=company_id, parent=task.id)
⋮----
query = query.only(*only)
⋮----
"""
    Deque and archive task
    Return 1 if successful
    """
user_id = identity.user
fields = (
⋮----
task = get_task_with_write_access(
⋮----
def archive_task_core(task_: Task) -> int
⋮----
# dequeue may fail if the task was not enqueued
⋮----
"""
    Unarchive task. Return 1 if successful
    """
fields = ("id", "type")
⋮----
def unarchive_task_core(task_: Task) -> int
⋮----
# get the task without write access to make sure that it actually exists
task = Task.get(
⋮----
res = TaskBLL.dequeue_and_change_status(
⋮----
queue = queue_bll.get_by_name(
⋮----
queue = queue_bll.create(company_id=company_id, name=queue_name)
queue_id = queue.id
⋮----
# try to get default queue
queue_id = queue_bll.get_default(company_id).id
⋮----
before_enqueue_status = task.status
⋮----
before_enqueue_status = task.enqueue_status
res = ChangeStatusRequest(
⋮----
# failed enqueueing, revert to previous state
⋮----
# set the current queue ID in the task
⋮----
# make sure that the task is not queued in any other queue
⋮----
def move_tasks_to_trash(tasks: Sequence[str]) -> int
⋮----
collection_name = Task._get_collection_name()
trash_collection_name = f"{collection_name}__trash"
⋮----
task = get_task_with_write_access(task_id, company_id=company_id, identity=identity)
⋮----
def delete_task_core(task_: Task, force_: bool) -> CleanupResult
⋮----
res = cleanup_task(
⋮----
# make sure that whatever changes were done to the task are saved
# the task itself will be deleted later in the move_tasks_to_trash operation
⋮----
task_ids = [task.id]
cleanup_res = CleanupResult.empty()
⋮----
cleanup_res = delete_task_core(task, force)
⋮----
dequeued = {}
updates = {}
⋮----
dequeued = TaskBLL.dequeue(
⋮----
cleaned_up = cleanup_task(
⋮----
previous_task_status = task.status
output = task.output or Output()
publish_failed = False
⋮----
# set state to publishing
⋮----
# publish task models
⋮----
model_id = task.models.output[-1].model
model = (
⋮----
# set task status to published, and update (or set) it's new output (view and models)
⋮----
publish_failed = True
⋮----
"""
    Stop a running task. Requires task status 'in_progress' and
    execution_progress 'running', or force=True. Development task or
    task that has no associated worker is stopped immediately.
    For a non-development task with worker only the status message
    is set to 'stopping' to allow the worker to stop the task and report by itself
    :return: updated task fields
    """
⋮----
def is_run_by_worker(t: Task) -> bool
⋮----
"""Checks if there is an active worker running the task"""
update_timeout = config.get("apiserver.workers.task_update_timeout", 600)
⋮----
def stop_task_core(task_: Task, force_: bool)
⋮----
is_queued = task_.status == TaskStatus.queued
set_stopped = (
⋮----
new_status = TaskStatus.stopped
status_message = f"Stopped by {user_name}"
⋮----
new_status = task_.status
status_message = TaskStatusMessage.stopping
</file>

<file path="apiserver/bll/task/utils.py">
valid_statuses = get_options(TaskStatus)
deleted_prefix = "__DELETED__"
task_running_statuses = [TaskStatus.queued, TaskStatus.in_progress]
⋮----
@typed_attrs
class ChangeStatusRequest(object)
⋮----
task = attr.ib(type=Task)
new_status = attr.ib(
status_reason = attr.ib(type=six.string_types, default="")
status_message = attr.ib(type=six.string_types, default="")
force = attr.ib(type=bool, default=False)
allow_same_state_transition = attr.ib(type=bool, default=True)
current_status_override = attr.ib(default=None)
user_id = attr.ib(type=str, default=None)
⋮----
def execute(self, **kwargs)
⋮----
current_status = self.current_status_override or self.task.status
project_id = self.task.project
⋮----
# Verify new status is allowed from current status (will throw exception if not valid)
⋮----
control = dict(upsert=False, multi=False, write_concern=None, full_result=False)
⋮----
now = datetime.now(timezone.utc)
fields = dict(
⋮----
def safe_mongoengine_key(key)
⋮----
# atomic change of task status by querying the task with the EXPECTED status before modifying it
params = fields.copy()
⋮----
updated = Task.objects(id=self.task.id, status=current_status).update(
⋮----
# failed to change status (someone else beat us to it?)
⋮----
def is_mongo_operator(field: str) -> bool
⋮----
# make sure to not return _raw_ queries or any of the update operators
fields = {
⋮----
def validate_transition(self, current_status)
⋮----
def validate_status_change(current_status, new_status)
⋮----
allowed_statuses = get_possible_status_changes(current_status)
⋮----
state_machine = {
⋮----
def get_possible_status_changes(current_status)
⋮----
"""
    :param current_status:
    :return possible states from current state
    """
possible = state_machine.get(current_status)
⋮----
missing = [f for f in ("company",) if f not in only]
⋮----
only = [*only, *missing]
⋮----
result = list(
⋮----
forbidden_tasks = {task.id for task in result if not task.company}
⋮----
result = [task for task in result if task.id not in forbidden_tasks]
⋮----
"""
    Gets a task that has a required write access
    :except errors.bad_request.InvalidTaskId: if the task is not found
    :except errors.forbidden.NoWritePermission: if write_access was required and the task cannot be modified
    """
query = dict(id=task_id, company=company_id)
⋮----
task = Task.get_for_writing(_only=only, **query)
⋮----
"""
    Loads only task id and return the task only if it is updatable (status == 'created')
    """
task = get_task_with_write_access(
⋮----
allowed_statuses = [TaskStatus.created]
⋮----
last_updates = dict(last_change=now, last_changed_by=user_id)
⋮----
max_values = config.get("services.tasks.max_last_metrics", 2000)
total_metrics = set()
⋮----
query = dict(id=task_id)
to_add = sum(len(v) for m, v in last_scalar_events.items())
⋮----
db_cls = Model if model_events else Task
task = db_cls.objects(**query).only("unique_metrics").first()
⋮----
total_metrics = set(task.unique_metrics)
⋮----
new_metrics = []
⋮----
"""
        Update new mean field based on the value in db and new data
        The count field is updated here too and not with inc__ so that
        it will not get updated in the db earlier than the corresponding mean
        """
metric_path = metric_path.replace("__", ".")
mean_value_field = f"{metric_path}.mean_value"
count_field = f"{metric_path}.count"
⋮----
"""
        Build an aggregation for an atomic update of the min or max value and the corresponding iteration
        """
⋮----
field_prefix = "first"
op = None
⋮----
field_prefix = "min"
op = "$gt"
⋮----
field_prefix = "max"
op = "$lt"
⋮----
value_field = f"{metric_path}__{field_prefix}_value".replace("__", ".")
exists = {"$lte": [f"${value_field}", None]}
⋮----
condition = {
⋮----
condition = exists
⋮----
value_iteration_field = (
⋮----
metric = f"{variant_data.get('metric')}/{variant_data.get('variant')}"
⋮----
path = f"last_metrics__{metric_key}__{variant_key}"
⋮----
count = variant_data.get("count")
total = variant_data.get("total")
</file>

<file path="apiserver/bll/user/__init__.py">
class UserBLL
⋮----
@staticmethod
    def create(request: CreateRequest)
⋮----
user_id = request.id
⋮----
user = User(
⋮----
@staticmethod
    def delete(user_id: str)
⋮----
res = User.objects(id=user_id).delete()
</file>

<file path="apiserver/bll/util.py">
@functools.lru_cache()
def get_server_uuid() -> Optional[str]
⋮----
def parallel_chunked_decorator(func: Callable = None, chunk_size: int = 100)
⋮----
"""
    Decorates a method for parallel chunked execution. The method should have
    one positional parameter (that is used for breaking into chunks)
    and arbitrary number of keyword params. The return value should be iterable
    The results are concatenated in the same order as the passed params
    """
⋮----
@functools.wraps(func)
    def wrapper(self, iterable: Iterable, **kwargs)
⋮----
func_with_params = functools.partial(func, self, **kwargs)
⋮----
T = TypeVar("T")
⋮----
results = list()
failures = list()
⋮----
def update_project_time(project_ids: Union[str, Sequence[str]])
⋮----
project_ids = [project_ids]
</file>

<file path="apiserver/bll/workers/__init__.py">
log = config.logger(__file__)
⋮----
class WorkerBLL
⋮----
_key_regex_trans = str.maketrans({"*": ".*", "?": ".?"})
⋮----
def __init__(self, es=None, redis=None)
⋮----
@property
    def stats(self) -> WorkerStats
⋮----
"""
        Register a worker
        :param company_id: worker's company ID
        :param user_id: user ID under which this worker is running
        :param worker: worker ID
        :param ip: the real ip of the worker
        :param queues: queues reported as being monitored by the worker
        :param timeout: registration expiration timeout in seconds
        :param tags: a list of tags for this worker
        :raise bad_request.InvalidUserId: in case the calling user or company does not exist
        :return: worker entry instance
        """
key = WorkerBLL._get_worker_key(company_id, user_id, worker)
⋮----
timeout = timeout or int(self.config.get("default_worker_timeout_sec", 10 * 60))
queues = queues or []
⋮----
query = dict(id=user_id, company=company_id)
user = User.objects(**query).only("id", "name").first()
⋮----
company = Company.objects(id=company_id).only("id", "name").first()
⋮----
queues_query = Q(company=company_id, id__in=queues)
queue_objs = Queue.objects(queues_query).only("id")
⋮----
invalid = set(queues).difference(q.id for q in queue_objs)
⋮----
now = datetime.utcnow()
entry = WorkerEntry(
⋮----
def unregister_worker(self, company_id: str, user_id: str, worker: str) -> None
⋮----
"""
        Unregister a worker
        :param company_id: worker's company ID
        :param user_id: user ID under which this worker is running
        :param worker: worker ID
        :raise bad_request.WorkerNotRegistered: the worker was not previously registered
        """
res = self.redis.delete(
⋮----
"""
        Write worker status report
        :param company_id: worker's company ID
        :param user_id: user_id ID under which this worker is running
        :param ip: worker IP
        :param report: the report itself
        :param tags: tags for this worker
        :raise bad_request.InvalidTaskId: the reported task was not found
        :return: worker entry instance
        """
entry = self._get_worker(company_id, user_id, report.worker)
⋮----
query = dict(id=report.task, company=company_id)
update = dict(
# modify(new=True, ...) returns the modified object
task = Task.objects(**query).modify(new=True, **update)
⋮----
project = Project.objects(id=task.project).only("name").first()
⋮----
msg = "Failed processing worker status report"
⋮----
"""
        Get all the company workers that were active during the last_seen period
        :param company_id: worker's company id
        :param last_seen: period in seconds to check. Min value is 1 second
        :return:
        """
⋮----
workers = self._get(
⋮----
ref_time = datetime.utcnow() - timedelta(seconds=max(1, last_seen))
workers = [
⋮----
helpers = [
⋮----
task_ids = set(filter(None, (helper.task_id for helper in helpers)))
all_queues = set(
⋮----
queues_info = {}
⋮----
projection = [
queues_info = {res["_id"]: res for res in Queue.aggregate(projection)}
task_ids = task_ids.union(
⋮----
tasks_info = {}
⋮----
tasks_info = {
⋮----
def update_queue_entries(*entries)
⋮----
info = queues_info.get(entry.id, None)
⋮----
task_id = nested_get(info, ("next_entry", "task"))
⋮----
task = tasks_info.get(task_id, None)
⋮----
worker = helper.worker
⋮----
task: Task = tasks_info.get(helper.task_id, None)
⋮----
@staticmethod
    def _get_worker_key(company: str, user: str, worker_id: str) -> str
⋮----
"""Build redis key from company, user and worker_id"""
⋮----
def _get_worker(self, company_id: str, user_id: str, worker: str) -> WorkerEntry
⋮----
"""
        Get a worker entry for the provided worker ID. The entry is loaded from Redis
        if it exists (i.e. worker has already been registered), otherwise the worker
        is registered and its entry stored into Redis).
        :param company_id: worker's company ID
        :param user_id: user ID under which this worker is running
        :param worker: worker ID
        :raise bad_request.InvalidWorkerId: in case the worker id was not found
        :return: worker entry instance
        """
key = self._get_worker_key(company_id, user_id, worker)
⋮----
data = self.redis.get(key)
⋮----
entry = WorkerEntry.from_json(data)
⋮----
msg = "Failed parsing worker entry"
⋮----
# Failed loading worker from Redis
⋮----
@staticmethod
    def _get_tagged_workers_key(company: str, tags_field: str, tag: str) -> str
⋮----
@staticmethod
    def _get_all_workers_key(company: str) -> str
⋮----
def _save_worker_data(self, entry: WorkerEntry)
⋮----
company_id = entry.company.id
expiration = int(time()) + entry.register_timeout
worker_item = {entry.key: expiration}
⋮----
name = self._get_tagged_workers_key(company_id, tags_field, tag)
⋮----
def _save_worker(self, entry: WorkerEntry) -> None
⋮----
"""Save worker entry in Redis"""
⋮----
msg = "Failed saving worker entry"
⋮----
match = self._get_worker_key(company, user, worker_pattern or "*")
⋮----
def filter_by_user_and_pattern(in_keys: Set[bytes]) -> Set[bytes]
⋮----
user_bytes = user.encode()
in_keys = {k for k in in_keys if user_bytes in k}
⋮----
worker_pattern_bytes = (
regex = re.compile(worker_pattern_bytes)
in_keys = {k for k in in_keys if regex.search(k)}
⋮----
worker_keys = set()
⋮----
timestamp = int(time())
⋮----
tagged_workers = set()
⋮----
tagged_workers_key = self._get_tagged_workers_key(
⋮----
tagged_workers = filter_by_user_and_pattern(tagged_workers)
worker_keys = (
⋮----
all_workers_key = self._get_all_workers_key(company)
⋮----
worker_keys = filter_by_user_and_pattern(worker_keys)
⋮----
"""Get worker entries matching the company and user, worker patterns"""
⋮----
entries = []
⋮----
data = self.redis.mget(keys)
⋮----
@staticmethod
    def _get_es_index_suffix()
⋮----
"""Get the index name suffix for storing current month data"""
⋮----
"""
        Actually writing the worker statistics to Elastic
        :return: The amount of logged documents
        """
es_index = (
⋮----
def make_doc(category, metric, variant, value) -> dict
⋮----
actions = []
⋮----
category = field.partition("_")[0]
metric = field
⋮----
es_res = elasticsearch.helpers.bulk(self.es_client, actions)
⋮----
@attr.s(auto_attribs=True)
class WorkerConversionHelper
⋮----
worker: WorkerResponseEntry
task_id: str
queue_ids: Set[str]
⋮----
@classmethod
    def from_worker_entry(cls, worker: WorkerEntry)
⋮----
data = worker.to_struct()
queue = data.pop("queue", None) or None
queue_ids = set(data.pop("queues", []))
queues = [QueueEntry(id=id) for id in queue_ids]
⋮----
queue = next((q for q in queues if q.id == queue), None)
</file>

<file path="apiserver/bll/workers/stats.py">
log = config.logger(__file__)
⋮----
class WorkerStats
⋮----
min_chart_interval = config.get("services.workers.min_chart_interval_sec", 40)
_max_metrics_concurrency = config.get("services.events.events_retrieval.max_metrics_concurrency", 4)
⋮----
def __init__(self, es)
⋮----
@staticmethod
    def worker_stats_prefix_for_company(company_id: str) -> str
⋮----
"""Returns the es index prefix for the company"""
⋮----
def search_company_stats(self, company_id: str, es_req: dict) -> dict
⋮----
"""
        Get dictionary of metric types grouped by categories
        :param company_id: company id
        :param worker_ids: optional list of workers to get metric types from.
        If not specified them metrics for all the company workers returned
        :return:
        """
es_req = {
⋮----
res = self.search_company_stats(company_id, es_req)
⋮----
agg_types_to_es = {
agg = {
split_by_resource = split_by_resource and metric_item.key.startswith("gpu_")
⋮----
split_aggs = {"split": {"terms": {"field": "variant"}, "aggs": agg}}
⋮----
split_aggs = {}
⋮----
query_terms = [
⋮----
data = self.search_company_stats(company_id, es_req)
⋮----
cutoff_date = (
⋮----
) * 1000  # do not return the point for the incomplete last interval
⋮----
def get_worker_stats(self, company_id: str, request: GetStatsRequest) -> dict
⋮----
"""
        Get statistics for company workers metrics in the specified time range
        Returned as date histograms for different aggregation types
        grouped by worker, metric type (and optionally metric variant)
        Buckets with no metrics are not returned
        Note: all the statistics are retrieved as one ES query
        """
from_date = request.from_date
to_date = request.to_date
⋮----
interval = max(request.interval, self.min_chart_interval)
⋮----
res = list(
⋮----
ret = defaultdict(lambda: defaultdict(dict))
⋮----
"""
        Clean results returned from elastic search (remove "aggregations", "buckets" etc.),
        leave only aggregation types requested by the user and return a clean dictionary
        :param data: aggregation data retrieved from ES
        """
⋮----
def extract_metric_results(metric: dict) -> dict
⋮----
aggregation = metric_item.aggregation.value
date_buckets = metric["dates"]["buckets"]
length = len(date_buckets)
⋮----
dates = [None] * length
agg_values = [0.0] * length
resource_series = defaultdict(lambda: [0.0] * length)
⋮----
date = date_buckets[idx]
⋮----
series = resource_series[resource["key"]]
⋮----
resource_series = {}
⋮----
"""
        Get statistics for company workers metrics in the specified time range
        Returned as date histograms for different aggregation types
        grouped by worker, metric type (and optionally metric variant)
        Note: all the statistics are retrieved using one ES query
        """
⋮----
interval = max(interval, self.min_chart_interval)
⋮----
must = [QueryBuilder.dates_range(from_date, to_date)]
⋮----
ret = [
⋮----
# remove last interval if it's incomplete. Allow 10% tolerance
</file>

<file path="apiserver/config_repo.py">
config = BasicConfig()
</file>

<file path="apiserver/config/__init__.py">

</file>

<file path="apiserver/config/basic.py">
EXTRA_CONFIG_PATHS = ("/opt/trains/config", "/opt/clearml/config")
DEFAULT_PREFIXES = ("clearml", "trains")
EXTRA_CONFIG_PATH_SEP = ":" if platform.system() != "Windows" else ";"
⋮----
class BasicConfig
⋮----
NotSet = object()
⋮----
extra_config_values_env_key_sep = "__"
default_config_dir = "default"
⋮----
folder = (
⋮----
def __getitem__(self, key)
⋮----
def get(self, key: str, default: Any = NotSet) -> Any
⋮----
value = self._config.get(key, default)
⋮----
def to_dict(self) -> dict
⋮----
def as_json(self) -> str
⋮----
def logger(self, name: str) -> logging.Logger
⋮----
name = Path(name).stem
⋮----
name = Path(name).parent.stem
path = ".".join((self.prefix, name))
⋮----
def _read_extra_env_config_values(self) -> ConfigTree
⋮----
"""Loads extra configuration from environment-injected values"""
result = ConfigTree()
⋮----
keys = sorted(k for k in os.environ if k.startswith(prefix))
⋮----
path = (
result = self._merge_configs(
⋮----
def _get_paths(self) -> List[Path]
⋮----
default_paths = EXTRA_CONFIG_PATH_SEP.join(EXTRA_CONFIG_PATHS)
value = first(map(getenv, self.extra_config_path_override_var), default_paths)
⋮----
paths = [
⋮----
invalid = [path for path in paths if not path.is_dir()]
⋮----
def reload(self)
⋮----
def _reload(self) -> ConfigTree
⋮----
extra_config_values = self._read_extra_env_config_values()
⋮----
configs = [
⋮----
@classmethod
    def _merge_configs(cls, a, b, copy_trees=False, override_prefix="-")
⋮----
"""Based on pyhocon.ConfigTree.merge_configs, with dict override support using a `-` key prefix"""
⋮----
override = key.startswith(override_prefix)
⋮----
key = key[len(override_prefix) :]
# if key is in both a and b and both values are dictionary then merge it otherwise override it
⋮----
def _read_recursive(self, conf_root, exclude_files: Set[str]) -> ConfigTree
⋮----
conf = ConfigTree()
⋮----
key = ".".join(file.relative_to(conf_root).with_suffix("").parts)
⋮----
def _read_single_file(self, file_path)
⋮----
msg = f"Failed parsing {file_path} ({ex.__class__.__name__}): (at char {ex.loc}, line:{ex.lineno}, col:{ex.column})"
⋮----
msg = f"Failed parsing {file_path} ({ex.__class__.__name__}): {ex}"
⋮----
def initialize_logging(self)
⋮----
logging_config = self.get("logging", None)
⋮----
class ConfigurationError(Exception)
⋮----
def __init__(self, msg, file_path=None, *args)
⋮----
ConfigType = TypeVar("ConfigType", bound=BasicConfig)
</file>

<file path="apiserver/config/default/apiserver.conf">
{
    watch: false            # Watch for changes (dev only)
    debug: false            # Debug mode
    pretty_json: false      # prettify json response
    return_stack: false      # return stack trace on error
    return_stack_to_caller: false # top-level control on whether to return stack trace in an API response

    # if 'return_stack' is true and error contains a status code, return stack trace only for these status codes
    # valid values are:
    #  - an integer number, specifying a status code
    #  - a tuple of (code, subcode or list of subcodes)
    return_stack_on_code: [
        [500, 0]  # raise on internal server error with no subcode
    ]

    listen {
        ip : "0.0.0.0"
        port: 8008
    }

    version {
        required: false
        default: 1.0
        # if set then calls to endpoints with the version
        # greater that the current max version will be rejected
        check_max_version: false
    }

    pre_populate {
        enabled: false
        zip_files: ["/path/to/export.zip"]
        fail_on_error: false
        # artifacts_path: "/mnt/fileserver"
    }

    # time in seconds to take an exclusive lock to init es and mongodb
    # not including the pre_populate
    db_init_timout: 120

    mongo {
        # controls whether FieldDoesNotExist exception will be raised for any extra attribute existing in stored data
        # but not declared in a data model
        strict: false
        ensure_db_version_on_startup: true
    }

    elastic {
        probing {
            # settings for inital probing of elastic connection
            max_retries: 4
            timeout: 30
        }
        upgrade_monitoring {
            v16_migration_verification: true
        }
    }

    auth {
        # verify user tokens
        verify_user_tokens: false

        # If set then users that were created from secure credentials or fixed user settings and are no longer in these settings will be deleted on startup
        delete_missing_autocreated_users: true

        # max token expiration timeout in seconds (1 year)
        max_expiration_sec: 31536000

        # default token expiration timeout in seconds (30 days)
        default_expiration_sec: 2592000

        # cookie containing auth token, for requests arriving from a web-browser
        session_auth_cookie_name: "clearml_token_basic"

        # cookie configuration for authorization cookies generated by auth.login
        cookies {
            httponly: true  # allow only http to access the cookies (no JS etc)
            secure: false   # not using HTTPS
            domain: null    # Limit to localhost is not supported
            samesite: Lax
            max_age: 99999999999
        }

        # provide a cookie domain override per company
#        cookies_domain_override {
#            <company-id>: <domain>
#        }

#        # A list of fixed users
#        # Note: password may be base64-encoded bcrypt-hashed (generate using `python -c 'import bcrypt,base64; print(base64.b64encode(bcrypt.hashpw("password".encode(), bcrypt.gensalt())))'`)
#        fixed_users {
#            enabled: true
#            pass_hashed: false
#            users: [
#                {
#                    username: "john"
#                    password: "123456"
#                    name: "john doe"
#                }
#
#            ]
#        }
    }

    cors {
        origins: "*"

        # Not supported when origins is "*"
        supports_credentials: true
    }

    default_company: "d1bd92a3b039400cbafc60a7a5b1e52b"

    workers {
        # Auto-register unknown workers on status reports and other calls
        auto_register: true
        # Assume unknow workers have unregistered (i.e. do not raise unregistered error)
        auto_unregister: true
        # Timeout in seconds on task status update. If exceeded
        # then task can be stopped without communicating to the worker
        task_update_timeout: 600

        # Timeout in seconds for worker registration (or status report). If a worker did not report for this long,
        # it is discarded from the server's table
        default_timeout: 600
    }

    check_for_updates {
        enabled: true

        # Check for updates every 24 hours
        check_interval_sec: 86400

        url: "https://updates.clear.ml/updates"

        component_name: "clearml-server"

        # GET request timeout
        request_timeout_sec: 3.0
    }

    statistics {
        # Note: statistics are sent ONLY if the user has actively opted-in
        supported: true

        url: "https://updates.clear.ml/stats"

        report_interval_hours: 24
        agent_relevant_threshold_days: 30

        max_retries: 5
        max_backoff_sec: 5
    }

    getting_started_info {
        "agentName": "clearml",
        "configure": "clearml-init",
        "install": "pip install clearml",
        "packageName": "clearml"
    }

}
</file>

<file path="apiserver/config/default/hosts.conf">
fileserver = "http://localhost:8081"

elastic {
  events {
    hosts: [{host: "127.0.0.1", port: 9200, scheme: http}]
    args {
      timeout: 60
      max_retries: 3
      retry_on_timeout: true
    }
    index_version: "1"
  }

  workers {
    hosts: [{host:"127.0.0.1", port:9200, scheme: http}]
    args {
      timeout: 60
      max_retries: 3
      retry_on_timeout: true
    }
    index_version: "1"
  }
}

mongo {
  backend {
    host: "mongodb://127.0.0.1:27017/backend"
  }
  auth {
    host: "mongodb://127.0.0.1:27017/auth"
  }
}

redis {
  apiserver {
      host: "127.0.0.1"
      port: 6379
      db: 0
  }
  workers {
    host: "127.0.0.1"
    port: 6379
    db: 4
  }
}
</file>

<file path="apiserver/config/default/logging.conf">
{
    version: 1
    disable_existing_loggers: false
    formatters: {
        standard: {
            format: "[%(asctime)s] [%(process)d] [%(levelname)s] [%(name)s] %(message)s"
        }
    }
    handlers {
        console {
            formatter: standard
            class: "logging.StreamHandler"
        }
        text_file: {
            formatter: standard,
            backupCount: 3
            maxBytes: 10240000,
            class: "logging.handlers.RotatingFileHandler",
            filename: "/var/log/clearml/apiserver.log"
        }
    }
    root {
        handlers: [console, text_file]
        level: INFO
    }
    loggers {
        urllib3 {
            handlers: [console, text_file]
            level: WARN
            propagate: false
        }
        werkzeug {
            handlers: [console, text_file]
            level: WARN
            propagate: false
        }
        elasticsearch {
            handlers: [console, text_file]
            level: WARN
            propagate: false
        }
        # disable pep8 auto-gen logging (at least part of it)
        RefactoringTool {
            handlers: []
            level: ERROR
            propagate: false
        }
    }
}
</file>

<file path="apiserver/config/default/secure.conf">
{
    http {
        session_secret {
            apiserver: "V8gcW3EneNDcNfO7G_TSUsWe7uLozyacc9_I33o7bxUo8rCN31VLRg"
        }
    }

    auth {
        # token sign secret
        token_secret: "Rq8FW84sSqVgq7WvBB_4EzNl9y8z8IGiDXX3C345_a5AZfcwZcwCIA"
    }

    credentials {
        # system credentials as they appear in the auth DB, used for intra-service communications
        apiserver {
            role: "system"
            user_key: "62T8CP7HGBC6647XF9314C2VY67RJO"
            user_secret: "gaOfhDX2-bpkeI7-cwEcaMuGijxaG2UG3jbIvg4DxmVGF0LNI7rgvCb1-ne38IlBo1w"
        }
        fileserver {
            role: "system"
            user_key: "GSQWPEKSKNKF354LC9V6BHXKTYFD5I"
            user_secret: "tuBXcGQBECsEhcNiK2kiWi750z9r8Z85XrQ9V0c24huTuCb2xf2X1nKG"
        }
        webserver {
            role: "system"
            user_key: "EYVQ385RW7Y2QQUH88CZ7DWIQ1WUHP"
            user_secret: "XhkH6a6ds9JBnM_MrahYyYdO-wS2bqFSm8gl-V0UZXH26Ydd6Eyi28TeBEoSr6Z3Bes"
            revoke_in_fixed_mode: true
        }
        services_agent {
            role: "admin"
            user_key: ""
            user_secret: ""
        }
        tests {
            role: "user"
            display_name: "Default User"
            user_key: "EGRTCO8JMSIGI6S39GTP43NFWXDQOW"
            user_secret: "LPEJbGJ6bK4tujQcmrD3i1dbMBDdwUwelVa-LG0K0FFmY9bzH_H0Sw"
            revoke_in_fixed_mode: true
        }
    }
}
</file>

<file path="apiserver/config/default/services/_mongo.conf">
max_page_size: 500

# expiration time in seconds for the redis scroll states in get_many family of apis
scroll_state_expiration_seconds: 600

allow_disk_use {
    sort: true
    aggregate: true
}
</file>

<file path="apiserver/config/default/services/async_urls_delete.conf">
# if set to true then on task delete/reset external file urls for known storage types are scheduled for async delete
# otherwise they are returned to a client for the client side delete
enabled: true
max_retries: 3
retry_timeout_sec: 60

fileserver {
    # fileserver url prefixes. Evaluated in the order of priority
    # Can be in the form <schema>://host:port/path or /path
    url_prefixes: ["https://files.community-master.hosted.allegro.ai/"]
    timeout_sec: 300
    token_expiration_sec: 600
}
</file>

<file path="apiserver/config/default/services/auth.conf">
fixed_users {
  guest {
    enabled: false

    default_company: "025315a9321f49f8be07f5ac48fbcf92"

    name: "Guest"
    username: "guest"
    password: "guest"

    # Allow access only to the following endpoints when using user/pass credentials
    allow_endpoints: [
      "auth.login"
    ]
  }
}
</file>

<file path="apiserver/config/default/services/events.conf">
es_index_prefix: "events"

ignore_iteration {
    metrics: [":monitor:machine", ":monitor:gpu"]
}


events_retrieval {
    state_expiration_sec: 3600

    # max number of concurrent queries to ES when calculating events metrics
    # should not exceed the amount of concurrent connections set in the ES driver
    max_metrics_concurrency: 4

    # If set then max_metrics_count and max_variants_count are calculated dynamically on user data
    dynamic_metrics_count: true

    # The percentage from the ES aggs limit (10000) to use for the max_metrics and max_variants calculation
    dynamic_metrics_count_threshold: 80

    # the max amount of metrics to aggregate on
    max_metrics_count: 100

    # the max amount of variants to aggregate on
    max_variants_count: 100

    debug_images {
        # Allow to return the debug images for the variants with uninitialized valid iterations border
        allow_uninitialized_variants: true
    }

    max_raw_scalars_size: 200000

    scroll_id_key: "cTN5VEtWEC6QrHvUl0FTx9kNyO0CcCK1p57akxma"

    multi_plots_batch_size: 1000
}

# if set then plot str will be checked for the valid json on plot add
# and the result of the check is written to the db
validate_plot_str: false

# If not 0 then the plots equal or greater to the size will be stored compressed in the DB
plot_compression_threshold: 100000

# async events delete threshold
max_async_deleted_events_per_sec: 1000
</file>

<file path="apiserver/config/default/services/models.conf">
metadata_values {
    # cache ttl sec
    cache_ttl_sec: 86400
}
</file>

<file path="apiserver/config/default/services/organization.conf">
tags_cache {
  expiration_seconds: 3600
}
download {
    redis_timeout_sec: 300
    batch_size: 500
    max_download_items: 50000
    max_project_name_length: 60
}

# the maximum amount of pool workers to use for performing parallel queries
# in get_entities_count call. 0 means sequential
max_entities_count_concurrency: 4

project_usages {
    # the tasks with these tags or of these types will not counted
    excluded_task_tags: ["dataset", "reports", "hidden"]
    excluded_task_types: ["dataset_import", "annotation", "annotation_manual", "report", "controller"]
    # if set to 'false' then only admin users can call this api. Otherwise all the users
    allow_non_admins: true

    # if set the top level app instance tasks are excluded
    exclude_app_parent_tasks: true

    # if set then in 'include_development': false mode all the tasks without queues are excluded
    # otherwise only the tasks with system_tags 'development' are excluded
    exclude_tasks_without_queue: true
}
</file>

<file path="apiserver/config/default/services/projects.conf">
# Order of featured projects, by name or ID
featured {
  order: [
    #  {id: "<project-id>"}
    #  OR
    #  {name: "<project-name>"}
    #  OR
    #  {name_regex: "<python-regex>"}
  ]

  # default featured index for public projects not specified in the order
  public_default: 9999
}

sub_projects {
    # the max sub project depth
    max_depth: 10
}
</file>

<file path="apiserver/config/default/services/queues.conf">
{
    metrics_before_from_date: 3600
    # interval in seconds to update queue metrics. Put 0 to disable
    metrics_refresh_interval_sec: 300
    # the queues with these tags will not be returned from get_all/get_all_ex unless id or name specified
    # or search_hidden is set
    hidden_tags: [k8s-glue]

    resource_usages: {
        cpu: 1.0
        gpu: 1.0
    }
}
</file>

<file path="apiserver/config/default/services/serving.conf">
default_container_timeout_sec: 600
# Auto-register unknown serving containers on status reports and other calls
container_auto_register: true
# Assume unknow serving containers have unregistered (i.e. do not raise unregistered error)
container_auto_unregister: true
# The minimal sampling interval for serving model monitor chars
min_chart_interval_sec: 40
</file>

<file path="apiserver/config/default/services/storage_credentials.conf">
aws {
    s3 {
        # S3 credentials, used for read/write access by various SDK elements
        # default, used for any bucket not specified below
        key: ""
        secret: ""
        region: ""
        use_credentials_chain: false
        # Additional ExtraArgs passed to boto3 when uploading files. Can also be set per-bucket under "credentials".
        extra_args: {}
        credentials: [
            # specifies key/secret credentials to use when handling s3 urls (read or write)
            # {
            #     bucket: "my-bucket-name"
            #     key: "my-access-key"
            #     secret: "my-secret-key"
            # },
//            {
//                # This will apply to all buckets in this host (unless key/value is specifically provided for a given bucket)
//                host: "localhost:9000"
//                key: "minioadmin"
//                secret: "minioadmin"
//                # region: my-server
//                multipart: false
//                secure: false
//            }
        ]
    }
}
google.storage {
    # Default project and credentials file
    # Will be used when no bucket configuration is found
//    project: "clearml"
//    credentials_json: "/path/to/credentials.json"
//
//    # Specific credentials per bucket and sub directory
//    credentials = [
//        {
//            bucket: "my-bucket"
//            subdir: "path/in/bucket" # Not required
//            project: "clearml"
//            credentials_json: "/path/to/credentials.json"
//        },
//    ]
}
azure.storage {
    # containers: [
    #     {
    #         account_name: "clearml"
    #         account_key: "secret"
    #         # container_name:
    #     }
    # ]
}
</file>

<file path="apiserver/config/default/services/tasks.conf">
non_responsive_tasks_watchdog {
    enabled: true

    # In-progress tasks older than this value in seconds will be stopped by the watchdog
    threshold_sec: 7200

    # Watchdog will sleep for this number of seconds after each cycle
    watch_interval_sec: 900
}

multi_task_histogram_limit: 100

hyperparam_values {
    # max allowed outdate time for the cashed result
    cache_allowed_outdate_sec: 60

    # cache ttl sec
    cache_ttl_sec: 86400
}

# the maximum amount of unique last metrics/variants combinations
# for which the last values are stored in a task
max_last_metrics: 2000

# if set then call to tasks.delete/cleanup does not wait for ES events deletion
async_events_delete: true
# do not use async_delete if the deleted task has amount of events lower than this threshold
async_events_delete_threshold: 100000
</file>

<file path="apiserver/config/default/services/workers.conf">
default_worker_timeout_sec: 600
default_cluster_timeout_sec: 600

# The minimal sampling interval for resource dashboard and worker activity charts
min_chart_interval_sec: 40

stats {
    max_metrics_concurrency: 4
}
</file>

<file path="apiserver/config/info.py">
root = Path(__file__).parent.parent
⋮----
def _get(prop_name, env_suffix=None, default="")
⋮----
suffix = env_suffix or prop_name
keys = [f"{p}_SERVER_{suffix}" for p in ("CLEARML", "TRAINS")]
value = first(map(getenv, keys))
⋮----
@lru_cache()
def get_build_number()
⋮----
@lru_cache()
def get_version()
⋮----
@lru_cache()
def get_commit_number()
⋮----
@lru_cache()
def get_deployment_type() -> str
⋮----
def get_default_company()
⋮----
missed_es_upgrade = False
es_connection_error = False
</file>

<file path="apiserver/database/__init__.py">
log = config.logger("database")
⋮----
strict = config.get("apiserver.mongo.strict", True)
⋮----
OVERRIDE_HOST_ENV_KEY = (
OVERRIDE_PORT_ENV_KEY = (
⋮----
OVERRIDE_CONNECTION_STRING_ENV_KEY = "CLEARML_MONGODB_SERVICE_CONNECTION_STRING"
OVERRIDE_USERNAME_ENV_KEY = "CLEARML_MONGODB_SERVICE_USERNAME"
OVERRIDE_PASSWORD_ENV_KEY = "CLEARML_MONGODB_SERVICE_PASSWORD"
OVERRIDE_QUERY_ENV_KEY = "CLEARML_MONGODB_SERVICE_QUERY"
⋮----
class DatabaseEntry(models.Base)
⋮----
host = StringField(required=True)
alias = StringField()
name = StringField()
db = StringField()
⋮----
class DatabaseFactory
⋮----
_entries = []
⋮----
@classmethod
    def _create_db_entry(cls, alias: str, settings: dict) -> DatabaseEntry
⋮----
@classmethod
    def initialize(cls)
⋮----
db_entries = config.get("hosts.mongo", {})
missing = []
⋮----
override_connection_string = getenv(OVERRIDE_CONNECTION_STRING_ENV_KEY)
override_hostname = first(map(getenv, OVERRIDE_HOST_ENV_KEY), None)
override_port = first(map(getenv, OVERRIDE_PORT_ENV_KEY), None)
override_username = getenv(OVERRIDE_USERNAME_ENV_KEY)
override_password = getenv(OVERRIDE_PASSWORD_ENV_KEY)
override_query = getenv(OVERRIDE_QUERY_ENV_KEY)
⋮----
settings = {**db_entries.get(key)}
⋮----
entry = cls._create_db_entry(alias=alias, settings=settings)
⋮----
con_str = override_connection_string
⋮----
@classmethod
    def get_entries(cls)
⋮----
@classmethod
    def get_hosts(cls)
⋮----
@classmethod
    def get_aliases(cls)
⋮----
@classmethod
    def reconnect(cls)
⋮----
# there is bug in the current implementation that prevents
# reconnection from work so workaround this
# get_connection(entry.alias, reconnect=True)
⋮----
db = DatabaseFactory()
</file>

<file path="apiserver/database/defs.py">
class Database(object)
⋮----
""" Database names for our different DB instances """
⋮----
backend = 'backend-db'
''' Used for all backend objects (tasks, models etc.) '''
⋮----
auth = 'auth-db'
''' Used for all authentication and permission objects '''
</file>

<file path="apiserver/database/errors.py">
class MakeGetAllQueryError(Exception)
⋮----
def __init__(self, error, field)
⋮----
class ParseCallError(Exception)
⋮----
def __init__(self, msg, **kwargs)
⋮----
def throws_default_error(err_cls, shorten_width: int = None)
⋮----
"""
    Used to make functions (Exception, str) -> Optional[str] searching for specialized error messages raise those
    messages in ``err_cls``. If the decorated function does not find a suitable error message,
    the underlying exception is returned.
    :param err_cls: Error class (generated by apierrors)
    """
⋮----
def decorator(func)
⋮----
@wraps(func)
        def wrapper(self, e, message, **kwargs)
⋮----
extra_info = func(self, e, message, **kwargs)
err = str(e)
⋮----
err = shorten(err, shorten_width, placeholder="...")
⋮----
# noinspection RegExpRedundantEscape
class ElasticErrorsHandler(object)
⋮----
@classmethod
    def _bulk_meta_error(cls, error)
⋮----
@classmethod
@throws_default_error(errors.server_error.DataError, shorten_width=200)
    def bulk_error(cls, e, _, **__)
⋮----
# Currently we only handle the first error
error = e.errors[0]
⋮----
# Else try returning a better error string
⋮----
class MongoEngineErrorsHandler(object)
⋮----
# NotUniqueError
__not_unique_regex = re.compile(
__not_unique_value_regex = re.compile(r':\s"(?P<value>[^"]+)"')
__id_index = "_id_"
__index_sep_regex = re.compile(r"_[0-9]+_?")
⋮----
# FieldDoesNotExist
__not_exist_fields_regex = re.compile(r'"{(?P<fields>.+?)}".+?"(?P<document>.+?)"')
__not_exist_field_regex = re.compile(r"'(?P<field>\w+)'")
⋮----
@classmethod
    def validation_error(cls, e: ValidationError, message, **_)
⋮----
# Thrown when a document is validated. Documents are validated by default on save and on update
err_dict = e.errors or {e.field_name: e.message}
err_dict = {key: str(value) for key, value in err_dict.items()}
⋮----
@classmethod
    def not_unique_error(cls, e, message, **_)
⋮----
# Thrown when a save/update violates a unique index constraint
m = cls.__not_unique_regex.search(str(e))
⋮----
values = cls.__not_unique_value_regex.findall(m.group("values"))
index = m.group("index")
⋮----
fields = "id"
⋮----
fields = cls.__index_sep_regex.split(index)[:-1]
⋮----
@classmethod
    def field_does_not_exist(cls, e, message, **kwargs)
⋮----
# Strict mode. Unknown fields encountered in loaded document(s)
field_does_not_exist_cls = kwargs.get(
m = cls.__not_exist_fields_regex.search(str(e))
params = {}
⋮----
fields = cls.__not_exist_field_regex.findall(m.group("fields"))
⋮----
@classmethod
@throws_default_error(errors.server_error.DataError)
    def invalid_document_error(cls, e, message, **_)
⋮----
# Reverse_delete_rule used in reference field
⋮----
@classmethod
    def lookup_error(cls, e, message, **_)
⋮----
@classmethod
@throws_default_error(errors.bad_request.InvalidRegexError)
    def invalid_regex_error(cls, e, _, **__)
⋮----
@classmethod
@throws_default_error(errors.server_error.InternalError)
    def invalid_query_error(cls, e, message, **_)
⋮----
inner = e.args[0]
⋮----
@contextmanager
def translate_errors_context(message=None, **kwargs)
⋮----
"""
    A context manager that translates MongoEngine's and Elastic thrown errors into our apierrors classes,
    with an appropriate message.
    """
⋮----
message = "while " + message
</file>

<file path="apiserver/database/fields.py">
NoneType = type(None)
⋮----
class LengthRangeListField(ListField)
⋮----
def __init__(self, field=None, max_length=maxsize, min_length=0, **kwargs)
⋮----
def validate(self, value)
⋮----
class LengthRangeEmbeddedDocumentListField(LengthRangeListField)
⋮----
def __init__(self, field=None, *args, **kwargs)
⋮----
class UniqueEmbeddedDocumentListField(EmbeddedDocumentListField)
⋮----
def __init__(self, document_type, key, **kwargs)
⋮----
"""
        Create a unique embedded document list field for a document type with a unique comparison key func/property
        :param document_type: The type of :class:`~mongoengine.EmbeddedDocument` the list will hold.
        :param key: A callable to extract a key from each item
        """
⋮----
def object_to_key_value_pairs(obj)
⋮----
class EmbeddedDocumentSortedListField(EmbeddedDocumentListField)
⋮----
"""
    A sorted list of embedded documents
    """
⋮----
def to_mongo(self, value, use_db_field=True, fields=None)
⋮----
value = super(EmbeddedDocumentSortedListField, self).to_mongo(
⋮----
class LengthRangeSortedListField(LengthRangeListField, SortedListField)
⋮----
class CustomFloatField(FloatField)
⋮----
def __init__(self, greater_than=None, **kwargs)
⋮----
class CanonicEmailField(EmailField)
⋮----
"""email field that is always lower cased"""
def __set__(self, instance, value: str)
⋮----
value = value.lower()
⋮----
def prepare_query_value(self, op, value)
⋮----
class StrippedStringField(StringField)
⋮----
def __set__(self, instance, value)
⋮----
value = value.strip(self._strip_chars)
⋮----
def contains_empty_key(d)
⋮----
"""
    Helper function to recursively determine if any key in a
    dictionary is empty (based on mongoengine.fields.key_not_string)
    """
⋮----
class DictValidationMixin
⋮----
"""
    DictField validation in MongoEngine requires default alias and permissions to access DB version:
    https://github.com/MongoEngine/mongoengine/issues/2239
    This is a stripped down implementation that does not require any of the above and implies Mongo ver 3.6+
    """
⋮----
def _safe_validate(self: DictField, value)
⋮----
msg = "Invalid dictionary key - documents must have only string keys"
⋮----
class SafeMapField(MapField, DictValidationMixin)
⋮----
class NullableStringField(StringField)
⋮----
class SafeDictField(DictField, DictValidationMixin)
⋮----
class SafeSortedListField(SortedListField)
⋮----
"""
    SortedListField that does not raise an error in case items are not comparable
    (in which case they will be sorted by their string representation)
    """
⋮----
def to_mongo(self, *args, **kwargs)
⋮----
def _safe_to_mongo(self, value, use_db_field=True, fields=None)
⋮----
value = super(SortedListField, self).to_mongo(value, use_db_field, fields)
⋮----
def key(v)
⋮----
key = str
⋮----
class UnionField(DynamicField)
⋮----
def __init__(self, types, *args, **kwargs)
⋮----
def validate(self, value, clean=True)
⋮----
type_names = [t.__name__ for t in self.types]
expected = " or ".join(
</file>

<file path="apiserver/database/model/__init__.py">
class AttributedDocument(DbModelMixin, Document)
⋮----
"""
    Represents objects which are attributed to a company and a user or to "no one".
    Company must be required since it can be used as unique field.
    """
meta = ABSTRACT_FLAG
company = StringField(required=True, reference_field=Company)
user = StringField(reference_field=User)
⋮----
def is_public(self) -> bool
⋮----
class PrivateDocument(AttributedDocument)
⋮----
"""
    Represents documents which always belong to a single company
    """
⋮----
# can not have an empty string as this is the "public" marker
company = StringField(required=True, reference_field=Company, min_length=1)
user = StringField(reference_field=User, required=True)
⋮----
def validate_id(cls, company, **kwargs)
⋮----
"""
    Validate existence of objects with certain IDs. within company.
    :param cls: Model class to search in
    :param company: Company to search in
    :param kwargs: Mapping of field name to object ID. If any ID does not have a corresponding object,
                    it will be reported along with the name it was assigned to.
    :return:
    """
ids = set(kwargs.values())
objs = list(cls.objects(company=company, id__in=ids).only('id'))
missing = ids - set(x.id for x in objs)
⋮----
id_to_name = {}
⋮----
class EntityVisibility(Enum)
⋮----
active = "active"
archived = "archived"
hidden = "hidden"
</file>

<file path="apiserver/database/model/auth.py">
class Entities(object)
⋮----
company = "company"
task = "task"
user = "user"
model = "model"
⋮----
class Role(object)
⋮----
system = "system"
""" Internal system component """
root = "root"
""" Root admin (person) """
admin = "admin"
""" Company administrator """
superuser = "superuser"
""" Company super user """
⋮----
""" Company user """
annotator = "annotator"
""" Annotator with limited access"""
guest = "guest"
""" Guest user. Read Only."""
⋮----
@classmethod
    def get_system_roles(cls) -> set
⋮----
@classmethod
    def get_company_roles(cls) -> set
⋮----
class Credentials(EmbeddedDocument)
⋮----
meta = {"strict": False}
key = StringField(required=True)
secret = StringField(required=True)
label = StringField()
last_used = DateTimeField()
last_used_from = StringField()
⋮----
class User(DbModelMixin, AuthDocument)
⋮----
meta = {"db_alias": Database.auth, "strict": strict}
⋮----
id = StringField(primary_key=True)
name = StringField()
⋮----
created = DateTimeField()
""" User auth entry creation time """
⋮----
validated = DateTimeField()
""" Last validation (login) time """
⋮----
role = StringField(required=True, choices=get_options(Role), default=Role.user)
""" User role """
⋮----
company = StringField(required=True)
""" Company this user belongs to """
⋮----
credentials = EmbeddedDocumentListField(Credentials, default=list)
""" Credentials generated for this user """
⋮----
email = EmailField(unique=True, sparse=True)
""" Email uniquely identifying the user """
⋮----
autocreated = BooleanField(default=False)
""" Set to true if the user was auto created based on config settings"""
</file>

<file path="apiserver/database/model/base.py">
log = config.logger("dbmodel")
mongo_conf = config.get("services._mongo")
ACCESS_REGEX = re.compile(r"^(?P<prefix>>=|>|<=|<)?(?P<value>.*)$")
ACCESS_MODIFIER = {">=": "gte", ">": "gt", "<=": "lte", "<": "lt"}
⋮----
ABSTRACT_FLAG = {"abstract": True}
⋮----
class AuthDocument(Document)
⋮----
meta = ABSTRACT_FLAG
⋮----
class ProperDictMixin(object)
⋮----
res = d
⋮----
res = {k: v for k, v in res.items() if k[0] != "_"}
⋮----
res = project_dict(res, only)
⋮----
class GetMixin(PropsMixin)
⋮----
_text_score = "$text_score"
_projection_key = "projection"
_ordering_key = "order_by"
_search_text_key = "search_text"
⋮----
_start_key = "start"
_size_key = "size"
⋮----
_multi_field_param_sep = "__"
_multi_field_param_prefix = {
⋮----
@attr.s(auto_attribs=True)
    class MultiFieldParameters
⋮----
fields: Sequence[str]
pattern: str = None
datetime: Union[list, str] = None
⋮----
def __attrs_post_init__(self)
⋮----
_numeric_locale = {"locale": "en_US", "numericOrdering": True}
_field_collation_overrides = {}
⋮----
class QueryParameterOptions(object)
⋮----
"""
            :param pattern_fields: Fields for which a "string contains" condition should be generated
            :param list_fields: Fields for which a "list contains" condition should be generated
            :param datetime_fields: Fields for which datetime condition should be generated (see ACCESS_MODIFIER)
            :param fields: Fields which which a simple equality condition should be generated (basically filters out all
                other unsupported query fields)
            """
⋮----
class NewListFieldBucketHelper
⋮----
op_prefix = "__$"
_unary_operators = {
_reset_operator = "__$nop"
_operators = {
default_global_operator = Q.AND
default_context = Q.OR
# not_all modifier currently not supported due to the backwards compatibility
mongo_modifiers = {
⋮----
@attr.s(auto_attribs=True)
        class Term
⋮----
operator: str = None
reset: bool = False
include: bool = True
value: str = None
⋮----
def __init__(self, field: str, data: Sequence[str], legacy=False)
⋮----
current_context = self.default_context
⋮----
current_context = d.operator
⋮----
def _get_next_term(self, data: Sequence[str]) -> Generator[Term, None, None]
⋮----
unary_operator = None
⋮----
unary_operator = value
⋮----
operator = self._operators.get(value)
⋮----
value = value[1:]
⋮----
term = self.Term(value=value)
⋮----
get_all_query_options = QueryParameterOptions()
⋮----
class GetManyScrollState(ProperDictMixin, Document)
⋮----
meta = {"db_alias": Database.backend, "strict": False}
⋮----
id = StringField(primary_key=True)
position = IntField(default=0)
⋮----
_cache_manager = None
⋮----
@classmethod
    def get_cache_manager(cls)
⋮----
q = cls.objects(
⋮----
q = q.only(*_only)
⋮----
"""
        Prepare a query object based on the provided query dictionary and various fields.
        :param parameters_options: Specifies options for parsing the parameters (see ParametersOptions)
        :param company: Company ID (required)
        :param allow_public: Allow results from public objects
        :param parameters: Query dictionary (relevant keys are these specified by the various field names parameters).
            Supported parameters:
            - <field_name>: <value> Will query for items with this value in the field (see QueryParameterOptions for
                specific rules on handling values). Only items matching ALL of these conditions will be retrieved.
            - <any|all>: {fields: [<field1>, <field2>, ...], pattern: <pattern>} Will query for items where any or all
                provided fields match the provided pattern.
            - <any|all>: {fields: [<field1>, <field2>, ...], datetime: <datetime condition>} Will query for items where any or all
                provided datetime fields match the provided condition.
        :return: mongoengine.Q query object
        """
⋮----
"""
        Pop the parameters that match the specified patterns and return
        the dictionary of matching parameters
        Pop None parameters since they are not the real queries
        """
⋮----
fields = set()
⋮----
prefix = pattern[:-1]
⋮----
pairs = ((field, parameters.pop(field, None)) for field in fields)
⋮----
@classmethod
    def _try_convert_to_numeric(cls, value: Union[str, Sequence[str]])
⋮----
def convert_str(val: str) -> Union[float, str]
⋮----
@classmethod
    def _get_fixed_field_value(cls, field: str, value)
⋮----
@classmethod
    def _get_dates_query(cls, field: str, data: Union[list, str]) -> Union[Q, dict]
⋮----
"""
        Return dates query for the field
        If the data is 2 values array and none of the values starts from dates comparison operations
        then return the simplified range query
        Otherwise return the dictionary of dates conditions
        """
⋮----
data = [data]
⋮----
dict_query = {}
⋮----
m = ACCESS_REGEX.match(d)
⋮----
value = parse_datetime(m.group("value"))
prefix = m.group("prefix")
modifier = ACCESS_MODIFIER.get(prefix)
f = (
⋮----
"""
        Prepare a query object based on the provided query dictionary and various fields.

        NOTE: BE VERY CAREFUL WITH THIS CALL, as it allows creating queries that span across companies.
        IMPLEMENTATION NOTE: Make sure that inside this function or the functions it depends on RegexQ is always
        used instead of Q. Otherwise we can and up with some combination that is not processed according to
        RegexQ rules
        :param parameters_options: Specifies options for parsing the parameters (see ParametersOptions)
        :param parameters: Query dictionary (relevant keys are these specified by the various field names parameters).
            Supported parameters:
            - <field_name>: <value> Will query for items with this value in the field (see QueryParameterOptions for
                specific rules on handling values). Only items matching ALL of these conditions will be retrieved.
            - <any|all>: {fields: [<field1>, <field2>, ...], pattern: <pattern>} Will query for items where any or all
                provided fields match the provided pattern.
        :return: mongoengine.Q query object
        """
parameters_options = parameters_options or cls.get_all_query_options
⋮----
query = RegexQ()
field = None
# noinspection PyBroadException
⋮----
parameters = {
filters = parameters.pop("filters", {})
⋮----
opts = parameters_options
⋮----
pattern = parameters.pop(field, None)
⋮----
data = parameters.pop(field, None)
⋮----
dates_q = cls._get_dates_query(field, data)
⋮----
data = cls.MultiFieldParameters(**value)
⋮----
q = reduce(
⋮----
regex = RegexWrapper(data.pattern, flags=re.IGNORECASE)
sep_fields = [f.replace(".", "__") for f in data.fields]
⋮----
date_fields = [field for field in data.fields if field in opts.datetime_fields]
⋮----
q = Q()
⋮----
dates_q = cls._get_dates_query(date_f, data.datetime)
⋮----
dates_q = RegexQ(**dates_q)
q = func(q, dates_q)
⋮----
query = query & q
⋮----
@classmethod
    def get_range_field_query(cls, field: str, data: Sequence[Optional[str]]) -> RegexQ
⋮----
"""
        Return a range query for the provided field. The data should contain min and max values
        Both intervals are included. For open range queries either min or max can be None
        In case the min value is None the records with missing or None value from db are included
        """
⋮----
mongoengine_field = field.replace(".", "__")
query = {}
⋮----
q = RegexQ(**query)
⋮----
@attr.s(auto_attribs=True)
    class ListQueryFilter
⋮----
"""
        Deserialize filters data and build db_query object that represents it with the corresponding
        mongo engine operations
        Each part has include and exclude lists that map to mongoengine operations as following:
        "any"
          - include -> 'in'
          - exclude -> 'not_all'
          - combined by 'or' operation
        "all"
          - include -> 'all'
          - exclude -> 'nin'
          - combined by 'and' operation
        "op" optional parameter for combining "and" and "all" parts. Can be "and" or "or". The default is "and"
        """
⋮----
_and_op = "and"
_or_op = "or"
_allowed_op = [_and_op, _or_op]
_db_modifiers: Mapping = {
⋮----
@attr.s(auto_attribs=True)
        class ListFilter
⋮----
include: Sequence[str] = []
exclude: Sequence[str] = []
⋮----
@classmethod
            def from_dict(cls, d: Mapping)
⋮----
any: ListFilter = attr.ib(converter=ListFilter.from_dict, default=None)
all: ListFilter = attr.ib(converter=ListFilter.from_dict, default=None)
op: str = attr.ib(default="and")
db_query: dict = attr.ib(init=False)
⋮----
# noinspection PyUnresolvedReferences
⋮----
@op.validator
        def op_validator(self, _, value)
⋮----
@property
        def and_op(self) -> bool
⋮----
operations = {}
⋮----
unique = set(vals)
⋮----
# noinspection PyTypeChecker
⋮----
@classmethod
        def from_data(cls, field, data: Mapping)
⋮----
filter_ = cls.ListQueryFilter.from_data(field, data)
⋮----
queries = []
⋮----
ops = []
⋮----
# cannot just check vals here since 0 is acceptable value
⋮----
operation = Q.AND if filter_.and_op else Q.OR
⋮----
@classmethod
    def get_list_field_query(cls, field: str, data: Sequence[Optional[str]]) -> RegexQ
⋮----
"""
        Get a proper mongoengine Q object that represents an "or" query for the provided values
        with respect to the given list field, with support for "none of empty" in case a None value
        is included.

        - Exclusion can be specified by a leading "-" for each value (API versions <2.8)
            or by a preceding "__$not" value (operator)
        - AND can be achieved using a preceding "__$all" or "__$and" value (operator)
        """
⋮----
helper = cls.NewListFieldBucketHelper(field, data=data, legacy=True)
global_op = helper.global_operator
actions = helper.actions
⋮----
queries = [
⋮----
q = RegexQ()
⋮----
q = RegexQCombination(operation=global_op, children=queries)
⋮----
@classmethod
    def _prepare_perm_query(cls, company, allow_public=False)
⋮----
@classmethod
    def validate_order_by(cls, parameters, search_text) -> Sequence
⋮----
"""
        Validate and extract order_by params as a list
        If ordering is specified then make sure that id field is part of it
        This guarantees the unique order when paging
        """
order_by = parameters.get(cls._ordering_key)
⋮----
order_by = order_by if isinstance(order_by, list) else [order_by]
order_by = [cls._text_score if x == "@text_score" else x for x in order_by]
⋮----
@classmethod
    def validate_paging(cls, parameters=None, default_page=0, default_page_size=None)
⋮----
"""
        Validate and extract paging info from from the provided dictionary. Supports default values.
        If page is specified then it should be non-negative, if page size is specified then it should be positive
        If page size is specified and page is not then 0 page is assumed
        If page is specified then page size should be specified too
        """
parameters = parameters or {}
⋮----
start = parameters.get(cls._start_key)
⋮----
max_page_size = mongo_conf.get("max_page_size", 500)
page = parameters.get("page", default_page)
⋮----
page_size = parameters.get("page_size", default_page_size or max_page_size)
⋮----
page = page or 0
page_size = min(page_size, max_page_size)
⋮----
@classmethod
    def get_projection(cls, parameters, override_projection=None, **__)
⋮----
"""Extract a projection list from the provided dictionary. Supports an override projection."""
⋮----
"""Return include and exclude lists based on passed projection and class definition"""
⋮----
exclude = {x.lstrip(ProjectionHelper.exclusion_prefix) for x in exclude}
⋮----
@classmethod
    def set_projection(cls, parameters: dict, value: Sequence[str]) -> Sequence[str]
⋮----
@classmethod
    def get_ordering(cls, parameters: dict) -> Optional[Sequence[str]]
⋮----
@classmethod
    def set_ordering(cls, parameters: dict, value: Sequence[str]) -> Sequence[str]
⋮----
@classmethod
    def set_default_ordering(cls, parameters: dict, value: Sequence[str]) -> None
⋮----
@classmethod
    def validate_scroll_size(cls, query_dict: dict) -> int
⋮----
size = query_dict.get(cls._size_key)
⋮----
"""
        Retrieves the data by calling the provided data_getter api
        If scroll parameters are specified then put the query_dict 'start' parameter to the last
        scroll position and continue retrievals from that position
        If refresh_scroll is requested then bring once more the data from the beginning
        till the current scroll position
        In the end the scroll position is updated and accumulated frames are returned
        """
query_dict = query_dict or {}
state: Optional[cls.GetManyScrollState] = None
⋮----
size = cls.validate_scroll_size(query_dict)
state = cls.get_cache_manager().get_or_create_state_core(
⋮----
data = data_getter()
⋮----
"""
        Fetch all documents matching a provided query with support for joining referenced documents according to the
        requested projection. See get_many() for more info.
        :param expand_reference_ids: If True, reference fields that contain just an ID string are expanded into
            a sub-document in the format {_id: <ID>}. Otherwise, field values are left as a string.
        """
⋮----
# Refuse projection (join) for auth documents (auth.User etc.) to avoid inadvertently disclosing
# auth-related secrets and prevent security leaks
⋮----
override_projection = cls.get_projection(
⋮----
helper = ProjectionHelper(
⋮----
# Make the main query
results = cls.get_many(
⋮----
def projection_func(doc_type, projection, ids)
⋮----
@classmethod
    def _get_collation_override(cls, field: str) -> Optional[dict]
⋮----
_query = cls.get_combined_query(
⋮----
q = cls.prepare_query(
⋮----
q = cls._prepare_perm_query(company, allow_public=allow_public)
⋮----
"""
        Fetch all documents matching a provided query. Supported several built-in options
         (aside from those provided by the parameters):
            - Ordering: using query field `order_by` which can contain a string or a list of strings corresponding to
                    field names. Using field names not defined in the document will cause an error.
            - Paging: using query fields page and page_size. page must be larger than or equal to 0, page_size must be
                    larger than 0 and is required when specifying a page.
            - Text search: using query field `search_text`. If used, text score can be used in the ordering, using the
                    `@text_score` keyword. A text index must be defined on the document type, otherwise an error will
                    be raised.
        :param return_dicts: Return a list of dictionaries. If True, a list of dicts is returned (if projection was
            requested, each contains only the requested projection). If False, a QuerySet object is returned
            (lazy evaluated). If return_dicts is requested then the entities with the None value in order_by field
            are returned last in the ordering.
        :param company: Company ID (required)
        :param parameters: Parameters dict from which paging ordering and searching parameters are extracted.
        :param query_dict: If provided, passed to prepare_query() along with all of the relevant arguments to produce
            a query. The resulting query is AND'ed with the `query` parameter (if provided).
        :param query_options: query parameters options (see ParametersOptions)
        :param query: Optional query object (mongoengine.Q)
        :param override_projection: A list of projection fields overriding any projection specified in the `param_dict`
            argument
        :param allow_public: If True, objects marked as public (no associated company) are also queried.
        :return: A list of objects matching the query.
        """
override_collation = None
⋮----
override_collation = cls._get_collation_override(field)
⋮----
data_getter = partial(
⋮----
"""
        Fetch all public documents matching a provided query.
        :param query: Optional query object (mongoengine.Q).
        :param projection: A list of projection fields.
        :return: A list of documents matching the query.
        """
q = get_company_or_none_constraint()
_query = (q & query) if query else q
⋮----
@staticmethod
    def _get_qs_with_ordering(qs: QuerySet, order_by: Sequence)
⋮----
disk_use_setting = mongo_conf.get("allow_disk_use.sort", None)
⋮----
qs = qs.allow_disk_use(disk_use_setting)
⋮----
"""
        Fetch all documents matching a provided query.
        This is a company-less version for internal uses. We assume the caller has either added any necessary
        constraints to the query or that no constraints are required.

        NOTE: BE VERY CAREFUL WITH THIS CALL, as it allows returning data across companies.

        :param query: Query object (mongoengine.Q)
        :param parameters: Parameters dict from which paging ordering and searching parameters are extracted.
        :param override_projection: A list of projection fields overriding any projection specified in the `param_dict`
            argument
        """
⋮----
search_text = parameters.get(cls._search_text_key)
order_by = cls.validate_order_by(parameters=parameters, search_text=search_text)
⋮----
override_collation = cls._get_collation_override(order_by[0])
⋮----
qs = cls.objects(query)
⋮----
qs = qs.collation(collation=override_collation)
⋮----
qs = qs.search_text(search_text)
⋮----
# add ordering
qs = cls._get_qs_with_ordering(qs, order_by)
⋮----
# add projection
qs = qs.only(*include)
⋮----
qs = qs.exclude(*exclude)
⋮----
qs = qs.fields(**projection_fields)
⋮----
# add paging
qs = qs.skip(start).limit(size)
⋮----
"""
        In case the order_field is one of the cls fields and the sorting is ascending
        then return the tuple of 2 queries:
        1. original query with not empty constraint on the order_by field
        2. original query with empty constraint on the order_by field
        """
⋮----
mongo_field_name = order_field.replace(".", "__")
mongo_field = first(
⋮----
params = {"is_list": True}
⋮----
params = {"empty_value": ""}
⋮----
params = {}
non_empty = query & field_exists(mongo_field_name, **params)
empty = query & field_does_not_exist(mongo_field_name, **params)
⋮----
"""
        Fetch all documents matching a provided query. For the first order by field
        the None values are sorted in the end regardless of the sorting order.
        If the first order field is a user defined parameter (either from execution.parameters,
        or from last_metrics) then the collation is set that sorts strings in numeric order where possible.
        This is a company-less version for internal uses. We assume the caller has either added any necessary
        constraints to the query or that no constraints are required.

        NOTE: BE VERY CAREFUL WITH THIS CALL, as it allows returning data across companies.

        :param query: Query object (mongoengine.Q)
        :param parameters: Parameters dict from which paging ordering and searching parameters are extracted.
        :param override_projection: A list of projection fields overriding any projection specified in the `param_dict`
            argument
        """
⋮----
query_sets = [cls.objects(query)]
⋮----
order_field = first(
res = cls._get_queries_for_order_field(query, order_field)
⋮----
query_sets = [cls.objects(q) for q in res]
query_sets = [cls._get_qs_with_ordering(qs, order_by) for qs in query_sets]
⋮----
override_collation = cls._get_collation_override(order_field)
⋮----
query_sets = [
⋮----
query_sets = [qs.search_text(search_text) for qs in query_sets]
⋮----
query_sets = [qs.only(*include) for qs in query_sets]
⋮----
query_sets = [qs.exclude(*exclude) for qs in query_sets]
⋮----
query_sets = [qs.fields(**projection_fields) for qs in query_sets]
⋮----
ret = []
last_set = len(query_sets) - 1
⋮----
last_size = len(ret)
⋮----
added = len(ret) - last_size
⋮----
start = 0
size = max(0, size - added)
⋮----
_only = list(set(_only) | {"company"})
result = cls.get(*args, _only=_only, include_public=True, **kwargs)
⋮----
object_name = cls.__name__.lower()
⋮----
class UpdateMixin(object)
⋮----
__user_set_allowed_fields = None
__locked_when_published_fields = None
⋮----
@classmethod
    def user_set_allowed(cls)
⋮----
@classmethod
    def locked_when_published(cls)
⋮----
@classmethod
    def get_safe_update_dict(cls, fields)
⋮----
valid_fields = cls.user_set_allowed()
fields = [(k, v, fields[k]) for k, v in valid_fields.items() if k in fields]
update_dict = {
⋮----
update_dict = cls.get_safe_update_dict(partial_update_dict)
⋮----
update_count = cls.objects(id=id, company=company_id).update(
⋮----
class DbModelMixin(GetMixin, ProperDictMixin, UpdateMixin)
⋮----
"""Provide convenience methods for a subclass of mongoengine.Document"""
⋮----
"""
        Aggregate objects of this document class according to the provided pipeline.
        :param pipeline: a list of dictionaries describing the pipeline stages
        :param allow_disk_use: if True, allow the server to use disk space if aggregation query cannot fit in memory.
        If None, default behavior will be used (see apiserver.conf/mongo/aggregate/allow_disk_use)
        :param kwargs: additional keyword arguments passed to mongoengine
        :return:
        """
⋮----
items = list(cls.objects(id__in=ids, company=company_id).only("id"))
update: dict = dict(set__company_origin=company_id, set__company="")
⋮----
items = list(
update: dict = dict(set__company=company_id, unset__company_origin=1)
⋮----
missing = tuple(set(ids).difference(i.id for i in items))
⋮----
def validate_id(cls, company, **kwargs)
⋮----
"""
    Validate existence of objects with certain IDs. within company.
    :param cls: Model class to search in
    :param company: Company to search in
    :param kwargs: Mapping of field name to object ID. If any ID does not have a corresponding object,
                    it will be reported along with the name it was assigned to.
    :return:
    """
ids = set(kwargs.values())
objs = list(cls.objects(company=company, id__in=ids).only("id"))
missing = ids - set(x.id for x in objs)
⋮----
id_to_name = {}
</file>

<file path="apiserver/database/model/company.py">
class ReportStatsOption(EmbeddedDocument)
⋮----
enabled = BooleanField(default=False)  # opt-in for statistics reporting
enabled_version = StringField()  # server version when enabled
enabled_time = DateTimeField()  # time when enabled
enabled_user = StringField()  # ID of user who enabled
⋮----
class CompanyDefaults(EmbeddedDocument)
⋮----
cluster = StringField()
stats_option = EmbeddedDocumentField(ReportStatsOption, default=ReportStatsOption)
⋮----
class Company(DbModelMixin, Document)
⋮----
meta = {"db_alias": Database.backend, "strict": strict}
⋮----
id = StringField(primary_key=True)
name = StrippedStringField(min_length=3)
defaults = EmbeddedDocumentField(CompanyDefaults, default=CompanyDefaults)
⋮----
@classmethod
    def _prepare_perm_query(cls, company, allow_public=False)
⋮----
""" Override default behavior since a 'company' constraint is not supported for this document... """
</file>

<file path="apiserver/database/model/metadata.py">
class MetadataItem(EmbeddedDocument, ProperDictMixin)
⋮----
key = StringField(required=True)
type = StringField(required=True)
value = StringField(required=True)
⋮----
def metadata_add_or_update(cls: Type[Document], _id: str, items: Sequence[dict]) -> int
⋮----
collection: Collection = cls._get_collection()
res = collection.update_one(
⋮----
requests = [
res = collection.bulk_write(requests)
⋮----
def metadata_delete(cls: Type[Document], _id: str, keys: Sequence[str]) -> int
</file>

<file path="apiserver/database/model/model_labels.py">
class ModelLabels(SafeMapField)
⋮----
def __init__(self, *args, **kwargs)
⋮----
def validate(self, value)
⋮----
non_empty_values = list(filter(None, value.values()))
</file>

<file path="apiserver/database/model/model.py">
class Model(AttributedDocument)
⋮----
_field_collation_overrides = {
⋮----
meta = {
⋮----
# distinct queries support
⋮----
get_all_query_options = GetMixin.QueryParameterOptions(
⋮----
id = StringField(primary_key=True)
name = StrippedStringField(user_set_allowed=True, min_length=3)
parent = StringField(reference_field="Model", required=False)
project = StringField(reference_field=Project, user_set_allowed=True)
created = DateTimeField(required=True, user_set_allowed=True)
task = StringField(reference_field=Task)
comment = StringField(user_set_allowed=True)
tags = SafeSortedListField(StringField(required=True), user_set_allowed=True)
system_tags = SafeSortedListField(StringField(required=True), user_set_allowed=True)
uri = StrippedStringField(default="", user_set_allowed=True)
framework = StringField()
design = SafeDictField()
labels = ModelLabels()
ready = BooleanField(required=True)
last_update = DateTimeField()
last_change = DateTimeField()
last_changed_by = StringField()
ui_cache = SafeDictField(
company_origin = StringField(exclude_by_default=True)
metadata = SafeMapField(
last_iteration = IntField(default=0)
last_metrics = SafeMapField(field=SafeMapField(EmbeddedDocumentField(MetricEvent)))
unique_metrics = ListField(StringField(required=True), exclude_by_default=True)
⋮----
def get_index_company(self) -> str
</file>

<file path="apiserver/database/model/project.py">
class Project(AttributedDocument)
⋮----
min_name_length = 3
⋮----
get_all_query_options = GetMixin.QueryParameterOptions(
⋮----
meta = {
⋮----
id = StringField(primary_key=True)
name = StrippedStringField(
basename = StrippedStringField(required=True)
description = StringField()
created = DateTimeField(required=True)
tags = SafeSortedListField(StringField(required=True))
system_tags = SafeSortedListField(StringField(required=True))
default_output_destination = StrippedStringField()
last_update = DateTimeField()
featured = IntField(default=9999)
logo_url = StringField()
logo_blob = StringField(exclude_by_default=True)
company_origin = StringField(exclude_by_default=True)
parent = StringField(reference_field="Project")
path = ListField(StringField(required=True), exclude_by_default=True)
</file>

<file path="apiserver/database/model/queue.py">
class Entry(EmbeddedDocument, ProperDictMixin)
⋮----
""" Entry representing a task waiting in the queue """
⋮----
task = StringField(required=True, reference_field=Task)
""" Task ID """
added = DateTimeField(required=True)
""" Added to the queue """
⋮----
class Resources(EmbeddedDocument)
⋮----
cpu_usage = FloatField()
gpu_usage = FloatField()
⋮----
class Queue(DbModelMixin, Document)
⋮----
_field_collation_overrides = {
⋮----
get_all_query_options = GetMixin.QueryParameterOptions(
⋮----
meta = {
⋮----
id = StringField(primary_key=True)
name = StrippedStringField(
display_name = StringField(user_set_allowed=True)
company = StringField(required=True, reference_field=Company)
created = DateTimeField(required=True)
tags = SafeSortedListField(
system_tags = SafeSortedListField(StringField(required=True), user_set_allowed=True)
entries = EmbeddedDocumentListField(Entry, default=list)
last_update = DateTimeField()
metadata = SafeMapField(
resources = EmbeddedDocumentField(Resources)
</file>

<file path="apiserver/database/model/settings.py">
class SettingKeys
⋮----
server__uuid = "server.uuid"
⋮----
class Settings(DbModelMixin, Document)
⋮----
meta = {
⋮----
key = StringField(primary_key=True)
value = DynamicField()
⋮----
@classmethod
    def get_by_key(cls, key: str, default: Optional[Any] = None, sep: str = ".") -> Any
⋮----
key = key.strip(sep)
res = Settings.objects(key=key).first()
⋮----
key_prefix = key_prefix.strip(sep)
query = Q(key=key_prefix) | Q(key__startswith=key_prefix + sep)
res = Settings.objects(query)
⋮----
@classmethod
    def set_or_add_value(cls, key: str, value: Any, sep: str = ".") -> bool
⋮----
""" Sets a new value or adds a new key/value setting (if key does not exist) """
⋮----
res = Settings.objects(key=key).update(key=key, value=value, upsert=True)
⋮----
@classmethod
    def add_value(cls, key: str, value: Any, sep: str = ".") -> bool
⋮----
""" Adds a new key/value settings. Fails if key already exists. """
⋮----
res = cls(key=key, value=value).save(force_insert=True)
</file>

<file path="apiserver/database/model/storage_settings.py">
class AWSBucketSettings(EmbeddedDocument, ProperDictMixin)
⋮----
bucket = StringField()
subdir = StringField()
host = StringField()
key = StringField()
secret = StringField()
token = StringField()
multipart = BooleanField()
acl = StringField()
secure = BooleanField()
region = StringField()
verify = BooleanField()
use_credentials_chain = BooleanField()
⋮----
class AWSSettings(EmbeddedDocument, DbModelMixin)
⋮----
buckets = EmbeddedDocumentListField(AWSBucketSettings)
⋮----
class GoogleBucketSettings(EmbeddedDocument, ProperDictMixin)
⋮----
project = StringField()
credentials_json = StringField()
⋮----
class GoogleStorageSettings(EmbeddedDocument, DbModelMixin)
⋮----
buckets = EmbeddedDocumentListField(GoogleBucketSettings)
⋮----
class AzureStorageContainerSettings(EmbeddedDocument, ProperDictMixin)
⋮----
account_name = StringField(required=True)
account_key = StringField(required=True)
container_name = StringField()
⋮----
class AzureStorageSettings(EmbeddedDocument, DbModelMixin)
⋮----
containers = EmbeddedDocumentListField(AzureStorageContainerSettings)
⋮----
class StorageSettings(DbModelMixin, Document)
⋮----
meta = {
⋮----
id = StringField(primary_key=True)
company = StringField(required=True, unique=True)
last_update = DateTimeField()
aws: AWSSettings = EmbeddedDocumentField(AWSSettings)
google: GoogleStorageSettings = EmbeddedDocumentField(GoogleStorageSettings)
azure: AzureStorageSettings = EmbeddedDocumentField(AzureStorageSettings)
</file>

<file path="apiserver/database/model/task/metrics.py">
class MetricEvent(EmbeddedDocument)
⋮----
meta = {
⋮----
# For backwards compatibility reasons
⋮----
metric = StringField(required=True)
variant = StringField(required=True)
value = DynamicField(required=True)
min_value = DynamicField()  # for backwards compatibility reasons
min_value_iteration = IntField()
max_value = DynamicField()  # for backwards compatibility reasons
max_value_iteration = IntField()
first_value = FloatField()
first_value_iteration = IntField()
count = IntField()
mean_value = FloatField()
x_axis_label = StringField()
⋮----
class EventStats(EmbeddedDocument)
⋮----
last_update = LongField()
⋮----
class MetricEventStats(EmbeddedDocument)
⋮----
event_stats_by_type = SafeMapField(field=EmbeddedDocumentField(EventStats))
</file>

<file path="apiserver/database/model/task/output.py">
class Result(object)
⋮----
success = 'success'
failure = 'failure'
⋮----
class Output(EmbeddedDocument)
⋮----
destination = StrippedStringField()
error = StringField(user_set_allowed=True)
result = StringField(choices=get_options(Result))
</file>

<file path="apiserver/database/model/task/task.py">
DEFAULT_LAST_ITERATION = 0
⋮----
class TaskStatus(object)
⋮----
created = "created"
queued = "queued"
in_progress = "in_progress"
stopped = "stopped"
publishing = "publishing"
published = "published"
closed = "closed"
failed = "failed"
completed = "completed"
unknown = "unknown"
⋮----
class TaskStatusMessage(object)
⋮----
stopping = "stopping"
⋮----
class TaskSystemTags(object)
⋮----
development = "development"
⋮----
class Script(EmbeddedDocument, ProperDictMixin)
⋮----
binary = StringField(default="python", strip=True)
repository = StringField(default="", strip=True)
tag = StringField(strip=True)
branch = StringField(strip=True)
version_num = StringField(strip=True)
entry_point = StringField(default="", strip=True)
working_dir = StringField(strip=True)
requirements = SafeDictField()
diff = StringField()
⋮----
class ArtifactTypeData(EmbeddedDocument)
⋮----
preview = StringField()
content_type = StringField()
data_hash = StringField()
⋮----
class ArtifactModes
⋮----
input = "input"
output = "output"
⋮----
DEFAULT_ARTIFACT_MODE = ArtifactModes.output
⋮----
class Artifact(EmbeddedDocument)
⋮----
key = StringField(required=True)
type = StringField(required=True)
mode = StringField(
uri = StringField()
hash = StringField()
content_size = LongField()
timestamp = LongField()
type_data = EmbeddedDocumentField(ArtifactTypeData)
display_data = SafeSortedListField(
⋮----
class ParamsItem(EmbeddedDocument, ProperDictMixin)
⋮----
section = StringField(required=True)
name = StringField(required=True)
value = StringField(required=True)
type = StringField()
description = StringField()
⋮----
class ConfigurationItem(EmbeddedDocument, ProperDictMixin)
⋮----
class TaskModelTypes
⋮----
TaskModelNames = {
⋮----
class ModelItem(EmbeddedDocument, ProperDictMixin)
⋮----
model = StringField(required=True, reference_field="Model")
updated = DateTimeField()
⋮----
class Models(EmbeddedDocument, ProperDictMixin)
⋮----
input: Sequence[ModelItem] = EmbeddedDocumentListField(ModelItem, default=list)
output: Sequence[ModelItem] = EmbeddedDocumentListField(ModelItem, default=list)
⋮----
class Execution(EmbeddedDocument, ProperDictMixin)
⋮----
meta = {"strict": strict}
test_split = IntField(default=0)
parameters = SafeDictField(default=dict)
model_desc = SafeMapField(StringField(default=""))
model_labels = ModelLabels()
framework = StringField()
artifacts: Dict[str, Artifact] = SafeMapField(field=EmbeddedDocumentField(Artifact))
queue = StringField(reference_field="Queue")
""" Queue ID where task was queued """
⋮----
class TaskType(object)
⋮----
training = "training"
testing = "testing"
inference = "inference"
data_processing = "data_processing"
application = "application"
monitor = "monitor"
controller = "controller"
report = "report"
optimizer = "optimizer"
service = "service"
qc = "qc"
custom = "custom"
⋮----
external_task_types = set(get_options(TaskType))
⋮----
class Task(AttributedDocument)
⋮----
_field_collation_overrides = {
⋮----
meta = {
⋮----
("status", "last_update"),  # for maintenance tasks
⋮----
# distinct queries support
⋮----
get_all_query_options = GetMixin.QueryParameterOptions(
⋮----
id = StringField(primary_key=True)
name = StrippedStringField(
⋮----
type = StringField(required=True, choices=get_options(TaskType))
status = StringField(default=TaskStatus.created, choices=get_options(TaskStatus))
status_reason = StringField()
status_message = StringField(user_set_allowed=True)
status_changed = DateTimeField()
comment = StringField(user_set_allowed=True)
report = StringField()
report_assets = ListField(StringField())
created = DateTimeField(required=True, user_set_allowed=True)
started = DateTimeField()
completed = DateTimeField()
published = DateTimeField()
active_duration = IntField(default=None)
parent = StringField(reference_field="Task")
project = StringField(reference_field=Project, user_set_allowed=True)
output: Output = EmbeddedDocumentField(Output, default=Output)
execution: Execution = EmbeddedDocumentField(Execution, default=Execution)
tags = SafeSortedListField(StringField(required=True), user_set_allowed=True)
system_tags = SafeSortedListField(StringField(required=True), user_set_allowed=True)
script: Script = EmbeddedDocumentField(Script, default=Script)
last_worker = StringField()
last_worker_report = DateTimeField()
last_update = DateTimeField()
last_change = DateTimeField()
last_iteration = IntField(default=DEFAULT_LAST_ITERATION)
last_metrics = SafeMapField(field=SafeMapField(EmbeddedDocumentField(MetricEvent)))
unique_metrics = ListField(StringField(required=True), exclude_by_default=True)
metric_stats = SafeMapField(field=EmbeddedDocumentField(MetricEventStats))
company_origin = StringField(exclude_by_default=True)
duration = IntField()  # obsolete, do not use
hyperparams = SafeMapField(field=SafeMapField(EmbeddedDocumentField(ParamsItem)))
configuration = SafeMapField(field=EmbeddedDocumentField(ConfigurationItem))
runtime = SafeDictField(default=dict)
models: Models = EmbeddedDocumentField(Models, default=Models)
container = SafeMapField(field=NullableStringField())
enqueue_status = StringField(
last_changed_by = StringField()
⋮----
def get_index_company(self) -> str
⋮----
"""
        Returns the company ID used for locating indices containing task data.
        In case the task has a valid company, this is the company ID.
        Otherwise, if the task has a company_origin, this is a task that has been made public and the
         origin company should be used.
        Otherwise, an empty company is used.
        """
</file>

<file path="apiserver/database/model/url_to_delete.py">
class StorageType(StrEnum)
⋮----
fileserver = "fileserver"
s3 = "s3"
azure = "azure"
gs = "gs"
unknown = "unknown"
⋮----
class FileType(StrEnum)
⋮----
file = "file"
folder = "folder"
⋮----
class DeletionStatus(StrEnum)
⋮----
created = "created"
retrying = "retrying"
failed = "failed"
⋮----
class UrlToDelete(AttributedDocument)
⋮----
_field_collation_overrides = {
⋮----
meta = {
⋮----
id = StringField(primary_key=True)
url = StringField(required=True, unique_with="company")
task = StringField(required=True)
created = DateTimeField(required=True)
storage_type = EnumField(StorageType, default=StorageType.unknown)
type = EnumField(FileType, default=FileType.file)
retry_count = IntField(default=0)
last_failure_time = DateTimeField()
last_failure_reason = StringField()
status = EnumField(DeletionStatus, default=DeletionStatus.created)
</file>

<file path="apiserver/database/model/user.py">
class User(DbModelMixin, Document)
⋮----
meta = {
get_all_query_options = GetMixin.QueryParameterOptions(list_fields=("id",))
⋮----
id = StringField(primary_key=True)
company = StringField(required=True, reference_field=Company)
name = StringField(required=True, user_set_allowed=True)
family_name = StringField(user_set_allowed=True)
given_name = StringField(user_set_allowed=True)
avatar = StringField()
preferences = DynamicField(default="", exclude_by_default=True)
created_in_version = StringField()
created = DateTimeField()
</file>

<file path="apiserver/database/model/version.py">
class Version(DbModelMixin, Document)
⋮----
meta = {
⋮----
"collection": "versions",  # custom collection name ('version' is not a proper collection name...)
"db_alias": Database.backend,  # although we'll use this model for all databases, a default must be defined
⋮----
id = StringField(primary_key=True)
num = StringField(required=True)
created = DateTimeField(required=True)
desc = StringField()
</file>

<file path="apiserver/database/projection.py">
SEP = "."
max_items_per_fetch = config.get("services._mongo.max_page_size", 500)
⋮----
class _ReferenceProxy(dict)
⋮----
def __init__(self, id)
⋮----
class _ProxyManager
⋮----
lock = threading.Lock()
⋮----
def __init__(self)
⋮----
def add(self, id)
⋮----
proxy = self._proxies.get(id)
⋮----
proxy = self._proxies[id] = _ReferenceProxy(id)
⋮----
def update(self, result)
⋮----
proxy = self._proxies.get(result.get("id"))
⋮----
class ProjectionHelper(object)
⋮----
pool = ThreadPoolExecutor()
exclusion_prefix = "-"
⋮----
@property
    def doc_projection(self)
⋮----
def __init__(self, doc_cls, projection, expand_reference_ids=False)
⋮----
def _collect_projection_fields(self, doc_cls, projection)
⋮----
"""
        Collect projection for the given document into immediate document projection and reference documents projection
        :param doc_cls: Document class
        :param projection: List of projection fields
        :return: A tuple of document projection and reference fields information
        """
doc_projection = (
⋮----
)  # Projection fields for this class (used in the main query)
ref_projection_info = (
⋮----
)  # Projection information for reference fields (used in join queries)
⋮----
field_ = field.lstrip(self.exclusion_prefix)
⋮----
# Doesn't start with a reference field
⋮----
# Field is exactly a reference field. In this case we won't perform any inner projection (for that,
# use '<reference field name>.*')
⋮----
subfield = field_[len(ref_field) :]
⋮----
# Starts with something that looks like a reference field, but isn't
⋮----
# Not a reference field, just add to the top-level projection
# We strip any trailing '*' since it means nothing for simple fields and for embedded documents
orig_field = field
⋮----
field = field[:-2]
⋮----
def _parse_projection(self, projection)
⋮----
"""
        Prepare the projection data structures for get_many_with_join().
        :param projection: A list of field names that should be returned by the query. Sub-fields can be specified
            using '.' (i.e. "parent.name"). A field terminated by '.*' indicated that all of the field's sub-fields
            should be returned (only relevant for fields that represent sub-documents or referenced documents)
        :type projection: list of strings
        :returns A tuple of (class fields projection, reference fields projection)
        """
doc_cls = self._doc_cls
⋮----
def normalize_cls_projection(cls_, fields)
⋮----
""" Normalize projection for this class and group (expand *, for once) """
⋮----
def compute_ref_cls_projection(cls_, group)
⋮----
""" Compute inner projection for this class and group """
subfields = set([x[2] for x in group if x[2]])
⋮----
def sort_key(proj_info)
⋮----
# Aggregate by reference field. We'll leave out '*' from the projected items since
ref_projection = {
⋮----
# Make sure this doesn't contain any reference field we'll join anyway
# (i.e. in case only_fields=[project, project.name])
doc_projection = normalize_cls_projection(
⋮----
# Make sure that in case one or more field is a subfield of another field, we only use the the top-level field.
# This is done since in such a case, MongoDB will only use the most restrictive field (most nested field) and
# won't return some of the data we need.
# This way, we make sure to use the most inclusive field that contains all requested subfields.
projection_set = set(doc_projection)
doc_projection = [
⋮----
# Make sure we didn't get any invalid projection fields for this class
invalid_fields = [
⋮----
# Join mode - use both normal projection fields and top-level reference fields
doc_projection = set(doc_projection)
⋮----
doc_projection = list(doc_projection)
⋮----
# If there are include fields (not only exclude) then add an id field
⋮----
"""
        Search for a path in the given object, return the list of values found for the
        given path (multiple values may exist if the path is a glob expression)
        :param doc_cls: The document class represented by the object
        :param obj: Data object
        :param path: Path to a leaf in the data object ("." separated, may contain "*")
         (in case the path contains "*", there may be multiple values)
        :param factory: If provided, replace each value found with an instance provided by the factory.
        """
norm_path = doc_cls.get_dpath_translated_path(path)
globlist = norm_path.strip(SEP).split(SEP)
⋮----
def _search_and_replace(target: dict, p: Sequence[str]) -> Sequence[str]
⋮----
parent = None
⋮----
parent = target
target = target[part]
⋮----
def project(self, results, projection_func)
⋮----
"""
        Perform projection on query results, using the provided projection func.
        :param results: A list of results dictionaries on which projection should be performed
        :param projection_func: A callable that receives a document type, list of ids and projection and returns query
            results. This callable is used in order to perform sub-queries during projection
        :return: Modified results (in-place)
        """
cls = self._doc_cls
ref_projection = self._ref_projection
⋮----
# Join mode - get results for each reference fields projection required (this is the join step)
# Note: this is a recursive step, so nested reference fields are supported
⋮----
def collect_ids(ref_field_name)
⋮----
"""
                Collect unique IDs for the given reference path from all result documents.
                All collected IDs are replaced in the result dictionaries with a reference proxy generated by the
                proxies manager to allow rapid update later on when projection results are obtained.
                """
all_ids = (
⋮----
items = [
⋮----
def do_projection(item)
⋮----
doc_type = data["cls"]
doc_only = list(filter(None, data["only"]))
doc_only = list({"id"} | set(doc_only)) if doc_only else None
⋮----
# From ThreadPoolExecutor.map() documentation: If a call raises an exception then that exception
#  will be raised when its value is retrieved from the map() iterator
⋮----
def do_expand_reference_ids(result, skip_fields=None)
⋮----
ref_fields = cls.get_reference_fields()
⋮----
ref_fields = set(ref_fields) - set(skip_fields)
⋮----
# any reference field not projected should be expanded
⋮----
def _expand_reference_fields(self, doc_cls, result, fields)
⋮----
def expand_reference_ids(self, doc_cls, result)
</file>

<file path="apiserver/database/props.py">
class PropsMixin(object)
⋮----
__cached_fields = None
__cached_reference_fields = None
__cached_exclude_fields = None
__cached_fields_with_instance = None
__cached_all_fields_with_instance = None
⋮----
__cached_dpath_computed_fields_lock = Lock()
__cached_dpath_computed_fields = None
⋮----
_document_classes = {}
⋮----
def __init_subclass__(cls, **kwargs)
⋮----
@classmethod
    def get_fields(cls)
⋮----
@classmethod
    def get_all_fields_with_instance(cls)
⋮----
@classmethod
    def get_fields_with_instance(cls, doc_cls)
⋮----
@staticmethod
    def _get_fields_with_attr(cls_, attr)
⋮----
""" Get all fields with the specified attribute (supports nested fields) """
res = get_fields_attr(cls_, attr=attr)
⋮----
def resolve_doc(v)
⋮----
doc_cls = PropsMixin._document_classes.get(v)
⋮----
fields = {k: resolve_doc(v) for k, v in res.items()}
⋮----
def collect_embedded_docs(doc_cls, embedded_doc_field_getter)
⋮----
embedded_doc_cls = embedded_doc_field_getter(
⋮----
@classmethod
    def _translate_fields_path(cls, parts)
⋮----
current_cls = cls
translated_parts = []
⋮----
current_cls = field.document_type
⋮----
current_cls = field.field.document_type
⋮----
current_cls = None
⋮----
@classmethod
    def get_reference_fields(cls)
⋮----
fields = cls._get_fields_with_attr(cls, "reference_field")
⋮----
@classmethod
    def get_extra_projection(cls, fields: Sequence) -> tuple
⋮----
fields = [fields]
⋮----
@classmethod
    def get_exclude_fields(cls)
⋮----
fields = cls._get_fields_with_attr(cls, "exclude_by_default")
⋮----
@classmethod
    def get_dpath_translated_path(cls, path, separator=".")
⋮----
parts = path.split(separator)
translated = cls._translate_fields_path(parts)
result = separator.join(translated)
⋮----
def get_field_value(self, field_path: str, default=None)
⋮----
"""
        Return the document field_path value by the field_path name.
        The path may contain '.'. If on any level the path is
        not found then the default value is returned
        """
path_elements = field_path.split(".")
current = self
⋮----
current = getattr(current, name, default)
</file>

<file path="apiserver/database/query.py">
class RegexWrapper(object)
⋮----
def __init__(self, pattern, flags=None)
⋮----
@property
    def regex(self)
⋮----
class RegexMixin(object)
⋮----
def to_query(self: Union["RegexMixin", QNode], document)
⋮----
query = self.accept(SimplificationVisitor())
query = query.accept(RegexQueryCompilerVisitor(document))
⋮----
def _combine(self: Union["RegexMixin", QNode], other, operation)
⋮----
"""Combine this node with another node into a QCombination
        object.
        """
⋮----
class RegexQCombination(RegexMixin, QCombination)
⋮----
class RegexQ(RegexMixin, Q)
⋮----
class RegexQueryCompilerVisitor(QueryCompilerVisitor)
⋮----
"""
    Improved mongoengine complied queries visitor class that supports compiled regex expressions as part of the query.

    We need this class since mongoengine's Q (QNode) class uses copy.deepcopy() as part of the tree simplification
     stage, which does not support re.compiled objects (since Python 2.5).
    This class allows users to provide regex strings wrapped in QueryRegex instances, which are lazily evaluated to
     to re.compile instances just before being visited for compilation (this is done after the simplification stage)
    """
⋮----
def visit_query(self, query)
⋮----
query = copy.deepcopy(query)
⋮----
def _transform_query(self, query)
</file>

<file path="apiserver/database/utils.py">
def get_fields(cls, of_type=BaseField, return_instance=False, subfields=False)
⋮----
def get_fields_attr(cls, attr)
⋮----
""" get field names from a class containing mongoengine fields """
⋮----
def get_fields_choices(cls, attr)
⋮----
def get_choices(field_name: str, field: BaseField) -> Tuple
⋮----
fields = []
⋮----
field_path = path + (field_name,)
⋮----
full_name = "__".join(field_path)
⋮----
def get_items(cls)
⋮----
""" get key/value items from an enum-like class (members represent enumeration key/value) """
⋮----
res = {k: v for k, v in getmembers(cls) if not (k.startswith("_") or ismethod(v))}
⋮----
def get_options(cls)
⋮----
""" get options from an enum-like class (members represent enumeration key/value) """
⋮----
# return a dictionary of items which:
# 1. are in the call_data
# 2. are in the fields dictionary, and their value in the call_data matches the type in fields
# 3. are in the cls_fields
def parse_from_call(call_data, fields, cls_fields, discard_none_values=True)
⋮----
# fields should be key=>type dict
fields = {k: None for k in fields}
fields = {k: v for k, v in fields.items() if k in cls_fields}
res = {}
⋮----
value = call_data.get(field)
⋮----
# we'll keep the None value in case the field actually exists in the call data
⋮----
def init_cls_from_base(cls, instance)
⋮----
def get_company_or_none_constraint(company="")
⋮----
def field_does_not_exist(field: str, empty_value=None, is_list=False) -> Q
⋮----
"""
    Creates a query object used for finding a field that doesn't exist, or has None or an empty value.
    :param field: Field name
    :param empty_value: The empty value to test for (None means no specific empty value will be used)
    :param is_list: Is this a list (array) field. In this case, instead of testing for an empty value,
                    the length of the array will be used (len==0 means empty)
    :return:
    """
query = Q(**{f"{field}__exists": False}) | Q(
⋮----
def field_exists(field: str, empty_value=None, is_list=False) -> Q
⋮----
"""
    Creates a query object used for finding a field that exists and is not None or empty.
    :param field: Field name
    :param empty_value: The empty value to test for (None means no specific empty value will be used)
    :param is_list: Is this a list (array) field. In this case, instead of testing for an empty value,
                    the length of the array will be used (len==0 means empty)
    :return:
    """
query = Q(**{f"{field}__exists": True}) & Q(
⋮----
def get_subkey(d, key_path, default=None)
⋮----
""" Get a key from a nested dictionary. kay_path is a '.' separated string of keys used to traverse
        the nested dictionary.
    """
keys = key_path.split(".")
⋮----
d = d.get(key)
⋮----
def id()
⋮----
def hash_field_name(s)
⋮----
""" Hash field name into a unique safe string """
⋮----
def merge_dicts(*dicts)
⋮----
base = {}
⋮----
def filter_fields(cls, fields)
⋮----
"""From the fields dictionary return only the fields that match cls fields"""
⋮----
def _names_set(*names: str) -> Set[str]
⋮----
"""
    Given a list of names return set with names and '-names'
    """
⋮----
_system_tag_names = {
⋮----
_system_tag_prefixes = {"task": _names_set("annotat")}
⋮----
"""
    Partition the given tags sequence into system and user-defined tags
    :param entity: The name of the entity that defines the list of the system tags
    :param tags: The tags to partition
    :param system_tags: Optional. If passed then these tags are considered system together
    with those defined for the entity.
    :return: a tuple where the first element is the sequence of user-defined tags and
    the second element is the sequence of system tags
    """
tags = set(tags)
system_tags = set(system_tags)
⋮----
prefixes = system_tag_prefixes.get(entity, [])
</file>

<file path="apiserver/documentation/api_versions.md">
### Supported api versions

| Release | ApiVersion |
|---------|------------|
| v2.3    | 2.34       |
| v2.2    | 2.33       |
| v2.1    | 2.32       |
| v2.0    | 2.31       | 
| v1.17   | 2.31       |
| v1.16   | 2.30       |
| v1.15   | 2.29       |
| v1.14   | 2.28       |
| v1.13   | 2.27       |
| v1.12   | 2.26       |
| v1.11   | 2.25       |
| v1.10   | 2.24       |
| v1.9    | 2.23       |
| v1.8    | 2.22       |
| v1.7    | 2.21       |
| v1.6    | 2.20       |
| v1.5    | 2.19       |
| v1.4    | 2.18       |
| v1.3    | 2.17       |
| v1.2    | 2.16       |
| v1.1    | 2.15       |
| v1.0    | 2.14       |
| v0.17   | 2.13       |
</file>

<file path="apiserver/elastic/apply_mappings.py">
#!/usr/bin/env python3
"""
Apply elasticsearch mappings to given hosts.
"""
⋮----
HERE = Path(__file__).resolve().parent
⋮----
"""Hosts maybe a sequence of strings or dicts in the form {"host": <host>, "port": <port>}"""
⋮----
def _send_component_template(ct_file)
⋮----
body = json.load(json_data)
template_name = f"{ct_file.stem}"
res = es.cluster.put_component_template(name=template_name, body=body)
⋮----
def _send_index_template(it_file)
⋮----
template_name = f"{it_file.stem}"
res = es.indices.put_index_template(name=template_name, body=body)
⋮----
# def _send_legacy_template(f):
#     with f.open() as json_data:
#         data = json.load(json_data)
#         template_name = f.stem
#         res = es.indices.put_template(name=template_name, body=data)
#         return {"mapping": template_name, "result": res}
⋮----
def _delete_legacy_templates(legacy_folder)
⋮----
res_list = []
⋮----
template_name = lt.stem
⋮----
res = es.indices.delete_template(name=template_name)
⋮----
es = Elasticsearch(hosts=hosts, http_auth=http_auth, **(es_args or {}))
root = HERE / "index_templates"
⋮----
folders = [root / key]
⋮----
folders = [f for f in root.iterdir() if f.is_dir()]
⋮----
ret = []
⋮----
legacy_root = HERE / "mappings"
⋮----
legacy_f = legacy_root / f.stem
⋮----
# p = HERE / "mappings"
# if key:
#     files = (p / key).glob("*.json")
# else:
#     files = p.glob("**/*.json")
#
# return [_send_template(f) for f in files]
⋮----
def parse_args()
⋮----
parser = argparse.ArgumentParser(
⋮----
def main()
⋮----
args = parse_args()
⋮----
res = apply_mappings_to_cluster(args.hosts, args.key)
</file>

<file path="apiserver/elastic/index_templates/events/component_templates/events_common.json">
{
  "template": {
    "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 1
    },
    "mappings": {
      "_source": {
        "enabled": true
      },
      "properties": {
        "@timestamp": {
          "type": "date"
        },
        "task": {
          "type": "keyword"
        },
        "type": {
          "type": "keyword"
        },
        "worker": {
          "type": "keyword"
        },
        "timestamp": {
          "type": "date"
        },
        "iter": {
          "type": "long"
        },
        "metric": {
          "type": "keyword"
        },
        "variant": {
          "type": "keyword"
        },
        "value": {
          "type": "float"
        },
        "company_id": {
          "type": "keyword"
        },
        "model_event": {
          "type": "boolean"
        }
      }
    }
  }
}
</file>

<file path="apiserver/elastic/index_templates/events/events_log.json">
{
  "index_patterns": "events-log-*",
  "template": {
    "mappings": {
      "properties": {
        "msg": {
          "type": "text",
          "index": false
        },
        "level": {
          "type": "keyword"
        }
      }
    }
  },
  "priority": 500,
  "composed_of": ["events_common"]
}
</file>

<file path="apiserver/elastic/index_templates/events/events_plot.json">
{
  "index_patterns": "events-plot-*",
  "template": {
    "mappings": {
      "properties": {
        "plot_str": {
          "type": "text",
          "index": false
        },
        "plot_data": {
          "type": "binary"
        }
      }
    }
  },
  "priority": 500,
  "composed_of": ["events_common"]
}
</file>

<file path="apiserver/elastic/index_templates/events/events_training_debug_image.json">
{
  "index_patterns": "events-training_debug_image-*",
  "template": {
    "mappings": {
      "properties": {
        "key": {
          "type": "keyword"
        },
        "url": {
          "type": "keyword"
        }
      }
    }
  },
  "priority": 500,
  "composed_of": ["events_common"]
}
</file>

<file path="apiserver/elastic/index_templates/events/events_training_stats_scalar.json">
{
  "index_patterns": "events-training_stats_scalar-*",
  "priority": 500,
  "composed_of": ["events_common"]
}
</file>

<file path="apiserver/elastic/index_templates/workers/queue_metrics.json">
{
  "index_patterns": "queue_metrics_*",
  "template": {
    "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 1
    },
    "mappings": {
      "_source": {
        "enabled": true
      },
      "properties": {
        "timestamp": {
          "type": "date"
        },
        "queue": {
          "type": "keyword"
        },
        "average_waiting_time": {
          "type": "float"
        },
        "queue_length": {
          "type": "integer"
        },
        "company_id": {
          "type": "keyword"
        }
      }
    }
  }
}
</file>

<file path="apiserver/elastic/index_templates/workers/serving_stats.json">
{
  "index_patterns": "serving_stats_*",
  "template": {
    "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 1
    },
    "mappings": {
      "_source": {
        "enabled": true
      },
      "properties": {
        "timestamp": {
          "type": "date"
        },
        "container_id": {
          "type": "keyword"
        },
        "company_id": {
          "type": "keyword"
        },
        "endpoint_url": {
          "type": "keyword"
        },
        "requests_num": {
          "type": "integer"
        },
        "requests_min": {
          "type": "float"
        },
        "uptime_sec": {
          "type": "integer"
        },
        "latency_ms": {
          "type": "integer"
        },
        "cpu_usage": {
          "type": "float"
        },
        "cpu_num": {
          "type": "integer"
        },
        "gpu_usage": {
          "type": "float"
        },
        "gpu_num": {
          "type": "integer"
        },
        "memory_used": {
          "type": "float"
        },
        "memory_free": {
          "type": "float"
        },
        "memory_total": {
          "type": "float"
        },
        "gpu_memory_used": {
          "type": "float"
        },
        "gpu_memory_free": {
          "type": "float"
        },
        "gpu_memory_total": {
          "type": "float"
        },
        "disk_free_home": {
          "type": "float"
        },
        "network_rx": {
          "type": "float"
        },
        "network_tx": {
          "type": "float"
        }
      }
    }
  }
}
</file>

<file path="apiserver/elastic/index_templates/workers/worker_stats.json">
{
  "index_patterns": "worker_stats_*",
  "template": {
    "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 1
    },
    "mappings": {
      "_source": {
        "enabled": true
      },
      "properties": {
        "timestamp": {
          "type": "date"
        },
        "worker": {
          "type": "keyword"
        },
        "category": {
          "type": "keyword"
        },
        "metric": {
          "type": "keyword"
        },
        "variant": {
          "type": "keyword"
        },
        "value": {
          "type": "float"
        },
        "unit": {
          "type": "keyword"
        },
        "task": {
          "type": "keyword"
        },
        "company_id": {
          "type": "keyword"
        }
      }
    }
  }
}
</file>

<file path="apiserver/elastic/initialize.py">
log = config.logger(__file__)
⋮----
class MissingElasticConfiguration(Exception)
⋮----
"""
    Exception when cluster configuration is not found in config files
    """
⋮----
class ElasticConnectionError(Exception)
⋮----
"""
    Exception when could not connect to elastic during init
    """
⋮----
class ConnectionErrorFilter(logging.Filter)
⋮----
def filter(self, record)
⋮----
filter_out = (
⋮----
def check_elastic_empty() -> bool
⋮----
"""
    Check for elasticsearch connection
    Use probing settings and not the default es cluster ones
    so that we can handle correctly the connection rejects due to ES not fully started yet
    :return:
    """
cluster_conf = es_factory.get_cluster_config("events")
max_retries = config.get("apiserver.elastic.probing.max_retries", 4)
timeout = config.get("apiserver.elastic.probing.timeout", 30)
⋮----
es_logger = logging.getLogger("elasticsearch")
log_filter = ConnectionErrorFilter(
⋮----
def events_legacy_template()
⋮----
def events_template()
⋮----
es = Elasticsearch(
⋮----
def init_es_data()
⋮----
cluster_conf = es_factory.get_cluster_config(name)
hosts_config = cluster_conf.get("hosts")
⋮----
args = cluster_conf.get("args", {})
http_auth = es_factory.get_credentials(name)
⋮----
res = apply_mappings_to_cluster(
</file>

<file path="apiserver/elastic/mappings/events/events_log.json">
{
  "index_patterns": "events-log-*",
  "order": 1,
  "mappings": {
    "properties": {
      "msg": {
        "type": "text",
        "index": false
      },
      "level": {
        "type": "keyword"
      }
    }
  }
}
</file>

<file path="apiserver/elastic/mappings/events/events_plot.json">
{
  "index_patterns": "events-plot-*",
  "order": 1,
  "mappings": {
    "properties": {
      "plot_str": {
        "type": "text",
        "index": false
      },
      "plot_data": {
        "type": "binary"
      }
    }
  }
}
</file>

<file path="apiserver/elastic/mappings/events/events_training_debug_image.json">
{
  "index_patterns": "events-training_debug_image-*",
  "order": 1,
  "mappings": {
    "properties": {
      "key": {
        "type": "keyword"
      },
      "url": {
        "type": "keyword"
      }
    }
  }
}
</file>

<file path="apiserver/elastic/mappings/events/events.json">
{
  "index_patterns": "events-*",
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1
  },
  "mappings": {
    "_source": {
      "enabled": true
    },
    "properties": {
      "@timestamp": {
        "type": "date"
      },
      "task": {
        "type": "keyword"
      },
      "type": {
        "type": "keyword"
      },
      "worker": {
        "type": "keyword"
      },
      "timestamp": {
        "type": "date"
      },
      "iter": {
        "type": "long"
      },
      "metric": {
        "type": "keyword"
      },
      "variant": {
        "type": "keyword"
      },
      "value": {
        "type": "float"
      },
      "company_id": {
        "type": "keyword"
      },
      "model_event": {
        "type": "boolean"
      }
    }
  }
}
</file>

<file path="apiserver/elastic/mappings/workers/queue_metrics.json">
{
  "index_patterns": "queue_metrics_*",
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1
  },
  "mappings": {
    "_source": {
      "enabled": true
    },
    "properties": {
      "timestamp": {
        "type": "date"
      },
      "queue": {
        "type": "keyword"
      },
      "average_waiting_time": {
        "type": "float"
      },
      "queue_length": {
        "type": "integer"
      },
      "company_id": {
        "type": "keyword"
      }
    }
  }
}
</file>

<file path="apiserver/elastic/mappings/workers/worker_stats.json">
{
  "index_patterns": "worker_stats_*",
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1
  },
  "mappings": {
    "_source": {
      "enabled": true
    },
    "properties": {
      "timestamp": {
        "type": "date"
      },
      "worker": {
        "type": "keyword"
      },
      "category": {
        "type": "keyword"
      },
      "metric": {
        "type": "keyword"
      },
      "variant": {
        "type": "keyword"
      },
      "value": {
        "type": "float"
      },
      "unit": {
        "type": "keyword"
      },
      "task": {
        "type": "keyword"
      },
      "company_id": {
        "type": "keyword"
      }
    }
  }
}
</file>

<file path="apiserver/elastic/requirements.txt">
requests>=2.21.0
</file>

<file path="apiserver/es_factory.py">
log = config.logger(__file__)
⋮----
OVERRIDE_HOST_ENV_KEY = (
OVERRIDE_PORT_ENV_KEY = (
⋮----
OVERRIDE_USERNAME_ENV_KEY = ("CLEARML_ELASTIC_SERVICE_USERNAME",)
⋮----
OVERRIDE_PASSWORD_ENV_KEY = ("CLEARML_ELASTIC_SERVICE_PASSWORD",)
⋮----
OVERRIDE_HOST = first(filter(None, map(getenv, OVERRIDE_HOST_ENV_KEY)))
⋮----
OVERRIDE_PORT = first(filter(None, map(getenv, OVERRIDE_PORT_ENV_KEY)))
⋮----
OVERRIDE_PORT = int(OVERRIDE_PORT)
⋮----
OVERRIDE_USERNAME = first(filter(None, map(getenv, OVERRIDE_USERNAME_ENV_KEY)))
⋮----
OVERRIDE_PASSWORD = first(filter(None, map(getenv, OVERRIDE_PASSWORD_ENV_KEY)))
⋮----
_instances = {}
⋮----
class MissingClusterConfiguration(Exception)
⋮----
"""
    Exception when cluster configuration is not found in config files
    """
⋮----
class InvalidClusterConfiguration(Exception)
⋮----
"""
    Exception when cluster configuration does not contain required properties
    """
⋮----
class MissingPasswordForElasticUser(Exception)
⋮----
class ESFactory
⋮----
@classmethod
    def connect(cls, cluster_name) -> Elasticsearch
⋮----
"""
        Returns the es client for the cluster.
        Connects to the cluster if did not connect previously
        :param cluster_name: Dot separated cluster path in the configuration file
        :return: es client
        :raises MissingClusterConfiguration: in case no config section is found for the cluster
        :raises InvalidClusterConfiguration: in case cluster config section misses needed properties
        """
⋮----
cluster_config = cls.get_cluster_config(cluster_name)
hosts = cluster_config.get("hosts", None)
⋮----
http_auth = cls.get_credentials(cluster_name)
⋮----
args = cluster_config.get("args", {})
⋮----
@classmethod
    def get_credentials(cls, cluster_name: str, cluster_config: dict = None) -> Optional[Tuple[str, str]]
⋮----
cluster_config = cluster_config or cls.get_cluster_config(cluster_name)
⋮----
elastic_user = OVERRIDE_USERNAME or config.get("secure.elastic.user", None)
⋮----
elastic_password = OVERRIDE_PASSWORD or config.get(
⋮----
@classmethod
    def get_all_cluster_names(cls)
⋮----
@classmethod
    def get_override_host(cls, cluster_name: str) -> Tuple[str, str]
⋮----
@classmethod
@lru_cache()
    def get_cluster_config(cls, cluster_name)
⋮----
"""
        Returns cluster config for the specified cluster path
        :param cluster_name: Dot separated cluster path in the configuration file
        :return: config section for the cluster
        :raises MissingClusterConfiguration: in case no config section is found for the cluster
        """
cluster_key = ".".join(("hosts.elastic", cluster_name))
cluster_config = config.get(cluster_key, None)
⋮----
def set_host_prop(key, value)
⋮----
@classmethod
    def connect_all(cls)
⋮----
clusters = config.get("hosts.elastic").as_plain_ordered_dict()
⋮----
@classmethod
    def instances(cls)
⋮----
@classmethod
    def timestamp_str_to_millis(cls, ts_str)
⋮----
epoch = datetime.utcfromtimestamp(0)
current_date = datetime.strptime(ts_str, "%Y-%m-%dT%H:%M:%S.%fZ")
⋮----
@classmethod
    def get_timestamp_millis(cls)
⋮----
now = datetime.utcnow()
⋮----
@classmethod
    def get_es_timestamp_str(cls)
⋮----
es_factory = ESFactory()
</file>

<file path="apiserver/fix_mongo_urls.py">
client = MongoClient(
backend_db: Database = client.backend
⋮----
def get_updated_uri(uri: str)
⋮----
relative_url = uri[len(host_source):]
⋮----
host_source = host_source
host_target = host_target
model_collection: Collection = backend_db.get_collection("model")
⋮----
models_count = model_collection.count_documents({})
updated_models = 0
⋮----
updated_uri = get_updated_uri(model.get("uri"))
⋮----
result = model_collection.update_one(
⋮----
task_collection: Collection = backend_db.get_collection("task")
⋮----
tasks_count = task_collection.count_documents({})
updated_tasks = 0
⋮----
artifacts = task.get("execution", {}).get("artifacts")
⋮----
uri_updated = False
⋮----
updated_uri = get_updated_uri(artifact.get("uri"))
⋮----
uri_updated = True
⋮----
result = task_collection.update_one(
⋮----
def normalise_host(host)
⋮----
def main()
⋮----
def valid_url_prefix(url: str)
⋮----
parser = ArgumentParser(
⋮----
args = parser.parse_args()
</file>

<file path="apiserver/jobs/async_urls_delete.py">
log = config.logger(f"JOB-{Path(__file__).name}")
conf = config.get("services.async_urls_delete")
max_retries = conf.get("max_retries", 3)
retry_timeout = timedelta(seconds=conf.get("retry_timeout_sec", 60))
storage_bll = StorageBLL()
⋮----
def mark_retry_failed(ids: Sequence[str], reason: str)
⋮----
def mark_failed(query: Q, reason: str)
⋮----
def scheme_prefix(scheme: str) -> str
⋮----
T = TypeVar("T", bound=Hashable)
⋮----
class Storage(Generic[T], metaclass=ABCMeta)
⋮----
class Client(ABC)
⋮----
@property
@abstractmethod
        def chunk_size(self) -> int
⋮----
def get_path(self, url: UrlToDelete) -> str
⋮----
@property
@abstractmethod
    def name(self) -> str
⋮----
def get_client(self, base: T, urls: Sequence[UrlToDelete]) -> Client
⋮----
def delete_urls(urls_query: Q, storage: Storage)
⋮----
to_delete = list(UrlToDelete.objects(urls_query).order_by("url").limit(10000))
⋮----
grouped_urls = storage.group_urls(to_delete)
⋮----
msg = f"Invalid {storage.name} url or missing {storage.name} configuration for account"
⋮----
client = storage.get_client(base, urls)
⋮----
failed = [url.id for url in urls]
⋮----
paths = []
path_to_id_mapping = defaultdict(list)
ids_to_delete = set()
⋮----
path = client.get_path(url)
⋮----
err = str(ex)
⋮----
failed_ids = set()
⋮----
error_ids = set(
⋮----
deleted_ids = set(
⋮----
missing_ids = ids_to_delete - deleted_ids - failed_ids
⋮----
class FileserverStorage(Storage)
⋮----
class Client(Storage.Client)
⋮----
timeout = conf.get("fileserver.timeout_sec", 300)
⋮----
def __init__(self, session: requests.Session, host: str)
⋮----
@property
        def chunk_size(self) -> int
⋮----
path = url.url.strip("/")
⋮----
res = self.session.post(
⋮----
res_data = res.json()
⋮----
token_expiration_sec = conf.get("fileserver.token_expiration_sec", 600)
⋮----
def __init__(self, company: str, fileserver_host: str = None)
⋮----
fileserver_host = fileserver_host or config.get("hosts.fileserver", None)
⋮----
def _parse_url_prefix(prefix) -> Tuple[str, str]
⋮----
url = furl(prefix)
host = f"{url.scheme}://{url.netloc}" if url.scheme else None
⋮----
url_prefixes = [
⋮----
@property
    def name(self) -> str
⋮----
def _resolve_base_url(self, url: UrlToDelete) -> Optional[str]
⋮----
"""
        For the url return the base_url containing schema, optional host and bucket name
        """
⋮----
parsed = furl(url.url)
url_host = f"{parsed.scheme}://{parsed.netloc}" if parsed.scheme else None
url_path = str(parsed.path)
⋮----
def get_client(self, base: str, urls: Sequence[UrlToDelete]) -> Client
⋮----
host = base
token = AuthBLL.get_token_for_user(
session = requests.session()
⋮----
res = session.get(url=host, timeout=self.Client.timeout)
⋮----
class AzureStorage(Storage)
⋮----
def __init__(self, container: ContainerClient)
⋮----
# noinspection PyTypeChecker
⋮----
@staticmethod
        def _path_from_request_url(request_url: str) -> str
⋮----
res = self.container.delete_blobs(*paths)
⋮----
deleted = []
errors = defaultdict(list)
⋮----
def __init__(self, company: str)
⋮----
def _resolve_base_url(self, url: UrlToDelete) -> Optional[Tuple]
⋮----
parsed = urlparse(url.url)
⋮----
azure_conf = self.configs.get_config_by_uri(url.url)
⋮----
account_url = parsed.netloc
⋮----
def get_client(self, base: Tuple, urls: Sequence[UrlToDelete]) -> Client
⋮----
sample_url = urls[0].url
cfg = self.configs.get_config_by_uri(sample_url)
⋮----
class AWSStorage(Storage)
⋮----
def __init__(self, base_url: str, container: AWSBucket)
⋮----
""" Normalize remote path. Remove any prefix that is already handled by the container """
path = url.url
⋮----
path = path[len(self.base_url) :]
path = path.lstrip("/")
⋮----
res = self.container.delete_objects(
⋮----
msg = err.get("Message", "")
⋮----
s3_conf = self.configs.get_config_by_uri(url.url)
⋮----
s3_bucket = s3_conf.bucket
⋮----
parts = Path(parsed.path.strip("/")).parts
⋮----
s3_bucket = parts[0]
⋮----
boto_kwargs = {
name = base[len(scheme_prefix(self.scheme)) :]
bucket_name = name[len(cfg.host) + 1 :] if cfg.host else name
⋮----
class GoogleCloudStorage(Storage)
⋮----
def __init__(self, base_url: str, container: google_storage.Bucket)
⋮----
not_found = set()
⋮----
def error_callback(blob: google_storage.Blob)
⋮----
errors = {"Not found": list(not_found)} if not_found else {}
⋮----
gs_conf = self.configs.get_config_by_uri(url.url)
⋮----
credentials = service_account.Credentials.from_service_account_file(
⋮----
credentials = None
⋮----
bucket_name = base[len(scheme_prefix(self.scheme)) :]
⋮----
def run_delete_loop(fileserver_host: str)
⋮----
storage_helpers = {
⋮----
now = datetime.utcnow()
urls_query = (
⋮----
url_to_delete: UrlToDelete = UrlToDelete.objects(
⋮----
company = url_to_delete.company
user = url_to_delete.user
storage_type = url_to_delete.storage_type
⋮----
company_storage_urls_query = urls_query & Q(
⋮----
def main()
⋮----
parser = ArgumentParser(description=__doc__)
⋮----
args = parser.parse_args()
</file>

<file path="apiserver/LICENSE">
Server Side Public License
                     VERSION 1, OCTOBER 16, 2018

                    Copyright © 2025 ClearML Inc.

  Everyone is permitted to copy and distribute verbatim copies of this
  license document, but changing it is not allowed.

                       TERMS AND CONDITIONS

  0. Definitions.
  
  “This License” refers to Server Side Public License.

  “Copyright” also means copyright-like laws that apply to other kinds of
  works, such as semiconductor masks.

  “The Program” refers to any copyrightable work licensed under this
  License.  Each licensee is addressed as “you”. “Licensees” and
  “recipients” may be individuals or organizations.

  To “modify” a work means to copy from or adapt all or part of the work in
  a fashion requiring copyright permission, other than the making of an
  exact copy. The resulting work is called a “modified version” of the
  earlier work or a work “based on” the earlier work.

  A “covered work” means either the unmodified Program or a work based on
  the Program.

  To “propagate” a work means to do anything with it that, without
  permission, would make you directly or secondarily liable for
  infringement under applicable copyright law, except executing it on a
  computer or modifying a private copy. Propagation includes copying,
  distribution (with or without modification), making available to the
  public, and in some countries other activities as well.

  To “convey” a work means any kind of propagation that enables other
  parties to make or receive copies. Mere interaction with a user through a
  computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays “Appropriate Legal Notices” to the
  extent that it includes a convenient and prominently visible feature that
  (1) displays an appropriate copyright notice, and (2) tells the user that
  there is no warranty for the work (except to the extent that warranties
  are provided), that licensees may convey the work under this License, and
  how to view a copy of this License. If the interface presents a list of
  user commands or options, such as a menu, a prominent item in the list
  meets this criterion.

  1. Source Code.

  The “source code” for a work means the preferred form of the work for
  making modifications to it. “Object code” means any non-source form of a
  work.

  A “Standard Interface” means an interface that either is an official
  standard defined by a recognized standards body, or, in the case of
  interfaces specified for a particular programming language, one that is
  widely used among developers working in that language.  The “System
  Libraries” of an executable work include anything, other than the work as
  a whole, that (a) is included in the normal form of packaging a Major
  Component, but which is not part of that Major Component, and (b) serves
  only to enable use of the work with that Major Component, or to implement
  a Standard Interface for which an implementation is available to the
  public in source code form. A “Major Component”, in this context, means a
  major essential component (kernel, window system, and so on) of the
  specific operating system (if any) on which the executable work runs, or
  a compiler used to produce the work, or an object code interpreter used
  to run it.

  The “Corresponding Source” for a work in object code form means all the
  source code needed to generate, install, and (for an executable work) run
  the object code and to modify the work, including scripts to control
  those activities. However, it does not include the work's System
  Libraries, or general-purpose tools or generally available free programs
  which are used unmodified in performing those activities but which are
  not part of the work. For example, Corresponding Source includes
  interface definition files associated with source files for the work, and
  the source code for shared libraries and dynamically linked subprograms
  that the work is specifically designed to require, such as by intimate
  data communication or control flow between those subprograms and other
  parts of the work.

  The Corresponding Source need not include anything that users can
  regenerate automatically from other parts of the Corresponding Source.

  The Corresponding Source for a work in source code form is that same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
  copyright on the Program, and are irrevocable provided the stated
  conditions are met. This License explicitly affirms your unlimited
  permission to run the unmodified Program, subject to section 13. The
  output from running a covered work is covered by this License only if the
  output, given its content, constitutes a covered work. This License
  acknowledges your rights of fair use or other equivalent, as provided by
  copyright law.  Subject to section 13, you may make, run and propagate
  covered works that you do not convey, without conditions so long as your
  license otherwise remains in force. You may convey covered works to
  others for the sole purpose of having them make modifications exclusively
  for you, or provide you with facilities for running those works, provided
  that you comply with the terms of this License in conveying all
  material for which you do not control copyright. Those thus making or
  running the covered works for you must do so exclusively on your
  behalf, under your direction and control, on terms that prohibit them
  from making any copies of your copyrighted material outside their
  relationship with you.

  Conveying under any other circumstances is permitted solely under the
  conditions stated below. Sublicensing is not allowed; section 10 makes it
  unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
  measure under any applicable law fulfilling obligations under article 11
  of the WIPO copyright treaty adopted on 20 December 1996, or similar laws
  prohibiting or restricting circumvention of such measures.

  When you convey a covered work, you waive any legal power to forbid
  circumvention of technological measures to the extent such circumvention is
  effected by exercising rights under this License with respect to the
  covered work, and you disclaim any intention to limit operation or
  modification of the work as a means of enforcing, against the work's users,
  your or third parties' legal rights to forbid circumvention of
  technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
  receive it, in any medium, provided that you conspicuously and
  appropriately publish on each copy an appropriate copyright notice; keep
  intact all notices stating that this License and any non-permissive terms
  added in accord with section 7 apply to the code; keep intact all notices
  of the absence of any warranty; and give all recipients a copy of this
  License along with the Program.  You may charge any price or no price for
  each copy that you convey, and you may offer support or warranty
  protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
  produce it from the Program, in the form of source code under the terms
  of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified it,
    and giving a relevant date.

    b) The work must carry prominent notices stating that it is released
    under this License and any conditions added under section 7. This
    requirement modifies the requirement in section 4 to “keep intact all
    notices”.

    c) You must license the entire work, as a whole, under this License to
    anyone who comes into possession of a copy. This License will therefore
    apply, along with any applicable section 7 additional terms, to the
    whole of the work, and all its parts, regardless of how they are
    packaged. This License gives no permission to license the work in any
    other way, but it does not invalidate such permission if you have
    separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your work
    need not make them do so.

  A compilation of a covered work with other separate and independent
  works, which are not by their nature extensions of the covered work, and
  which are not combined with it such as to form a larger program, in or on
  a volume of a storage or distribution medium, is called an “aggregate” if
  the compilation and its resulting copyright are not used to limit the
  access or legal rights of the compilation's users beyond what the
  individual works permit. Inclusion of a covered work in an aggregate does
  not cause this License to apply to the other parts of the aggregate.
  
  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms of
  sections 4 and 5, provided that you also convey the machine-readable
  Corresponding Source under the terms of this License, in one of these
  ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium customarily
    used for software interchange.
   
    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a written
    offer, valid for at least three years and valid for as long as you
    offer spare parts or customer support for that product model, to give
    anyone who possesses the object code either (1) a copy of the
    Corresponding Source for all the software in the product that is
    covered by this License, on a durable physical medium customarily used
    for software interchange, for a price no more than your reasonable cost
    of physically performing this conveying of source, or (2) access to
    copy the Corresponding Source from a network server at no charge.
   
    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source. This alternative is
    allowed only occasionally and noncommercially, and only if you received
    the object code with such an offer, in accord with subsection 6b.
   
    d) Convey the object code by offering access from a designated place
    (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge. You need not require recipients to copy the
    Corresponding Source along with the object code. If the place to copy
    the object code is a network server, the Corresponding Source may be on
    a different server (operated by you or a third party) that supports
    equivalent copying facilities, provided you maintain clear directions
    next to the object code saying where to find the Corresponding Source.
    Regardless of what server hosts the Corresponding Source, you remain
    obligated to ensure that it is available for as long as needed to
    satisfy these requirements.
   
    e) Convey the object code using peer-to-peer transmission, provided you
    inform other peers where the object code and Corresponding Source of
    the work are being offered to the general public at no charge under
    subsection 6d.

  A separable portion of the object code, whose source code is excluded
  from the Corresponding Source as a System Library, need not be included
  in conveying the object code work.

  A “User Product” is either (1) a “consumer product”, which means any
  tangible personal property which is normally used for personal, family,
  or household purposes, or (2) anything designed or sold for incorporation
  into a dwelling. In determining whether a product is a consumer product,
  doubtful cases shall be resolved in favor of coverage. For a particular
  product received by a particular user, “normally used” refers to a
  typical or common use of that class of product, regardless of the status
  of the particular user or of the way in which the particular user
  actually uses, or expects or is expected to use, the product. A product
  is a consumer product regardless of whether the product has substantial
  commercial, industrial or non-consumer uses, unless such uses represent
  the only significant mode of use of the product.

  “Installation Information” for a User Product means any methods,
  procedures, authorization keys, or other information required to install
  and execute modified versions of a covered work in that User Product from
  a modified version of its Corresponding Source. The information must
  suffice to ensure that the continued functioning of the modified object
  code is in no case prevented or interfered with solely because
  modification has been made.

  If you convey an object code work under this section in, or with, or
  specifically for use in, a User Product, and the conveying occurs as part
  of a transaction in which the right of possession and use of the User
  Product is transferred to the recipient in perpetuity or for a fixed term
  (regardless of how the transaction is characterized), the Corresponding
  Source conveyed under this section must be accompanied by the
  Installation Information. But this requirement does not apply if neither
  you nor any third party retains the ability to install modified object
  code on the User Product (for example, the work has been installed in
  ROM).

  The requirement to provide Installation Information does not include a
  requirement to continue to provide support service, warranty, or updates
  for a work that has been modified or installed by the recipient, or for
  the User Product in which it has been modified or installed. Access
  to a network may be denied when the modification itself materially
  and adversely affects the operation of the network or violates the
  rules and protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided, in
  accord with this section must be in a format that is publicly documented
  (and with an implementation available to the public in source code form),
  and must require no special password or key for unpacking, reading or
  copying.

  7. Additional Terms.

  “Additional permissions” are terms that supplement the terms of this
  License by making exceptions from one or more of its conditions.
  Additional permissions that are applicable to the entire Program shall be
  treated as though they were included in this License, to the extent that
  they are valid under applicable law. If additional permissions apply only
  to part of the Program, that part may be used separately under those
  permissions, but the entire Program remains governed by this License
  without regard to the additional permissions.  When you convey a copy of
  a covered work, you may at your option remove any additional permissions
  from that copy, or from any part of it. (Additional permissions may be
  written to require their own removal in certain cases when you modify the
  work.) You may place additional permissions on material, added by you to
  a covered work, for which you have or can give appropriate copyright
  permission.

  Notwithstanding any other provision of this License, for material you add
  to a covered work, you may (if authorized by the copyright holders of
  that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some trade
    names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that material
    by anyone who conveys the material (or modified versions of it) with
    contractual assumptions of liability to the recipient, for any
    liability that these contractual assumptions directly impose on those
    licensors and authors.

  All other non-permissive additional terms are considered “further
  restrictions” within the meaning of section 10. If the Program as you
  received it, or any part of it, contains a notice stating that it is
  governed by this License along with a term that is a further restriction,
  you may remove that term. If a license document contains a further
  restriction but permits relicensing or conveying under this License, you
  may add to a covered work material governed by the terms of that license
  document, provided that the further restriction does not survive such
  relicensing or conveying.

  If you add terms to a covered work in accord with this section, you must
  place, in the relevant source files, a statement of the additional terms
  that apply to those files, or a notice indicating where to find the
  applicable terms.  Additional terms, permissive or non-permissive, may be
  stated in the form of a separately written license, or stated as
  exceptions; the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
  provided under this License. Any attempt otherwise to propagate or modify
  it is void, and will automatically terminate your rights under this
  License (including any patent licenses granted under the third paragraph
  of section 11).

  However, if you cease all violation of this License, then your license
  from a particular copyright holder is reinstated (a) provisionally,
  unless and until the copyright holder explicitly and finally terminates
  your license, and (b) permanently, if the copyright holder fails to
  notify you of the violation by some reasonable means prior to 60 days
  after the cessation.

  Moreover, your license from a particular copyright holder is reinstated
  permanently if the copyright holder notifies you of the violation by some
  reasonable means, this is the first time you have received notice of
  violation of this License (for any work) from that copyright holder, and
  you cure the violation prior to 30 days after your receipt of the notice.

  Termination of your rights under this section does not terminate the
  licenses of parties who have received copies or rights from you under
  this License. If your rights have been terminated and not permanently
  reinstated, you do not qualify to receive new licenses for the same
  material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or run a
  copy of the Program. Ancillary propagation of a covered work occurring
  solely as a consequence of using peer-to-peer transmission to receive a
  copy likewise does not require acceptance. However, nothing other than
  this License grants you permission to propagate or modify any covered
  work. These actions infringe copyright if you do not accept this License.
  Therefore, by modifying or propagating a covered work, you indicate your
  acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically receives
  a license from the original licensors, to run, modify and propagate that
  work, subject to this License. You are not responsible for enforcing
  compliance by third parties with this License.

  An “entity transaction” is a transaction transferring control of an
  organization, or substantially all assets of one, or subdividing an
  organization, or merging organizations. If propagation of a covered work
  results from an entity transaction, each party to that transaction who
  receives a copy of the work also receives whatever licenses to the work
  the party's predecessor in interest had or could give under the previous
  paragraph, plus a right to possession of the Corresponding Source of the
  work from the predecessor in interest, if the predecessor has it or can
  get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the rights
  granted or affirmed under this License. For example, you may not impose a
  license fee, royalty, or other charge for exercise of rights granted
  under this License, and you may not initiate litigation (including a
  cross-claim or counterclaim in a lawsuit) alleging that any patent claim
  is infringed by making, using, selling, offering for sale, or importing
  the Program or any portion of it.

  11. Patents.

  A “contributor” is a copyright holder who authorizes use under this
  License of the Program or a work on which the Program is based. The work
  thus licensed is called the contributor's “contributor version”.

  A contributor's “essential patent claims” are all patent claims owned or
  controlled by the contributor, whether already acquired or hereafter
  acquired, that would be infringed by some manner, permitted by this
  License, of making, using, or selling its contributor version, but do not
  include claims that would be infringed only as a consequence of further
  modification of the contributor version. For purposes of this definition,
  “control” includes the right to grant patent sublicenses in a manner
  consistent with the requirements of this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
  patent license under the contributor's essential patent claims, to make,
  use, sell, offer for sale, import and otherwise run, modify and propagate
  the contents of its contributor version.

  In the following three paragraphs, a “patent license” is any express
  agreement or commitment, however denominated, not to enforce a patent
  (such as an express permission to practice a patent or covenant not to
  sue for patent infringement). To “grant” such a patent license to a party
  means to make such an agreement or commitment not to enforce a patent
  against the party.

  If you convey a covered work, knowingly relying on a patent license, and
  the Corresponding Source of the work is not available for anyone to copy,
  free of charge and under the terms of this License, through a publicly
  available network server or other readily accessible means, then you must
  either (1) cause the Corresponding Source to be so available, or (2)
  arrange to deprive yourself of the benefit of the patent license for this
  particular work, or (3) arrange, in a manner consistent with the
  requirements of this License, to extend the patent license to downstream
  recipients. “Knowingly relying” means you have actual knowledge that, but
  for the patent license, your conveying the covered work in a country, or
  your recipient's use of the covered work in a country, would infringe
  one or more identifiable patents in that country that you have reason
  to believe are valid.

  If, pursuant to or in connection with a single transaction or
  arrangement, you convey, or propagate by procuring conveyance of, a
  covered work, and grant a patent license to some of the parties receiving
  the covered work authorizing them to use, propagate, modify or convey a
  specific copy of the covered work, then the patent license you grant is
  automatically extended to all recipients of the covered work and works
  based on it.

  A patent license is “discriminatory” if it does not include within the
  scope of its coverage, prohibits the exercise of, or is conditioned on
  the non-exercise of one or more of the rights that are specifically
  granted under this License. You may not convey a covered work if you are
  a party to an arrangement with a third party that is in the business of
  distributing software, under which you make payment to the third party
  based on the extent of your activity of conveying the work, and under
  which the third party grants, to any of the parties who would receive the
  covered work from you, a discriminatory patent license (a) in connection
  with copies of the covered work conveyed by you (or copies made from
  those copies), or (b) primarily for and in connection with specific
  products or compilations that contain the covered work, unless you
  entered into that arrangement, or that patent license was granted, prior
  to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting any
  implied license or other defenses to infringement that may otherwise be
  available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
  otherwise) that contradict the conditions of this License, they do not
  excuse you from the conditions of this License. If you cannot use,
  propagate or convey a covered work so as to satisfy simultaneously your
  obligations under this License and any other pertinent obligations, then
  as a consequence you may not use, propagate or convey it at all. For
  example, if you agree to terms that obligate you to collect a royalty for
  further conveying from those to whom you convey the Program, the only way
  you could satisfy both those terms and this License would be to refrain
  entirely from conveying the Program.

  13. Offering the Program as a Service.

  If you make the functionality of the Program or a modified version
  available to third parties as a service, you must make the Service Source
  Code available via network download to everyone at no charge, under the
  terms of this License. Making the functionality of the Program or
  modified version available to third parties as a service includes,
  without limitation, enabling third parties to interact with the
  functionality of the Program or modified version remotely through a
  computer network, offering a service the value of which entirely or
  primarily derives from the value of the Program or modified version, or
  offering a service that accomplishes for users the primary purpose of the
  Program or modified version.

  “Service Source Code” means the Corresponding Source for the Program or
  the modified version, and the Corresponding Source for all programs that
  you use to make the Program or modified version available as a service,
  including, without limitation, management software, user interfaces,
  application program interfaces, automation software, monitoring software,
  backup software, storage software and hosting software, all such that a
  user could run an instance of the service using the Service Source Code
  you make available.  

  14. Revised Versions of this License.

  MongoDB, Inc. may publish revised and/or new versions of the Server Side
  Public License from time to time. Such new versions will be similar in
  spirit to the present version, but may differ in detail to address new
  problems or concerns.

  Each version is given a distinguishing version number. If the Program
  specifies that a certain numbered version of the Server Side Public
  License “or any later version” applies to it, you have the option of
  following the terms and conditions either of that numbered version or of
  any later version published by MongoDB, Inc. If the Program does not
  specify a version number of the Server Side Public License, you may
  choose any version ever published by MongoDB, Inc.

  If the Program specifies that a proxy can decide which future versions of
  the Server Side Public License can be used, that proxy's public statement
  of acceptance of a version permanently authorizes you to choose that
  version for the Program.

  Later license versions may give you additional or different permissions.
  However, no additional obligations are imposed on any author or copyright
  holder as a result of your choosing to follow a later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
  APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
  HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY
  OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
  IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
  ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
  
  16. Limitation of Liability.
  
  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
  WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
  THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING
  ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF
  THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO
  LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU
  OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
  PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
  POSSIBILITY OF SUCH DAMAGES.
  
  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided above
  cannot be given local legal effect according to their terms, reviewing
  courts shall apply local law that most closely approximates an absolute
  waiver of all civil liability in connection with the Program, unless a
  warranty or assumption of liability accompanies a copy of the Program in
  return for a fee.
  
                        END OF TERMS AND CONDITIONS
</file>

<file path="apiserver/mongo/initialize/__init__.py">
log = config.logger(__package__)
⋮----
def _pre_populate(company_id: str, zip_file: str)
⋮----
msg = f"Invalid pre-populate zip file: {zip_file}"
⋮----
def _resolve_zip_files(zip_files: Union[Sequence[str], str]) -> Sequence[str]
⋮----
zip_files = [zip_files]
⋮----
def pre_populate_data()
⋮----
def init_mongo_data()
⋮----
company_id = _ensure_company(get_default_company(), "clearml", log)
⋮----
fixed_mode = FixedUser.enabled()
⋮----
internal_user_emails = set()
⋮----
email = f"{user}@example.com"
user_data = {
⋮----
revoke = fixed_mode and credentials.get("revoke_in_fixed_mode", False)
user_id = _ensure_auth_user(
⋮----
backend_user = User.objects(id=user_id).first()
⋮----
fixed_users = None
⋮----
fixed_users = FixedUser.from_config()
</file>

<file path="apiserver/mongo/initialize/migration.py">
_migrations = "migrations"
_parent_dir = Path(__file__).resolve().parents[1]
_migration_dir = _parent_dir / _migrations
log = config.logger(__file__)
⋮----
def check_mongo_empty() -> bool
⋮----
collection_names = get_db(alias).list_collection_names()
⋮----
def get_last_server_version() -> Version
⋮----
previous_versions = sorted(
⋮----
def _ensure_mongodb_version()
⋮----
db: pymongo.database.Database = get_db(Database.backend)
db_version = db.client.server_info()["version"]
⋮----
res = db.client.admin.command({"getParameter": 1, "featureCompatibilityVersion": 1})
version = nested_get(res, ("featureCompatibilityVersion", "version"))
⋮----
res = db.client.admin.command({"setFeatureCompatibilityVersion": "7.0", "confirm": True})
⋮----
def _apply_migrations(log: Logger)
⋮----
"""
    Apply migrations as found in the migration dir.
    Returns a boolean indicating whether the database was empty prior to migration.
    """
log = log.getChild(Path(__file__).stem)
⋮----
empty_dbs = check_mongo_empty()
last_version = get_last_server_version()
⋮----
new_scripts = {
⋮----
dbs = {Database.auth: "migrate_auth", Database.backend: "migrate_backend"}
⋮----
script = new_scripts[script_version]
⋮----
spec = importlib.util.spec_from_file_location(
module = importlib.util.module_from_spec(spec)
⋮----
func = getattr(module, func_name, None)
⋮----
sig = signature(func)
kwargs = {}
⋮----
key = name.replace("_", "-")
</file>

<file path="apiserver/mongo/initialize/pre_populate.py">
replace_s3_scheme = os.getenv("CLEARML_REPLACE_S3_SCHEME")
⋮----
def _print(msg: str)
⋮----
time = datetime.now().isoformat(sep=" ", timespec="seconds")
⋮----
UrlTranslation = Tuple[str, str]
⋮----
class PrePopulate
⋮----
module_name_prefix = "apiserver."
event_bll = EventBLL()
events_file_suffix = "_events"
export_tag_prefix = "Exported:"
export_tag = f"{export_tag_prefix} %Y-%m-%d %H:%M:%S"
metadata_filename = "metadata.json"
users_filename = "users.json"
zip_args = dict(mode="w", compression=ZIP_BZIP2)
artifacts_ext = ".artifacts"
img_source_regex = re.compile(
_name_guid_ns = UUID("bda3acc1-e612-506c-bade-80071b6cf039")
_example_id_prefix = "e-"
task_cls: Type[Task]
project_cls: Type[Project]
model_cls: Type[Model]
user_cls: Type[User]
auth_user_cls: Type[AuthUser]
⋮----
@attr.s(auto_attribs=True)
    class ParentPrefix
⋮----
prefix: str
path: Sequence[str]
⋮----
# noinspection PyTypeChecker
⋮----
@classmethod
    def _init_entity_types(cls)
⋮----
class JsonLinesWriter
⋮----
def __init__(self, file: BinaryIO)
⋮----
def __enter__(self)
⋮----
def __exit__(self, exc_type, exc_value, exc_traceback)
⋮----
def _write(self, data: str)
⋮----
def write(self, line: str)
⋮----
@staticmethod
    def _get_last_update_time(entity) -> datetime
⋮----
files = []
⋮----
map_data = json.loads(map_file.read_text())
files = map_data.get("files", [])
⋮----
new_times = {
old_times = map_data.get("entities", {})
⋮----
@staticmethod
    def _filter_artifacts(artifacts: Sequence[str]) -> Sequence[str]
⋮----
def is_fileserver_link(a: str) -> bool
⋮----
a = a.lower()
⋮----
parsed = urlparse(a)
⋮----
fileserver_links = [a for a in artifacts if is_fileserver_link(a)]
⋮----
def export_to_zip_core(file_base_name: Path, projects_: Sequence[str])
⋮----
entities = cls._resolve_entities(
⋮----
hash_ = hashlib.md5()
⋮----
meta_str = json.dumps(metadata)
⋮----
metadata_hash = hash_.hexdigest()
⋮----
map_file = file_base_name.with_suffix(".map")
⋮----
old_path = Path(old)
⋮----
temp_file = file_base_name.with_suffix(file_base_name.suffix + "$")
⋮----
artifacts = cls._export(
⋮----
file_with_hash = file_base_name.with_stem(
⋮----
files = [str(file_with_hash)]
⋮----
artifacts = cls._filter_artifacts(artifacts)
⋮----
artifacts_file = file_with_hash.with_suffix(cls.artifacts_ext)
⋮----
filename = Path(filename)
⋮----
query = dict(parent=None)
⋮----
projects = list(cls.project_cls.objects(**query).scalar("id"))
# projects.append(None)
⋮----
created_files = list(
⋮----
created_files = export_to_zip_core(
⋮----
metadata = None
⋮----
metadata = json.loads(f.read())
⋮----
meta_public = metadata.get("public")
⋮----
company_id = "" if meta_public else get_default_company()
⋮----
meta_user_id = metadata.get("user_id", "")
meta_user_name = metadata.get("user_name", "")
⋮----
# Make sure we won't end up with an invalid company ID
⋮----
company_id = ""
⋮----
user_mapping = cls._import_users(zfile, company_id)
⋮----
existing_user = cls.user_cls.objects(id=user_id).only("id").first()
⋮----
artifacts_file = Path(filename).with_suffix(cls.artifacts_ext)
⋮----
@classmethod
    def upgrade_zip(cls, filename) -> Sequence
⋮----
task_file = cls._get_base_filename(cls.task_cls) + ".json"
temp_file = Path("temp.zip")
file = Path(filename)
⋮----
content = cls._upgrade_tasks(f)
⋮----
content = reader.read(file_info)
⋮----
new_hash = hash_.hexdigest()
⋮----
new_file = file.with_name(f"{base_file_name}_{new_hash}{file.suffix}")
⋮----
upadated = [str(new_file)]
⋮----
artifacts_file = file.with_suffix(cls.artifacts_ext)
⋮----
new_artifacts = new_file.with_suffix(cls.artifacts_ext)
⋮----
@classmethod
    def _upgrade_tasks(cls, f: IO[bytes]) -> bytes
⋮----
"""
        Build content array that contains upgraded tasks from the passed file
        For each task the old execution.parameters and model.design are
        converted to the new structure.
        The fix is done on Task objects (not the dictionary) so that
        the fields are serialized back in the same order as they were in the original file
        """
⋮----
task_data = cls.task_cls.from_json(line).to_proper_dict()
⋮----
new_task = cls.task_cls(**task_data)
⋮----
@classmethod
    def update_featured_projects_order(cls)
⋮----
order = config.get("services.projects.featured.order", [])
⋮----
public_default = config.get("services.projects.featured.public_default", 9999)
⋮----
def get_index(p: Project)
⋮----
featured_index = get_index(project)
⋮----
ids = set(ids)
items = list(entity_type.objects(id__in=list(ids)))
resolved = {i.id for i in items}
missing = ids - resolved
⋮----
resolved_by_name = defaultdict(list)
⋮----
not_found = missing - set(resolved_by_name)
⋮----
duplicates = [k for k, v in resolved_by_name.items() if len(v) > 1]
⋮----
def get_new_items(input_: Iterable) -> list
⋮----
def get_projects_with_children(projects: list) -> list
⋮----
project_ids = set(item.id for item in projects)
ids_with_children = project_ids_with_children(list(project_ids))
⋮----
new_items = get_new_items(chain(*resolved_by_name.values()))
⋮----
new_items = get_projects_with_children(new_items)
⋮----
@classmethod
    def _check_projects_hierarchy(cls, projects: Set[Project])
⋮----
"""
        For the projects that are exported not from the root
        fix their parents tree to exclude the not exported parents
        """
⋮----
project_ids = {p.id for p in projects}
orphans = [p for p in projects if p.parent and p.parent not in project_ids]
⋮----
prefixes = [
⋮----
prefix = first(
⋮----
# _print(
#     f"ERROR: the following projects are exported without their parents: {orphans}"
# )
# exit(1)
⋮----
entities: Dict[Any] = defaultdict(set)
⋮----
root = None in projects
projects = [p for p in projects if p]
⋮----
projects = project_ids_with_children(projects)
⋮----
p_ids = list(set(p.id for p in entities[cls.project_cls]))
⋮----
query = Q(
⋮----
objs = cls.task_cls.objects(query)
⋮----
objs = cls.project_cls.objects(
project_ids = {p.id for p in entities[cls.project_cls]}
⋮----
task_models = chain.from_iterable(
model_ids = {tm.model for tm in task_models}
⋮----
@classmethod
    def _filter_out_export_tags(cls, tags: Sequence[str]) -> Sequence[str]
⋮----
@classmethod
    def _cleanup_model(cls, model: Model)
⋮----
@classmethod
    def _cleanup_task(cls, task: Task)
⋮----
@classmethod
    def _cleanup_project(cls, project: Project)
⋮----
@classmethod
    def _cleanup_auth_user(cls, user: AuthUser)
⋮----
@classmethod
    def _cleanup_be_user(cls, user: User)
⋮----
@classmethod
    def _cleanup_entity(cls, entity_cls, entity, cleanup_users)
⋮----
@classmethod
    def _add_tag(cls, items: Sequence[Union[Project, Task, Model]], tag: str)
⋮----
@staticmethod
    def _translate_url(url_: str, url_trans: UrlTranslation) -> str
⋮----
artifacts = []
filename = f"{base_filename}_{task.id}{cls.events_file_suffix}.json"
⋮----
scroll_id = None
events_count = 0
⋮----
res = cls.event_bll.get_task_events(
⋮----
scroll_id = res.next_scroll_id
⋮----
event_type = event.get("type")
⋮----
url = cls._get_fixed_url(event.get("url"))
⋮----
plot_str: str = event.get("plot_str", "")
⋮----
url = cls._get_fixed_url(match)
⋮----
new_url = cls._translate_url(url, url_trans)
⋮----
plot_str = plot_str.replace(match, new_url)
⋮----
data = f.getvalue()
⋮----
@staticmethod
    def _get_fixed_url(url: Optional[str]) -> Optional[str]
⋮----
fixed = furl(url)
⋮----
url = cls._get_fixed_url(entity.uri)
⋮----
@classmethod
    def _get_task_output_artifacts(cls, task: Task, url_trans: UrlTranslation) -> Sequence[str]
⋮----
artifact_urls = []
⋮----
url = cls._get_fixed_url(a.uri)
⋮----
unique_paths = set(unquote(str(furl(artifact).path)) for artifact in artifacts)
⋮----
path = path.lstrip("/")
full_path = os.path.join(artifacts_path, path)
⋮----
@classmethod
    def _export_users(cls, writer: ZipFile)
⋮----
auth_users = {
⋮----
be_users = {
⋮----
auth_users = {uid: data for uid, data in auth_users.items() if uid in be_users}
⋮----
data = {}
⋮----
def get_field_bytes(k: str, v: bytes) -> bytes
⋮----
data_str = b",\n".join(get_field_bytes(k, v) for k, v in data.items())
⋮----
@classmethod
    def _get_base_filename(cls, cls_: type)
⋮----
name = f"{cls_.__module__}.{cls_.__name__}"
⋮----
name = name[len(cls.module_name_prefix) :]
⋮----
"""
        Export the requested experiments, projects and models and return the list of artifact files
        Always do the export on sorted items since the order of items influence hash
        The projects should be sorted by name so that on import the hierarchy is correctly restored from top to bottom
        """
⋮----
now = datetime.now(timezone.utc)
⋮----
items = sorted(entities[cls_], key=attrgetter("name", "id"))
⋮----
base_filename = cls._get_base_filename(cls_)
⋮----
filename = base_filename + ".json"
⋮----
@staticmethod
    def json_lines(file: IO[bytes])
⋮----
clean = (
⋮----
@staticmethod
    def _new_id(_)
⋮----
@classmethod
    def _hash_id(cls, name: str)
⋮----
@classmethod
    def _example_id(cls, orig_id: str)
⋮----
@classmethod
    def _private_id(cls, orig_id: str)
⋮----
ids = {}
⋮----
is_project = splitext(entity_file.orig_filename)[0].endswith(".Project")
⋮----
id_func = cls._new_id
⋮----
id_func = cls._example_id if not is_project else cls._hash_id
⋮----
id_func = cls._private_id if not is_project else cls._new_id
⋮----
doc = json.loads(item)
orig_id = doc.get("_id")
⋮----
@classmethod
    def _import_users(cls, reader: ZipFile, company_id: str = "") -> dict
⋮----
"""
        Import users to db and return the mapping of old user ids to the new ones
        If no users were in the users file then the mapping was empty
        If the user in the file has the same email as one of the existing ones then this user is skipped
        and its id is mapped to the existing user with the same email
        If the user with the same id exists in backend or auth db then its creation is skipped
        """
users_file = first(
⋮----
existing_user_ids = set(cls.user_cls.objects().scalar("id")) | set(
existing_user_emails = {u.email: u.id for u in cls.auth_user_cls.objects()}
user_id_mappings = {}
⋮----
data = json.loads(f.read())
⋮----
auth_users = {u["_id"]: u for u in data["auth"]}
be_users = {u["_id"]: u for u in data["backend"]}
⋮----
email = user.get("email")
existing_user_id = existing_user_emails.get(email)
⋮----
credentials = user.get("credentials", [])
⋮----
user_role = user.get("role", Role.user)
⋮----
auth_user = cls.auth_user_cls.from_json(json.dumps(user), created=True)
⋮----
be_user = cls.user_cls.from_json(json.dumps(be_users[uid]), created=True)
⋮----
"""
        Import entities and events from the zip file
        Start from entities since event import will require the tasks already in DB
        """
event_file_ending = cls.events_file_suffix + ".json"
entity_files = [
metadata = metadata or {}
old_to_new_ids = cls._generate_new_ids(reader, entity_files, metadata)
tasks = []
⋮----
full_name = splitext(entity_file.orig_filename)[0]
⋮----
res = cls._import_entity(
⋮----
tasks = res
⋮----
tasks = sorted(tasks, key=attrgetter("last_update"))
⋮----
new_to_old_ids = {v: k for k, v in old_to_new_ids.items()}
⋮----
old_task_id = new_to_old_ids.get(task.id, task.id)
events_file = first(
⋮----
full_name = splitext(events_file.orig_filename)[0]
⋮----
@classmethod
    def _get_entity_type(cls, full_name) -> Type[mongoengine.Document]
⋮----
module_name = cls.module_name_prefix + module_name
module = importlib.import_module(module_name)
⋮----
@classmethod
    def _upgrade_project_data(cls, project_data: dict) -> dict
⋮----
name: str = project_data["name"]
⋮----
@classmethod
    def _upgrade_model_data(cls, model_data: dict) -> dict
⋮----
metadata_key = "metadata"
metadata = model_data.get(metadata_key)
⋮----
metadata = {
⋮----
@staticmethod
    def _remove_incompatible_fields(cls_: Type[Document], data: dict)
⋮----
@classmethod
    def _upgrade_task_data(cls, task_data: dict) -> dict
⋮----
"""
        Migrate from execution/parameters and model_desc to hyperparams and configuration fiields
        Upgrade artifacts list to dict
        Migrate from execution.model and output.model to the new models field
        Move docker_cmd contents into the container field
        :param task_data: Upgraded in place
        :return: The upgraded task data
        """
⋮----
legacy_path = old_param_field.split(".")
legacy = nested_get(task_data, legacy_path)
⋮----
new_path = list(filter(None, (new_param_field, section, name)))
⋮----
new_param = dict(
⋮----
artifacts_path = ("execution", "artifacts")
artifacts = nested_get(task_data, artifacts_path)
⋮----
models = task_data.get("models", {})
⋮----
old_path = old_field.split(".")
old_model = nested_get(task_data, old_path)
new_models = [
name = TaskModelNames[type_]
⋮----
model_item = {"model": old_model, "name": name, "updated": now}
⋮----
new_models = [model_item, *new_models]
⋮----
new_models = [*new_models, model_item]
⋮----
docker_cmd_path = ("execution", "docker_cmd")
docker_cmd = nested_get(task_data, docker_cmd_path)
⋮----
user_mapping = user_mapping or {}
cls_ = cls._get_entity_type(full_name)
⋮----
override_project_count = 0
data_upgrade_funcs: Mapping[Type, Callable] = {
⋮----
# replace ids only when they are standalone strings
# otherwise artifacts uris that contain old ids may get damaged
item = item.replace(f'"{old_id}"', f'"{new_id}"')
upgrade_func = data_upgrade_funcs.get(cls_)
⋮----
item = json.dumps(upgrade_func(json.loads(item)))
⋮----
doc = cls_.from_json(item, created=True)
⋮----
override_project_name = metadata.get("project_name", None)
⋮----
override_project_name = (
⋮----
@classmethod
    def _import_events(cls, f: IO[bytes], company_id: str, user_id: str, task_id: str)
⋮----
events = [json.loads(item) for item in events_chunk]
</file>

<file path="apiserver/mongo/initialize/user.py">
new_credentials = Credentials(key=key, secret=secret)
⋮----
user_id = user_data.get("id", f"__{user_data['name']}__")
role = user_data["role"]
email = user_data["email"]
autocreated = user_data.get("autocreated", False)
⋮----
user: AuthUser = AuthUser.objects(id=user_id).first()
⋮----
credentials = (
⋮----
user = AuthUser(
⋮----
def ensure_fixed_user(user: FixedUser, log: Logger)
⋮----
# noinspection PyTypeChecker
data = attr.asdict(user)
⋮----
db_user = User.objects(company=user.company, id=user.id).first()
⋮----
# noinspection PyBroadException
</file>

<file path="apiserver/mongo/initialize/util.py">
log = config.logger(__file__)
⋮----
def _ensure_company(company_id, company_name, log: Logger)
⋮----
company = Company.objects(id=company_id).only("id").first()
⋮----
company = Company(id=company_id, name=company_name)
⋮----
def _ensure_default_queue(company)
⋮----
"""
    If no queue is present for the company then
    create a new one and mark it as a default
    """
queue = Queue.objects(company=company).only("id").first()
⋮----
def _ensure_uuid()
</file>

<file path="apiserver/mongo/migrations/0_12_1.py">
def migrate_backend(db: Database)
⋮----
collection: Collection = db[name]
⋮----
tags = doc.get("tags")
</file>

<file path="apiserver/mongo/migrations/0_13_0.py">
def migrate_auth(db: Database)
⋮----
collection: Collection = db["user"]
⋮----
def migrate_backend(db: Database)
⋮----
users = collection.find(
</file>

<file path="apiserver/mongo/migrations/0_14_0.py">
def _get_ids()
⋮----
def _switch_uuid(collection: Collection, uuid_field: str, uuids: dict)
⋮----
docs = list(collection.find({uuid_field: {"$in": [uuids]}}))
⋮----
replaced_uuids = [doc[uuid_field] for doc in docs]
⋮----
def migrate_auth(db: Database)
⋮----
uuids = _get_ids()
⋮----
collection: Collection = db["user"]
⋮----
def migrate_backend(db: Database)
</file>

<file path="apiserver/mongo/migrations/0_15_0.py">
def migrate_auth(db: Database)
⋮----
"""
    Remove the old indices from the collections since
    they may come out of sync with the latest changes
    in the code and mongo libraries update
    """
⋮----
def migrate_backend(db: Database)
⋮----
"""
    1. Sort tags and system tags
    2. Remove the old indices from the collections since
        they may come out of sync with the latest changes
        in the code and mongo libraries update
    """
⋮----
fields = ("tags", "system_tags")
query = {"$or": [{field: {"$exists": True, "$ne": []}} for field in fields]}
⋮----
collection = db[collection_name]
⋮----
update = {
</file>

<file path="apiserver/mongo/migrations/0_16_0.py">
def migrate_backend(db: Database)
⋮----
hyperparam_fields = ("execution.parameters", "hyperparams")
configuration_fields = ("execution.model_desc", "configuration")
collection: Collection = db["task"]
⋮----
set_commands = {}
⋮----
legacy = safe_get(doc, old_field, separator=".")
⋮----
new_path = list(filter(None, (new_field, section, name)))
# if safe_get(doc, new_path) is not None:
#    continue
new_value = dict(
</file>

<file path="apiserver/mongo/migrations/0_16_1.py">
def migrate_backend(db: Database)
⋮----
collection: Collection = db["project"]
featured = "featured"
query = {featured: {"$exists": False}}
</file>

<file path="apiserver/mongo/migrations/0_16_2.py">
def migrate_backend(db: Database)
⋮----
collection: Collection = db["task"]
artifacts_field = "execution.artifacts"
query = {artifacts_field: {"$type": 4}}
⋮----
artifacts = nested_get(doc, artifacts_field.split("."))
⋮----
new_artifacts = {get_artifact_id(a): a for a in artifacts}
</file>

<file path="apiserver/mongo/migrations/0_17_0.py">
def _add_active_duration(db: Database)
⋮----
active_duration_key = "active_duration"
query = {"$or": [{active_duration_key: {"$eq": None}}, {active_duration_key: {"$eq": 0}}]}
collection = db["task"]
⋮----
started = doc.get("started")
completed = doc.get("completed")
running = doc.get("status") == "running"
active_duration_value = doc.get(active_duration_key)
⋮----
def migrate_backend(db: Database)
⋮----
collection: Collection = db["task"]
artifacts_field = "execution.artifacts"
query = {artifacts_field: {"$type": 4}}
⋮----
artifacts = nested_get(doc, artifacts_field.split("."))
⋮----
new_artifacts = {get_artifact_id(a): a for a in artifacts}
⋮----
def migrate_auth(db: Database)
⋮----
"""
    Remove the old indices from the user collections
    to enable building of the updated user email index
    """
collection: Collection = db["user"]
</file>

<file path="apiserver/mongo/migrations/1_0_0.py">
def _migrate_task_models(db: Database)
⋮----
"""
    Move the execution and output models to new models.input and output lists
    """
tasks: Collection = db["task"]
⋮----
models_field = "models"
now = datetime.utcnow()
⋮----
fields = {
query = {"$or": [{field: {"$exists": True}} for field in fields.values()]}
⋮----
set_commands = {}
⋮----
value = nested_get(doc, field.split("."))
⋮----
name = TaskModelNames[mode]
model_item = {"model": value, "name": name, "updated": now}
existing_models = nested_get(doc, (models_field, mode), default=[])
existing_models = (
⋮----
updated_models = [model_item, *existing_models]
⋮----
updated_models = [*existing_models, model_item]
⋮----
def _migrate_docker_cmd(db: Database)
⋮----
docker_cmd_field = "execution.docker_cmd"
query = {docker_cmd_field: {"$exists": True}}
⋮----
docker_cmd = nested_get(doc, docker_cmd_field.split("."))
⋮----
def _migrate_model_labels(db: Database)
⋮----
fields = ("execution.model_labels", "container")
query = {"$or": [{field: {"$nin": [None, {}]}} for field in fields]}
⋮----
data = nested_get(doc, field.split("."))
⋮----
escaped = escape_dict(data)
⋮----
def _migrate_project_names(db: Database)
⋮----
projects: Collection = db["project"]
⋮----
regx = re.compile("/", re.IGNORECASE)
⋮----
name = doc.get("name")
⋮----
max_tries = int(os.getenv("CLEARML_MIGRATION_PROJECT_RENAME_MAX_TRIES", 10))
iteration = 0
⋮----
new_name = name.replace("/", "_" * (iteration + 1))
⋮----
def migrate_backend(db: Database)
</file>

<file path="apiserver/mongo/migrations/1_0_2.py">
def _migrate_project_description(db: Database)
⋮----
projects: Collection = db["project"]
filter = {
⋮----
def migrate_backend(db: Database)
</file>

<file path="apiserver/mongo/migrations/1_3_0.py">
def _convert_metadata(db: Database, name)
⋮----
collection: Collection = db[name]
⋮----
metadata_field = "metadata"
query = {metadata_field: {"$exists": True, "$type": 4}}
⋮----
metadata = {
⋮----
def migrate_backend(db: Database)
⋮----
collections = ["model", "queue"]
</file>

<file path="apiserver/mongo/migrations/1_6_0.py">
def migrate_backend(db: Database)
⋮----
projects: Collection = db["project"]
⋮----
name: str = doc["name"]
</file>

<file path="apiserver/mongo/migrations/1_7_0.py">
def migrate_backend(db: Database, auth_db: Database)
⋮----
users: Collection = db["user"]
auth_users: Collection = auth_db["user"]
created_field = "created"
⋮----
auth_user = auth_users.find_one({"_id": doc["_id"]}, projection=[created_field])
</file>

<file path="apiserver/mongo/migrations/1_9_0.py">
def migrate_backend(db: Database)
⋮----
"""
    Drop task text index so that the new one including reports field is created
    """
tasks: Collection = db["task"]
</file>

<file path="apiserver/mongo/migrations/utils.py">
def _drop_all_indices_from_collections(db: Database, names: Sequence[str])
⋮----
"""
    Drop all indices for the existing collections from the specified list
    """
⋮----
prefixes = {p.rstrip("*") for p in prefixes}
⋮----
collection: Collection = db[collection_name]
</file>

<file path="apiserver/redis_manager.py">
log = config.logger(__file__)
⋮----
OVERRIDE_HOST_ENV_KEY = (
OVERRIDE_PORT_ENV_KEY = (
OVERRIDE_PASSWORD_ENV_KEY = (
⋮----
OVERRIDE_HOST = first(filter(None, map(getenv, OVERRIDE_HOST_ENV_KEY)))
⋮----
OVERRIDE_PORT = first(filter(None, map(getenv, OVERRIDE_PORT_ENV_KEY)))
⋮----
OVERRIDE_PASSWORD = first(filter(None, map(getenv, OVERRIDE_PASSWORD_ENV_KEY)))
⋮----
class RedisManager(object)
⋮----
def __init__(self, redis_config_dict)
⋮----
alias_config = alias_config.as_plain_ordered_dict()
⋮----
is_cluster = alias_config.get("cluster", False)
⋮----
host = OVERRIDE_HOST or alias_config.get("host", None)
⋮----
port = OVERRIDE_PORT or alias_config.get("port", None)
⋮----
password = OVERRIDE_PASSWORD or alias_config.get("password", None)
⋮----
def connection(self, alias) -> StrictRedis
⋮----
obj = self.aliases.get(alias)
⋮----
def host(self, alias)
⋮----
r = self.connection(alias)
⋮----
connections = r.get_default_node().redis_connection.connection_pool._available_connections
⋮----
connections = r.connection_pool._available_connections
⋮----
redman = RedisManager(config.get("hosts.redis"))
</file>

<file path="apiserver/requirements.txt">
attrs>=22.1.0,<23
azure-storage-blob>=12.13.1
bcrypt>=3.1.4
boltons>=19.1.0
boto3>=1.26
boto3-stubs[s3]>=1.26
clearml>=1.10.3
clearml-agent>=1.5.2
dpath>=1.4.2,<2.0
elasticsearch==8.17.0
fastjsonschema>=2.8
flask-compress>=1.4.0
flask-cors>=3.0.5
flask>=2.3.3
furl>=2.0.0
google-cloud-storage>=2.8.0
gunicorn>=23.0.0
humanfriendly>=4.17
jinja2
jsonmodels>=2.3
jsonschema>=2.6.0
luqum>=0.10.0
mongoengine==0.29.1
nested_dict>=1.61
packaging==20.3
pillow>=10.3.0  # fix vulnerability derived from clearml 1.18.0
psutil>=5.6.5
pyhocon>=0.3.35r
pyjwt>=2.4.0
pymongo==4.12.1
python-rapidjson>=0.6.3
redis==5.2.1
requests>=2.13.0
semantic_version>=2.8.3,<3
setuptools>=78.1.1
six
validators>=0.12.4
urllib3>=1.26.18
werkzeug>=3.0.1
</file>

<file path="apiserver/schema/__init__.py">
__all__ = [EndpointSchema, EndpointVersionsGroup, SchemaReader, Schema]
</file>

<file path="apiserver/schema/meta/__init__.py">

</file>

<file path="apiserver/schema/meta/meta.conf">
// some definitions
definitions {
    // description of a reference
    reference {
        type: object
        additionalProperties: false
        required: [ "$ref" ]
        properties {
            "$ref" { type: string }
        }
    }
    // description of an "additionalProperties" section
    additional_properties {
        oneOf: [
            { type: object, additionalProperties: true },
            { type: boolean }
        ]
    }
    // each endpoint is a mapping of versions to actions
    endpoint {
        type: object
        additionalProperties: false
        properties {

            // whether endpoint is internal
            internal { type: boolean }
            // whether endpoint requires authorization
            authorize { type: boolean }
            // list of roles allowed to access endpoint
            allow_roles {
                type: array
                items { type: string }
            }
        }
        patternProperties {
            "^\d\.\d+$" { "$ref": "#/definitions/action" }
        }
    }
    // an action describes request and response
    action {
        type: object
        additionalProperties: false
        // must have response
        required: [ response, description ]
        // must have either request or batch_request
        oneOf: [
            { required: [ request ] }
            { required: [ batch_request ] }
        ]
        properties {
            method { const: post }
            description { type: string }
            request {
                oneOf: [
                    { "$ref": "#/definitions/reference" }
                    { "$ref": "#/definitions/request" }
                    { "$ref": "#/definitions/multi_request" }
                ]
            }
            response {
                type: object
                oneOf: [
                    { "$ref": "#/definitions/response" }
                    // { "$ref": "#/definitions/reference" }
                    {
                        type: object
                        properties {
                            type { const: string }
                        }
                        additionalProperties: false
                    }
                ]
            }
            batch_request {
                type: object
                additionalProperties: false
                // required: [ action, version, description ]
                required: [ action, version ]
                properties {
                    action { type: string }
                    version { type: number }
                    // description { type: string }
                }
            }
        }
    }
    // describes request to server
    request {
        type: object
        additionalProperties: false
        required: [
            type
        ]
        properties {
            // says it's an object
            type { const: object }
            // required fields
            required {
                type: array
                items { type: string }
            }
            // request fields,
            // an object that can have anything
            properties {
                type: object
                additionalProperties {
                    type: object
                    required: [ description ]
                    properties {
                        description { type: string }
                    }
                    additionalProperties: true
                }
            }
            dependencies { type: object }
            // can have an "additionalProperties" section
            additionalProperties { "$ref": "#/definitions/additional_properties" }
        }
    }
    multi_request {
        type: object
        required: [ type ]
        additionalProperties: false
        oneOf: [
            {
                required: [ anyOf ]
            }
            {
                required: [ oneOf ]
            }
        ]
        properties {
            type { const: object }
            anyOf {
                type: array
                items { "$ref": "#/definitions/reference" }
            }
            oneOf {
                type: array
                items { "$ref": "#/definitions/reference" }
            }
        }
    }
    response {
        type: object
        additionalProperties: false
        required: [
            type
        ]
        properties {
            // says it's an object
            type { const: object }
            // nothing is required
            // can have anything
            properties {
                type: object
                additionalProperties: true
            }
            // can have an "additionalProperties" section
            additionalProperties { "$ref": "#/definitions/additional_properties" }
        }
    }
}

// schema starts here!
type: object
required: [ _description ]
properties {
    _description { type: string }
    // definitions for generator
    _definitions {
        type: object
        additionalProperties {
            required: [ type ]
            properties {
                type { type: string }
            }
            // can have anything
            additionalProperties: true
        }
    }
    _references = ${properties._definitions}
    // default values for actions
    // can have anything
    _default {
        additionalProperties: true
    }
}
// describing each endpoint
additionalProperties {
    type: object
    // can be:
    oneOf: [
        // a reference
        { "$ref": "#/definitions/reference" }
        // or a mapping from versions to actions
        { "$ref": "#/definitions/endpoint" }
    ]
}
</file>

<file path="apiserver/schema/meta/validate.py">
#!/usr/bin/env python
⋮----
LINTER_URL = "https://www.jsonschemavalidator.net/"
⋮----
class LocalStorage(object)
⋮----
def __init__(self, driver)
⋮----
def __len__(self)
⋮----
def items(self)
⋮----
def keys(self)
⋮----
def get(self, key)
⋮----
def remove(self, key)
⋮----
def clear(self)
⋮----
def __getitem__(self, key)
⋮----
value = self.get(key)
⋮----
def __setitem__(self, key, value)
⋮----
def __contains__(self, key)
⋮----
def __iter__(self)
⋮----
def __repr__(self)
⋮----
class ValidationError(Exception)
⋮----
def __init__(self, *args)
⋮----
def report(self, schema_file)
⋮----
message = color(schema_file, fg='red')
⋮----
class InvalidFile(ValidationError)
⋮----
"""
    InvalidFile
    Wraps other exceptions that occur in file validation

    :param message: message to display
    """
⋮----
def __init__(self, message)
⋮----
def raise_original(self)
⋮----
def load_hocon(name)
⋮----
"""
    load_hocon
    load configuration from file

    :param name: file path
    """
⋮----
def validate_ascii_only(name)
⋮----
invalid_char = next(
⋮----
def validate_file(meta, name)
⋮----
"""
    validate_file
    validate file according to meta-scheme

    :param meta: meta-scheme
    :param name: file path
    """
⋮----
schema = load_hocon(name)
⋮----
path = "->".join(e.absolute_path)
message = "{}: {}".format(path, e.args[0])
⋮----
def parse_args()
⋮----
parser = argparse.ArgumentParser()
⋮----
def open_linter(driver, meta, schema)
⋮----
storage = LocalStorage(driver)
⋮----
class LazyDriver(object)
⋮----
def __init__(self)
⋮----
webdriver = None
common = None
⋮----
def __getattr__(self, item)
⋮----
@property
    def driver(self)
⋮----
def wait(self)
⋮----
def remove_description(dct)
⋮----
def main(here: str)
⋮----
args = parse_args()
meta = load_hocon(here + "/meta.conf")
⋮----
driver = LazyDriver()
⋮----
collisions = {}
⋮----
schema = validate_file(meta, schema_file)
⋮----
service_name = str(Path(schema_file).stem)
⋮----
warning = color("warning", fg="red")
⋮----
groups = [
</file>

<file path="apiserver/schema/schema_reader.py">
"""
Objects representing schema entities
"""
⋮----
log = config.logger(__file__)
⋮----
ALL_ROLES = "*"
⋮----
class EndpointSchema
⋮----
REQUEST_KEY = "request"
RESPONSE_KEY = "response"
BATCH_REQUEST_KEY = "batch_request"
DEFINITIONS_KEY = "definitions"
⋮----
"""
        Class for interacting with the schema of a single endpoint
        :param service_name: name of containing service
        :param action_name: name of action
        :param version: endpoint version
        :param schema: endpoint schema
        :param definitions: service definitions
        """
⋮----
class EndpointVersionsGroup
⋮----
endpoints: Sequence[EndpointSchema]
allow_roles: Sequence[str]
internal: bool
authorize: bool
⋮----
def __repr__(self)
⋮----
"""
        Represents multiple implementations of a single endpoint, discriminated by API version
        :param service_name: name of containing service
        :param action_name: name of action
        :param conf: mapping between minimum version to endpoint schema
        :param definitions: service definitions
        :param defaults: service defaults
        """
⋮----
def parse_version(version)
⋮----
def allows(self, role)
⋮----
def _pop_attr_with_default(self, conf, attr)
⋮----
def get_for_version(self, min_version: PartialVersion)
⋮----
"""
        Return endpoint schema for version
        """
⋮----
class Service
⋮----
endpoint_groups: Mapping[str, EndpointVersionsGroup]
⋮----
def __init__(self, name: str, conf: dict, api_defaults: dict)
⋮----
"""
        Represents schema of one service
        :param name: name of service
        :param conf: service configuration, containing endpoint groups and other details
        :param api_defaults: API-wide endpoint attributes default values
        """
⋮----
conf = subdict(conf, drop=("_description", "_references"))
⋮----
class Schema
⋮----
services: Mapping[str, Service]
⋮----
def __init__(self, services: dict, api_defaults: dict)
⋮----
"""
        Represents the entire API schema
        :param services: services schema
        :param api_defaults: default values of service configuration
        """
⋮----
@attr.s()
class SchemaReader
⋮----
root = Path(__file__).parent / "services"
cache_path: Path = None
⋮----
def __attrs_post_init__(self)
⋮----
@staticmethod
    def mod_time(path)
⋮----
"""
        return file modification time
        """
⋮----
@staticmethod
    def read_file(path)
⋮----
def get_schema(self)
⋮----
"""
        Parse the API schema to schema object.
        Load from config files and write to cache file if possible.
        """
services = [
⋮----
current_services_names = {path.stem for path in services}
⋮----
result = json.loads(self.cache_path.read_text())
cached_services_names = set(result.pop("services_names", []))
⋮----
services = {path.stem: self.read_file(path) for path in services}
api_defaults = self.read_file(self.root / "_api_defaults.conf")
</file>

<file path="apiserver/schema/services/_api_defaults.conf">
internal: false
allow_roles: ["*"]
authorize: true
</file>

<file path="apiserver/schema/services/_common.conf">
field_filter {
    type: object
    description: Filter on a field that includes combination of 'any' or 'all' included and excluded terms
    properties {
        any {
            type: object
            description: All the terms in 'any' condition are combined with 'or' operation
            properties {
                "include" {
                    type: array
                    items {type: string}
                }
                exclude {
                    type: array
                    items {type: string}
                }
            }
        }
        all {
            type: object
            description: All the terms in 'all' condition are combined with 'and' operation
            properties {
                "include" {
                    type: array
                    items {type: string}
                }
                exclude {
                    type: array
                    items {type: string}
                }
            }
        }
        op {
            type: string
            description: The operation between 'any' and 'all' parts of the filter if both are provided
            default: and
            enum: [and, or]
        }
    }
}
metadata_item {
    type: object
    properties {
        key {
            type: string
            description: The key uniquely identifying the metadata item inside the given entity
        }
        type {
            type: string
            description: The type of the metadata item
        }
        value {
            type: string
            description: The value stored in the metadata item
        }
    }
}
task_status_enum {
    type: string
    enum: [
        created
        queued
        in_progress
        stopped
        published
        publishing
        closed
        failed
        completed
        unknown
    ]
}
multi_field_pattern_data {
    type: object
    properties {
        pattern {
            description: "Pattern string (regex). Either 'pattern' or 'datetime' should be specified"
            type: string
        }
        datetime {
            description: "Date time conditions (applicable only to datetime fields). Either 'pattern' or 'datetime' should be specified"
            type: string
        }
        fields {
            description: "List of field names"
            type: array
            items { type: string }
        }
    }
}
credentials {
    type: object
    properties {
        access_key {
            type: string
            description: Credentials access key
        }
        secret_key {
            type: string
            description: Credentials secret key
        }
        label {
            type: string
            description: Optional credentials label
        }
    }
}
batch_operation {
    request {
        type: object
        required: [ids]
        properties {
            ids {
                type: array
                items {type: string}
            }
        }
    }
    response {
        type: object
        properties {
            succeeded {
                type: array
                items {
                    type: object
                    properties {
                        id: {
                            description: ID of the succeeded entity
                            type: string
                        }
                    }
                }
            }
            failed {
                type: array
                items {
                    type: object
                    properties {
                        id: {
                            description: ID of the failed entity
                            type: string
                        }
                        error: {
                            description: Error info
                            type: object
                            properties {
                                codes {
                                    type: array
                                    items {type: integer}
                                }
                                msg {
                                    type: string
                                }
                                data {
                                    type: object
                                    additionalProperties: True
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/_events_common.conf">
scalar_key_enum {
    type: string
    enum: [
        iter
        timestamp
        iso_time
    ]
}
metric_variants {
    type: object
    properties {
        metric {
            description: The metric name
            type: string
        }
        variants {
            type: array
            description: The names of the metric variants
            items {type: string}
        }
    }
}
debug_images_response_task_metrics {
    type: object
    properties {
        task {
            type: string
            description: Task ID
        }
        iterations {
            type: array
            items {
                type: object
                properties {
                    iter {
                        type: integer
                        description: Iteration number
                    }
                    events {
                        type: array
                        items {
                            type: object
                            description: Debug image event
                        }
                    }
                }
            }
        }
    }
}
debug_images_response {
    type: object
    properties {
        scroll_id {
            type: string
            description: "Scroll ID for getting more results"
        }
        metrics {
            type: array
            description: "Debug image events grouped by tasks and iterations"
            items {"$ref": "#/definitions/debug_images_response_task_metrics"}
        }
    }
}
plots_response_task_metrics {
    type: object
    properties {
        task {
            type: string
            description: Task ID
        }
        iterations {
            type: array
            items {
                type: object
                properties {
                    iter {
                        type: integer
                        description: Iteration number
                    }
                    events {
                        type: array
                        items {
                            type: object
                            description: Plot event
                        }
                    }
                }
            }
        }
    }
}
plots_response {
    type: object
    properties {
        scroll_id {
            type: string
            description: "Scroll ID for getting more results"
        }
        metrics {
            type: array
            description: "Plot events grouped by tasks and iterations"
            items {"$ref": "#/definitions/plots_response_task_metrics"}
        }
    }
}
single_value_task_metrics {
    type: object
    properties {
        task {
            type: string
            description: Task ID
        }
        task_name {
            type: string
            description: Task name
        }
        values {
            type: array
            items {
                type: object
                properties {
                    metric { type: string }
                    variant { type: string}
                    value { type: number }
                    timestamp { type: number }
                }
            }
        }
    }
}
single_value_metrics_response {
    type: object
    properties {
        tasks {
            description: Single value metrics grouped by task
            type: array
            items {"$ref": "#/definitions/single_value_task_metrics"}
        }
    }
}
</file>

<file path="apiserver/schema/services/_tasks_common.conf">
include "_common.conf"
task_type_enum {
    type: string
    enum: [
        training
        testing
        inference
        data_processing
        application
        monitor
        controller
        optimizer
        service
        qc
        custom
    ]
}
script {
    type: object
    properties {
        binary {
            description: "Binary to use when running the script"
            type: string
            default: python
        }
        repository {
            description: "Name of the repository where the script is located"
            type: string
        }
        tag {
            description: "Repository tag"
            type: string
        }
        branch  {
            description: "Repository branch id If not provided and tag not provided, default repository branch is used."
            type: string
        }
        version_num {
            description: "Version (changeset) number. Optional (default is head version) Unused if tag is provided."
            type: string
        }
        entry_point {
            description: "Path to execute within the repository"
            type: string
        }
        working_dir {
            description: "Path to the folder from which to run the script Default - root folder of repository"
            type: string
        }
        requirements {
            description: "A JSON object containing requirements strings by key"
            type: object
        }
        diff {
            description: "Uncommitted changes found in the repository when task was run"
            type: string
        }
    }
}
model_type_enum {
    type: string
    enum: ["input", "output"]
}
task_model_item {
    type: object
    required: [ name, model]
    properties {
        name {
            description: "The task model name"
            type: string
        }
        model {
            description: "The model ID"
            type: string
        }
    }
}
output {
        type: object
        properties {
            destination {
                description: "Storage id. This is where output files will be stored."
                type: string
            }
            model {
                description: "Model id."
                type: string
            }
            result {
                description: "Task result. Values: 'success', 'failure'"
                type: string
            }
            error {
                description: "Last error text"
                type: string
            }
        }
    }
task_execution_progress_enum {
    type: string
    enum: [
        unknown
        running
        stopping
        stopped
    ]
}
artifact_type_data {
    type: object
    properties {
        preview {
            description: "Description or textual data"
            type: string
        }
        content_type {
            description: "System defined raw data content type"
            type: string
        }
        data_hash {
            description: "Hash of raw data, without any headers or descriptive parts"
            type: string
        }
    }
}
artifact_mode_enum {
    type: string
    enum: [
        input
        output
    ]
    default: output
}
artifact {
    type: object
    required: [key, type]
    properties {
        key {
            description: "Entry key"
            type: string
        }
        type {
            description: "System defined type"
            type: string
        }
        mode {
            description: "System defined input/output indication"
            "$ref": "#/definitions/artifact_mode_enum"
        }
        uri {
            description: "Raw data location"
            type: string
        }
        content_size {
            description: "Raw data length in bytes"
            type: integer
        }
        hash {
            description: "Hash of entire raw data"
            type: string
        }
        timestamp {
            description: "Epoch time when artifact was created"
            type: integer
        }
        type_data {
            description: "Additional fields defined by the system"
            "$ref": "#/definitions/artifact_type_data"
        }
        display_data {
            description: "User-defined list of key/value pairs, sorted"
            type: array
            items {
                type: array
                items {
                    type: string  # can also be a number... TODO: upgrade the generator
                }
            }
        }
    }
}
artifact_id {
    type: object
    required: [key]
    properties {
        key {
            description: "Entry key"
            type: string
        }
        mode {
            description: "System defined input/output indication"
            "$ref": "#/definitions/artifact_mode_enum"
        }
    }
}
task_models {
    type: object
    properties {
        input {
            description: "The list of task input models"
            type: array
            items {"$ref": "#/definitions/task_model_item"}

        }
        output {
            description: "The list of task output models"
            type: array
            items {"$ref": "#/definitions/task_model_item"}
        }
    }
}
execution {
    type: object
    properties {
        queue {
            description: "Queue ID where task was queued."
            type: string
        }
        parameters {
            description: "Json object containing the Task parameters"
            type: object
            additionalProperties: true
        }
        model {
            description: "Execution input model ID Not applicable for Register (Import) tasks"
            type: string
        }
        model_desc {
            description: "Json object representing the Model descriptors"
            type: object
            additionalProperties: true
        }
        model_labels {
            description: """Json object representing the ids of the labels in the model.
            The keys are the layers' names and the values are the IDs.
            Not applicable for Register (Import) tasks.
            Mandatory for Training tasks"""
            type: object
            additionalProperties: { type: integer }
        }
        framework {
            description: """Framework related to the task. Case insensitive. Mandatory for Training tasks. """
            type: string
        }
        docker_cmd {
            description: "Command for running docker script for the execution of the task"
            type: string
        }
        artifacts {
            description: "Task artifacts"
            type: array
            items { "$ref": "#/definitions/artifact" }
        }
    }
}
last_metrics_event {
    type: object
    properties {
        metric {
            description: "Metric name"
            type: string
        }
        variant {
            description: "Variant name"
            type: string
        }
        value {
            description: "Last value reported"
            type: number
        }
        min_value {
            description: "Minimum value reported"
            type: number
        }
        min_value_iteration {
            description: "The iteration at which the minimum value was reported"
            type: integer
        }
        max_value {
            description: "Maximum value reported"
            type: number
        }
        max_value_iteration {
            description: "The iteration at which the maximum value was reported"
            type: integer
        }
        first_value {
            description: "First value reported"
            type: number
        }
        first_value_iteration {
            description: "The iteration at which the first value was reported"
            type: integer
        }
        mean_value {
            description: "The mean value"
            type: number
        }
        count {
            description: "The total count of reported values"
            type: integer
        }
        x_axis_label {
            description: The user defined value for the X-Axis name stored with the event
            type: string
        }
    }
}
last_metrics_variants {
    type: object
    description: "Last metric events, one for each variant hash"
    additionalProperties {
        "$ref": "#/definitions/last_metrics_event"
    }
}
params_item {
    type: object
    properties {
        section {
            description: "Section that the parameter belongs to"
            type: string
        }
        name {
            description: "Name of the parameter. The combination of section and name should be unique"
            type: string
        }
        value {
            description: "Value of the parameter"
            type: string
        }
        type {
            description: "Type of the parameter. Optional"
            type: string
        }
        description {
            description: "The parameter description. Optional"
            type: string
        }
    }
}
configuration_item {
    type: object
    properties {
        name {
            description: "Name of the parameter. Should be unique"
            type: string
        }
        value {
            description: "Value of the parameter"
            type: string
        }
        type {
            description: "Type of the parameter. Optional"
            type: string
        }
        description {
            description: "The parameter description. Optional"
            type: string
        }
    }
}
section_params {
    description: "Task section params"
    type: object
    additionalProperties {
        "$ref": "#/definitions/params_item"
    }
}
task {
    type: object
    properties {
        id {
            description: "Task id"
            type: string
        }
        name {
            description: "Task Name"
            type: string
        }
        user {
            description: "Associated user id"
            type: string
        }
        company {
            description: "Company ID"
            type: string
        }
        type {
            description: "Type of task. Values: 'training', 'testing'"
            "$ref": "#/definitions/task_type_enum"
        }
        status {
            description: ""
            "$ref": "#/definitions/task_status_enum"
        }
        comment {
            description: "Free text comment"
            type: string
        }
        created {
            description: "Task creation time (UTC) "
            type: string
            format: "date-time"
        }
        started {
            description: "Task start time (UTC)"
            type: string
            format: "date-time"
        }
        completed {
            description: "Task end time (UTC)"
            type: string
            format: "date-time"
        }
        active_duration {
            description: "Task duration time (seconds)"
            type: integer
        }
        parent {
            description: "Parent task id"
            type: string
        }
        project {
            description: "Project ID of the project to which this task is assigned"
            type: string
        }
        output {
            description: "Task output params"
            "$ref": "#/definitions/output"
        }
        execution {
            description: "Task execution params"
            "$ref": "#/definitions/execution"
        }
        container {
            description: "Docker container parameters"
            type: object
            additionalProperties { type: string }
        }
        models {
            description: "Task models"
            "$ref": "#/definitions/task_models"
        }
        // TODO: will be removed
        script {
            description: "Script info"
            "$ref": "#/definitions/script"
        }
        tags {
            description: "User-defined tags list"
            type: array
            items { type: string }
        }
        system_tags {
            description: "System tags list. This field is reserved for system use, please don't use it."
            type: array
            items { type: string }
        }
        status_changed {
            description: "Last status change time"
            type: string
            format: "date-time"
        }
        status_message {
            description: "free text string representing info about the status"
            type: string
        }
        status_reason {
            description: "Reason for last status change"
            type: string
        }
        published {
            description: "Task publish time"
            type: string
            format: "date-time"
        }
        last_worker {
            description: "ID of last worker that handled the task"
            type: string
        }
        last_worker_report {
            description: "Last time a worker reported while working on this task"
            type: string
            format: "date-time"
        }
        last_update {
            description: "Last time this task was created, edited, changed or events for this task were reported"
            type: string
            format: "date-time"
        }
        last_change {
            description: "Last time any update was done to the task"
            type: string
            format: "date-time"
        }
        last_iteration {
            description: "Last iteration reported for this task"
            type: integer
        }
        last_metrics {
            description: "Last metric variants (hash to events), one for each metric hash"
            type: object
            additionalProperties {
                "$ref": "#/definitions/last_metrics_variants"
            }
        }
        hyperparams {
            description: "Task hyper params per section"
            type: object
            additionalProperties {
                "$ref": "#/definitions/section_params"
            }
        }
        configuration {
            description: "Task configuration params"
            type: object
            additionalProperties {
                "$ref": "#/definitions/configuration_item"
            }
        }
        runtime {
            description: "Task runtime mapping"
            type: object
            additionalProperties: true
        }
    }
}
</file>

<file path="apiserver/schema/services/_workers_common.conf">
machine_stats  {
    type: object
    properties {
        cpu_usage {
            description: "Average CPU usage per core"
            type: array
            items { type: number }
        }
        gpu_usage {
            description: "Average GPU usage per GPU card"
            type: array
            items { type: number }
        }
        memory_used {
            description: "Used memory MBs"
            type: number
        }
        memory_free {
            description: "Free memory MBs"
            type: number
        }
        gpu_memory_free {
            description: "GPU free memory MBs"
            type: array
            items { type: number }
        }
        gpu_memory_used {
            description: "GPU used memory MBs"
            type: array
            items { type: number }
        }
        network_tx {
            description: "Mbytes per second"
            type: number
        }
        network_rx {
            description: "Mbytes per second"
            type: number
        }
        disk_free_home {
            description: "Free space in % of /home drive"
            type: number
        }
        disk_free_temp {
            description: "Free space in % of /tmp drive"
            type: number
        }
        disk_read {
            description: "Mbytes read per second"
            type: number
        }
        disk_write {
            description: "Mbytes write per second"
            type: number
        }
        cpu_temperature {
            description: "CPU temperature"
            type: array
            items { type: number }
        }
        gpu_temperature {
            description: "GPU temperature"
            type: array
            items { type: number }
        }
    }
}
</file>

<file path="apiserver/schema/services/auth.conf">
_description: """This service provides authentication management and authorization
validation for the entire system."""
_default {
    internal: true
    allow_roles: ["system", "root"]
}

_definitions {
    include "_common.conf"
    credential_key {
        type: object
        properties {
            access_key {
                type: string
                description: ""
            }
            label {
                type: string
                description: Optional credentials label
            }
            last_used {
                type: string
                description: ""
                format: "date-time"
            }
            last_used_from {
                type: string
                description: ""
            }
        }
    }
    role {
        type: string
        enum: [ admin, superuser, user, annotator ]
    }
}

login {
    internal: false
    allow_roles = [ "*" ]
    "2.1" {
        description: """Get a token based on supplied credentials (key/secret).
        Intended for use by users with key/secret credentials that wish to obtain a token
        for use with other services."""
        request {
            type: object
            properties {
                expiration_sec {
                    type: integer
                        description: """Requested token expiration time in seconds. 
                        Not guaranteed,  might be overridden by the service"""
                }
            }
        }
        response {
            type: object
            properties {
                token {
                    type: string
                    description: Token string
                }
            }
        }
    }
}

logout {
    internal: false
    allow_roles = [ "*" ]
    "2.2" {
        description: """Removes the authentication cookie from the current session"""
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}

get_token_for_user {
    "2.1" {
        description: """Get a token for the specified user. Intended for internal use."""
        request {
            type: object
            required: [
                user
            ]
            properties {
                user {
                    type: string
                    description: User ID
                }
                company {
                    type: string
                    description: Company ID
                }
                expiration_sec {
                    type: integer
                    description: """Requested token expiration time in seconds.
                    Not guaranteed,  might be overridden by the service"""
                }
            }
        }
        response {
            type: object
            properties {
                token {
                    type: string
                    description: ""
                }
            }
        }
    }
}

validate_token {
    "2.1" {
        description: """Validate a token and return user identity if valid.
        Intended for internal use. """
        request {
            type: object
            required: [ token ]
            properties {
                token {
                    type: string
                    description: Token string
                }
            }
        }
        response {
            type: object
            properties {
                valid {
                    type: boolean
                    description: Boolean indicating if the token is valid
                }
                user {
                    type: string
                    description: Associated user ID
                }
                company {
                    type: string
                    description: Associated company ID
                }
            }
        }
    }
}

create_user {
    "2.1" {
        allow_roles: ["system", "root", "admin"]
        description: """Creates a new user auth entry. Intended for internal use. """
        request {
            type: object
            required: [
                name
                company
                email
            ]
            properties {
                name {
                    type: string
                    description: User name (makes the auth entry more readable)
                }
                company {
                    type: string
                    description: Associated company ID
                }
                email {
                    type: string
                    description: Email address uniquely identifying the user
                }
                role {
                    description: User role
                    default: user
                    "$ref": "#/definitions/role"
                }
                given_name {
                    type: string
                    description: Given name
                }
                family_name {
                    type: string
                    description: Family name
                }
                avatar {
                    type: string
                    description: Avatar URL
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    type: string
                    description: New user ID
                }
            }
        }
    }
}

create_credentials {
    allow_roles = [ "*" ]
    internal: false
    "2.1" {
        description: """Creates a new set of credentials for the authenticated user.
                        New key/secret is returned.
                        Note: Secret will never be returned in any other API call.
                        If a secret is lost or compromised, the key should be revoked
                        and a new set of credentials can be created."""
        request {
            type: object
            properties {}
            additionalProperties: false
        }
        response {
            type: object
            properties {
                credentials {
                    "$ref": "#/definitions/credentials"
                    description: Created credentials
                }
            }
        }
    }
    "2.17": ${create_credentials."2.1"} {
        request.properties.label {
            type: string
            description: Optional credentials label
        }
    }
}

get_credentials {
    allow_roles = [ "*" ]
    internal: false
    "2.1" {
        description: """Returns all existing credential keys for the authenticated user.
        Note: Only credential keys are returned."""
        request {
            type: object
            properties {}
            additionalProperties: false
        }
        response {
            type: object
            properties {
                credentials {
                    description: "List of credentials, each with an empty secret field."
                    type: array
                    items { "$ref": "#/definitions/credential_key" }
                }
            }
        }
    }
}

edit_credentials {
    allow_roles = [ "*" ]
    internal: false
    "2.19" {
        description: """Updates the label of the existing credentials for the authenticated user."""
        request {
            type: object
            required: [ access_key ]
            properties {
                access_key {
                    type: string
                    description: Existing credentials key
                }
                label {
                    type: string
                    description: New credentials label
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of credentials updated"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}

revoke_credentials {
    allow_roles = [ "*" ]
    internal: false
    "2.1" {
        description: """Revokes (and deletes) a set (key, secret) of credentials for
        the authenticated user."""
        request {
            type: object
            required: [ key_id ]
            properties {
                access_key {
                    type: string
                    description: Credentials key
                }
            }
        }
        response {
            type: object
            properties {
                revoked {
                    description: "Number of credentials revoked"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}

edit_user {
    internal: false
    allow_roles: ["system", "root", "admin"]
    "2.1" {
        description:  """ Edit a users' auth data properties"""
        request {
            type: object
            properties {
                user {
                    description: "User ID"
                    type: string
                }
                role {
                    description: "The new user's role within the company"
                    type: string
                    enum: [admin, superuser, user, annotator]
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of users updated (0 or 1)"
                    type: number
                    enum: [ 0, 1 ]
                }
                fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
}

fixed_users_mode {
    authorize: false
    "2.1" {
        description:  """ Return fixed users mode status"""
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                enabled {
                    description: "Fixed users mode enabled"
                    type: boolean
                }
                server_errors {
                    description: "Server initialization errors"
                    type: object
                    additionalProperties: True
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/debug.conf">
_description: "debugging utilities"
ping {
    authorize: false
    "2.9" {
        description: "Ping server"
        request {
            type: object
            additionalProperties: true
        }
        response {
            type: object
            properties: {
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/events.conf">
_description : "Provides an API for running tasks to report events collected by the system."
_definitions {
    include "_events_common.conf"
    metrics_scalar_event {
        description: "Used for reporting scalar metrics during training task"
        type: object
        required: [ task, type ]
        properties {
            timestamp {
                description: "Epoch milliseconds UTC, will be set by the server if not set."
                type: number
            }
            type {
                description: "'training_stats_scalar'"
                type: string
            }
            task {
                description: "Task ID (required)"
                type: string
            }
            iter {
                description: "Iteration"
                type: integer
            }
            metric  {
                description: "Metric name, e.g. 'count', 'loss', 'accuracy'"
                type: string
            }
            variant {
                description: "E.g. 'class_1', 'total', 'average'"
                type: string
            }
            value {
                description: ""
                type: number
            }
            x_axis_label {
                description: "Custom X-Axis label to be used when displaying the scalars histogram"
                type: string
            }
        }
    }
    metrics_vector_event {
        description: "Used for reporting vector metrics during training task"
        type: object
        required: [ task ]
        properties {
            timestamp {
                description: "Epoch milliseconds UTC, will be set by the server if not set."
                type: number
            }
            type {
                description: "'training_stats_vector'"
                type: string
            }
            task {
                description: "Task ID (required)"
                type: string
            }
            iter {
                description: "Iteration"
                type: integer
            }
            metric {
                description: "Metric name, e.g. 'count', 'loss', 'accuracy'"
                type: string
            }
            variant {
                description: "E.g. 'class_1', 'total', 'average"
                type: string
            }
            values {
                description: "vector of float values"
                type: array
                items { type: number }
            }
        }
    }
    metrics_image_event {
        description: "An image or video was dumped to storage for debugging"
        type: object
        required: [ task, type ]
        properties {
            timestamp {
                description: "Epoch milliseconds UTC, will be set by the server if not set."
                type: number
            }
            type {
                description: "'training_debug_image'"
                type: string
            }
            task {
                description: "Task ID (required)"
                type: string
            }
            iter {
                description: "Iteration"
                type: integer
            }
            metric {
                description: "Metric name, e.g. 'count', 'loss', 'accuracy'"
                type: string
            }
            variant {
                description: "E.g. 'class_1', 'total', 'average"
                type: string
            }
            key {
                description: "File key"
                type: string
            }
            url {
                description: "File URL"
                type: string
            }
        }
    }
    metrics_plot_event {
        description: """ An entire plot (not single datapoint) and it's layout.
        Used for plotting ROC curves, confidence matrices, etc. when evaluating the net."""
        type: object
        required: [ task, type ]
        properties {
            timestamp {
                description: "Epoch milliseconds UTC, will be set by the server if not set."
                type: number
            }
            type {
                description: "'plot'"
                type: string
            }
            task {
                description: "Task ID (required)"
                type: string
            }
            iter {
                description: "Iteration"
                type: integer
            }
            metric {
                description: "Metric name, e.g. 'count', 'loss', 'accuracy'"
                type: string
            }
            variant {
                description: "E.g. 'class_1', 'total', 'average"
                type: string
            }
            plot_str {
                description: """An entire plot (not single datapoint) and it's layout.
                Used for plotting ROC curves, confidence matrices, etc. when evaluating the net.
                """
                type: string
            }
            skip_validation {
                description: "If set then plot_str is not checked for a valid json. The default is False"
                type: boolean
            }
        }
    }
    log_level_enum {
        type: string
        enum: [
            notset
            debug
            verbose
            info
            warn
            warning
            error
            fatal
            critical
        ]
    }
    event_type_enum {
        type: string
        enum: [
            training_stats_scalar
            training_stats_vector
            training_debug_image
            plot
            log
        ]
    }
    task_metric {
        type: object
        required: [task]
        properties {
            task {
                description: "Task ID"
                type: string
            }
            metric {
                description: "Metric name"
                type: string
            }
        }
    }
    task_metric_variants {
        type: object
        required: [task]
        properties {
            task {
                description: "Task ID"
                type: string
            }
            metric {
                description: "Metric name"
                type: string
            }
            variants {
                description: Metric variant names
                type: array
                items {type: string}
            }
        }
    }
    task_log_event {
        description: """A log event associated with a task."""
        type: object
        required: [ task, type ]
        properties {
            timestamp {
                description: "Epoch milliseconds UTC, will be set by the server if not set."
                type: number
            }
            type {
                description: "'log'"
                type: string
            }
            task {
                description: "Task ID (required)"
                type: string
            }
            level {
                description: "Log level."
                "$ref": "#/definitions/log_level_enum"
            }
            worker {
                description: "Name of machine running the task."
                type: string
            }
            msg {
                description: "Log message."
                type: string
            }
        }
    }
    debug_image_sample_response {
        type: object
        properties {
            scroll_id {
                type: string
                description: "Scroll ID to pass to the next calls to get_debug_image_sample or next_debug_image_sample"
            }
            event {
                type: object
                description: "Debug image event"
            }
            min_iteration {
                type: integer
                description: "minimal valid iteration for the variant"
            }
            max_iteration {
                type: integer
                description: "maximal valid iteration for the variant"
            }
        }
    }
    plot_sample_response {
        type: object
        properties {
            scroll_id {
                type: string
                description: "Scroll ID to pass to the next calls to get_plot_sample or next_plot_sample"
            }
            events {
                description: "Plot events"
                type: array
                items { type: object}
            }
            min_iteration {
                type: integer
                description: "minimal valid iteration for the metric"
            }
            max_iteration {
                type: integer
                description: "maximal valid iteration for the metric"
            }
        }
    }
}
add {
    "2.1" {
        description: "Adds a single event"
        request {
            type: object
            anyOf: [
                { "$ref": "#/definitions/metrics_scalar_event" }
                { "$ref": "#/definitions/metrics_vector_event" }
                { "$ref": "#/definitions/metrics_image_event" }
                { "$ref": "#/definitions/metrics_plot_event" }
                { "$ref": "#/definitions/task_log_event" }
            ]
        }
        response {
            type: object
            additionalProperties: true
        }
    }
    "2.22": ${add."2.1"} {
        request.properties {
            model_event {
                type: boolean
                description: If set then the event is for a model. Otherwise for a task. Cannot be used with task log events. If used in batch then all the events should be marked the same
                default: false
            }
            allow_locked {
                type: boolean
                description: Allow adding events to published tasks or models
                default: false
            }
        }
    }
}
add_batch {
    "2.1" {
        description: "Adds a batch of events in a single call (json-lines format, stream-friendly)"
        batch_request: {
            action: add
            version: 2.1
        }
        response {
            type: object
            properties {
                added { type: integer }
                errors { type: integer }
                errors_info { type: object }
            }
        }
    }
    "2.22": ${add_batch."2.1"} {
        batch_request: {
            action: add
            version: 2.22
        }
    }
}
delete_for_task {
    "2.1" {
        description: "Delete all task events. *This cannot be undone!*"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                allow_locked {
                    type: boolean
                    description: "Allow deleting events even if the task is locked"
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    type: boolean
                    description: "Number of deleted events"
                }
            }
        }
    }
}
delete_for_model {
    "2.22" {
        description: "Delete all model events. *This cannot be undone!*"
        request {
            type: object
            required: [
                model
            ]
            properties {
                model {
                    type: string
                    description: "Model ID"
                }
                allow_locked {
                    type: boolean
                    description: "Allow deleting events even if the model is locked"
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    type: boolean
                    description: "Number of deleted events"
                }
            }
        }
    }
}
debug_images {
    "2.1" {
        description: "Get all debug images of a task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                iters {
                    type: integer
                    description: "Max number of latest iterations for which to return debug images"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID of previous call (used for getting more results)"
                }
            }
        }
        response {
            type: object
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                images {
                    type: array
                    items { type: object }
                    description: "Images list"
                }
                returned {
                    type: integer
                    description: "Number of results returned"
                }
                total {
                    type: number
                    description: "Total number of results available for this query. In case there are more than 10000 results it is set to 10000"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID for getting more results"
                }
            }
        }
    }
    "2.7" {
        description: "Get the debug image events for the requested amount of iterations per each task"
        request {
            type: object
            required: [
                metrics
            ]
            properties {
                metrics {
                    type: array
                    items { "$ref": "#/definitions/task_metric" }
                    description: "List of task metrics for which the envents will be retreived"
                }
                iters {
                    type: integer
                    description: "Max number of latest iterations for which to return debug images"
                }
                navigate_earlier {
                    type: boolean
                    description: "If set then events are retreived from latest iterations to earliest ones. Otherwise from earliest iterations to the latest. The default is True"
                }
                refresh {
                    type: boolean
                    description: "If set then scroll will be moved to the latest iterations. The default is False"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID of previous call (used for getting more results)"
                }
            }
        }
        response {"$ref": "#/definitions/debug_images_response"}
    }
    "2.14": ${debug_images."2.7"} {
        request {
            properties {
                metrics {
                    type: array
                    description: List of metrics and variants
                    items { "$ref": "#/definitions/task_metric_variants" }
                }
            }
        }
    }
    "2.22": ${debug_images."2.14"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
plots {
    "2.20" {
        description: "Get plot events for the requested amount of iterations per each task"
        request {
            type: object
            required: [
                metrics
            ]
            properties {
                metrics {
                    type: array
                    description: List of metrics and variants
                    items { "$ref": "#/definitions/task_metric_variants" }
                }
                iters {
                    type: integer
                    description: "Max number of latest iterations for which to return plots"
                }
                navigate_earlier {
                    type: boolean
                    description: "If set then events are retreived from latest iterations to earliest ones. Otherwise from earliest iterations to the latest. The default is True"
                }
                refresh {
                    type: boolean
                    description: "If set then scroll will be moved to the latest iterations. The default is False"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID of previous call (used for getting more results)"
                }
            }
        }
        response {"$ref": "#/definitions/plots_response"}
    }
    "2.22": ${plots."2.20"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model plots. Otherwise task plots
            default: false
        }
    }
}
get_debug_image_sample {
    "2.12": {
        description: "Return the debug image per metric and variant for the provided iteration"
        request {
            type: object
            required: [task, metric, variant]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                metric {
                    description: "Metric name"
                    type: string
                }
                variant {
                    description: "Metric variant"
                    type: string
                }
                iteration {
                    description: "The iteration to bring debug image from. If not specified then the latest reported image is retrieved"
                    type: integer
                }
                refresh {
                    description: "If set then scroll state will be refreshed to reflect the latest changes in the debug images"
                    type: boolean
                }
                scroll_id {
                    type: string
                    description: "Scroll ID from the previous call to get_debug_image_sample or empty"
                }
            }
        }
        response {"$ref": "#/definitions/debug_image_sample_response"}
    }
    "2.20": ${get_debug_image_sample."2.12"} {
        request.properties.navigate_current_metric {
            description: If set then subsequent navigation with next_debug_image_sample is done on the debug images for the passed metric only. Otherwise for all the metrics
            type: boolean
            default: true
        }
    }
    "2.22": ${get_debug_image_sample."2.20"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model debug images. Otherwise task debug images
            default: false
        }
    }
}
next_debug_image_sample {
    "2.12": {
        description: "Get the image for the next variant for the same iteration or for the next iteration"
        request {
            type: object
            required: [task, scroll_id]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                scroll_id {
                    type: string
                    description: "Scroll ID from the previous call to get_debug_image_sample"
                }
                navigate_earlier {
                    type: boolean
                    description: """If set then get the either previous variant event from the current iteration or (if does not exist) the last variant event from the previous iteration.
                    Otherwise next variant event from the current iteration or first variant event from the next iteration"""
                }
            }
        }
        response {"$ref": "#/definitions/debug_image_sample_response"}
    }
    "2.22": ${next_debug_image_sample."2.12"} {
        request.properties.next_iteration {
            type: boolean
            default: false
            description: If set then navigate to the next/previous iteration
        }
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model debug images. Otherwise task debug images
            default: false
        }
    }
}
get_plot_sample {
    "2.20": {
        description: "Return plots for the provided iteration"
        request {
            type: object
            required: [task, metric]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                metric {
                    description: "Metric name"
                    type: string
                }
                iteration {
                    description: "The iteration to bring plot from. If not specified then the latest reported plot is retrieved"
                    type: integer
                }
                refresh {
                    description: "If set then scroll state will be refreshed to reflect the latest changes in the plots"
                    type: boolean
                }
                scroll_id {
                    type: string
                    description: "Scroll ID from the previous call to get_plot_sample or empty"
                }
                navigate_current_metric {
                    description: If set then subsequent navigation with next_plot_sample is done on the plots for the passed metric only. Otherwise for all the metrics
                    type: boolean
                    default: true
                }
            }
        }
        response {"$ref": "#/definitions/plot_sample_response"}
    }
    "2.22": ${get_plot_sample."2.20"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model plots. Otherwise task plots
            default: false
        }
    }
}
next_plot_sample {
    "2.20": {
        description: "Get the plot for the next metric for the same iteration or for the next iteration"
        request {
            type: object
            required: [task, scroll_id]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                scroll_id {
                    type: string
                    description: "Scroll ID from the previous call to get_plot_sample"
                }
                navigate_earlier {
                    type: boolean
                    description: """If set then get the either previous metric events from the current iteration or (if does not exist) the last metric events from the previous iteration.
                    Otherwise next metric events from the current iteration or first metric events from the next iteration"""
                }
            }
        }
        response {"$ref": "#/definitions/plot_sample_response"}
    }
    "2.22": ${next_plot_sample."2.20"} {
        request.properties.next_iteration {
            type: boolean
            default: false
            description: If set then navigate to the next/previous iteration
        }
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model plots. Otherwise task plots
            default: false
        }
    }
}
get_task_metrics{
    "2.7": {
        description: "For each task, get a list of metrics for which the requested event type was reported"
        request {
            type: object
            required: [
                tasks
            ]
            properties {
                tasks {
                    type: array
                    items { type: string }
                    description: "Task IDs"
                }
                event_type {
                    "description": "Event type"
                    "$ref": "#/definitions/event_type_enum"
                }
            }
        }
        response {
            type: object
            properties {
                metrics {
                    type: array
                    items { type: object }
                    description: "List of task with their metrics"
                }
            }
        }
    }
    "2.22": ${get_task_metrics."2.7"} {
        request.properties.model_events {
            type: boolean
            description: If set then get metrics from model events. Otherwise from task events
            default: false
        }
    }
}
get_multi_task_metrics {
    "2.28" {
        description: """Get unique metrics and variants from the events of the specified type.
        Only events reported for the passed task or model ids are analyzed."""
        request {
            type: object
            required: [ tasks ]
            properties {
                tasks {
                    description: task ids to get metrics from
                    type: array
                    items {type: string}
                }
                model_events {
                    description: If not set or set to false then passed ids are task ids otherwise model ids
                    type: boolean
                    default: false
                }
                event_type {
                    "description": Event type. If not specified then metrics are collected from the reported events of all types
                    "$ref": "#/definitions/event_type_enum"
                }
            }
        }
        response {
            type: object
            properties {
                metrics {
                    type: array
                    description: List of metrics and variants
                    items { "$ref": "#/definitions/metric_variants" }
                }
            }
        }
    }
}
get_task_log {
    "1.5" {
        description: "Get all 'log' events for this task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                order {
                    type: string
                    description: "Timestamp order in which log events will be returned (defaults to ascending)"
                    enum: [
                        asc
                        desc
                    ]
                }
                scroll_id {
                    type: string
                    description: ""
                }
                batch_size {
                    type: integer
                    description: ""
                }
            }
        }
        response {
            type: object
            properties {
                events {
                    type: array
                    # TODO: items: log event
                    items { type: object }
                }
                returned { type: integer }
                total { type: integer }
                scroll_id { type: string }
            }
        }
    }
    "1.7" {
        description: "Get all 'log' events for this task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                order {
                    type: string
                    description: "Timestamp order in which log events will be returned (defaults to ascending)"
                    enum: [
                        asc
                        desc
                    ]
                }
                from {
                    type: string
                    description: "Where will the log entries be taken from (default to the head of the log)"
                    enum: [
                        head
                        tail
                    ]
                }
                scroll_id {
                    type: string
                    description: ""
                }
                batch_size {
                    type: integer
                    description: ""
                }
            }
        }
        response {
            type: object
            properties {
                events {
                    type: array
                    # TODO: items: log event
                    items { type: object }
                    description: "Log items list"
                }
                returned {
                    type: integer
                    description: "Number of results returned"
                }
                total {
                    type: number
                    description: "Total number of results available for this query. In case there are more than 10000 results it is set to 10000"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID for getting more results"
                }
            }
        }
    }
    "2.9" {
        description: "Get 'log' events for this task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                batch_size {
                    type: integer
                    description: "The amount of log events to return"
                }
                navigate_earlier {
                    type: boolean
                    description: "If set then log events are retreived from the latest to the earliest ones (in timestamp descending order, unless order='asc'). Otherwise from the earliest to the latest ones (in timestamp ascending order, unless order='desc'). The default is True"
                }
                from_timestamp {
                    type: number
                    description: "Epoch time in UTC ms to use as the navigation start. Optional. If not provided, reference timestamp is determined by the 'navigate_earlier' parameter (if true, reference timestamp is the last timestamp and if false, reference timestamp is the first timestamp)"
                }
                order {
                    type: string
                    description: "If set, changes the order in which log events are returned based on the value of 'navigate_earlier'"
                    enum: [asc, desc]
                }
            }
        }
        response {
            type: object
            properties {
                events {
                    type: array
                    items { type: object }
                    description: "Log items list"
                }
                returned {
                    type: integer
                    description: "Number of log events returned"
                }
                total {
                    type: number
                    description: "Total number of log events available for this query. In case there are more than 10000 events it is set to 10000"
                }
            }
        }
    }
    "2.30": ${get_task_log."2.9"} {
        request.metrics {
            type: array
            description: List of metrics and variants
            items { "$ref": "#/definitions/metric_variants" }
        }
    }
}
get_task_events {
    "2.1" {
        description: "Scroll through task events, sorted by timestamp"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                order {
                    type:string
                    description: "'asc' (default) or 'desc'."
                    enum: [
                        asc
                        desc
                    ]
                }
                scroll_id {
                    type: string
                    description: "Pass this value on next call to get next page"
                }
                batch_size {
                    type: integer
                    description: "Number of events to return each time (default 500)"
                }
                event_type {
                    type: string
                    description: "Return only events of this type"
                }
            }
        }
        response {
            type: object
            properties {
                events {
                    type: array
                    items { type: object }
                    description: "Events list"
                }
                returned {
                    type: integer
                    description: "Number of results returned"
                }
                total {
                    type: number
                    description: "Total number of results available for this query. In case there are more than 10000 results it is set to 10000"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID for getting more results"
                }
            }
        }
    }
    "2.22": ${get_task_events."2.1"} {
        request.properties {
            model_events {
                type: boolean
                description: If set then get retrieving model events. Otherwise task events
                default: false
            }
            metrics {
                type: array
                description: List of metrics and variants
                items { "$ref": "#/definitions/metric_variants" }
            }
        }
    }
}

download_task_log {
    "2.1" {
        description: "Get an attachment containing the task's log"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                line_type {
                    description: "Line format type"
                    type: string
                    enum: [
                        json
                        text
                    ]
                }
                line_format {
                    type: string
                    description: "Line string format. Used if the line type is 'text'"
                    default: "{asctime} {worker} {level} {msg}"
                }
            }
        }
        response {
            type: string
        }
    }
}
get_task_plots {
    "2.1" {
        description: "Get all 'plot' events for this task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                iters {
                    type: integer
                    description: "Max number of latest iterations for which to return plots"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID of previous call (used for getting more results)"
                }
            }
        }
        response {
            type: object
            properties {
                plots {
                    type: array
                    items {
                        type: object
                    }
                    description: "Plots list"
                }
                returned {
                    type: integer
                    description: "Number of results returned"
                }
                total {
                    type: number
                    description: "Total number of results available for this query. In case there are more than 10000 results it is set to 10000"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID for getting more results"
                }
            }
        }
    }
    "2.14": ${get_task_plots."2.1"} {
        request {
            properties {
                metrics {
                    type: array
                    description: List of metrics and variants
                    items { "$ref": "#/definitions/metric_variants" }
                }
            }
        }
    }
    "2.16": ${get_task_plots."2.14"} {
        request.properties.no_scroll {
            description: If true then no scroll is created. Suitable for one time calls
            type: boolean
            default: false
        }
    }
    "2.22": ${get_task_plots."2.16"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
get_multi_task_plots {
    "2.1" {
        description: "Get 'plot' events for the given tasks"
        request {
            type: object
            required: [
                tasks
            ]
            properties {
                tasks {
                    description: "List of task IDs"
                    type: array
                    items {
                        type: string
                        description: "Task ID"
                    }
                }
                iters {
                    type: integer
                    description: "Max number of latest iterations for which to return plots"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID of previous call (used for getting more results)"
                }
            }
        }
        response {
            type: object
            properties {
                plots {
                    type: object
                    description: "Plots mapping (keyed by task name)"
                }
                returned {
                    type: integer
                    description: "Number of results returned"
                }
                total {
                    type: number
                    description: "Total number of results available for this query. In case there are more than 10000 results it is set to 10000"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID for getting more results"
                }
            }
        }
    }
    "2.16": ${get_multi_task_plots."2.1"} {
        request.properties.no_scroll {
            description: If true then no scroll is created. Suitable for one time calls
            type: boolean
            default: false
        }
    }
    "2.22": ${get_multi_task_plots."2.16"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
    "2.26": ${get_multi_task_plots."2.22"} {
        request.properties.last_iters_per_task_metric {
            type: boolean
            description: If set to 'true' and iters passed then last iterations for each task metrics are retrieved. Otherwise last iterations for the whole task are retrieved
            default: true
        }
    }
    "2.28": ${get_multi_task_plots."2.26"} {
        request.properties.metrics {
            type: array
            description: List of metrics and variants
            items { "$ref": "#/definitions/metric_variants" }
        }
    }
}
get_vector_metrics_and_variants {
    "2.1" {
        description: ""
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
            }
        }
        response {
            type: object
            properties {
                metrics {
                    description: ""
                    type: array
                    items: { type: object }
                    # TODO: items: ???
                }
            }
        }
    }
    "2.22": ${get_vector_metrics_and_variants."2.1"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
vector_metrics_iter_histogram {
    "2.1" {
        description: "Get histogram data of all the scalar metrics and variants in the task"
        request {
            type: object
            required: [
                task
                metric
                variant
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                metric {
                    type: string
                    description: ""
                }
                variant {
                    type: string
                    description: ""
                }
            }
        }
        response {
            type: object
            properties {
                images {
                    type: array
                    items {
                        type: object
                    }
                }
            }
        }
    }
    "2.22": ${vector_metrics_iter_histogram."2.1"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
scalar_metrics_iter_histogram {
    "2.1" {
        description: "Get histogram data of all the vector metrics and variants in the task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                samples {
                    description: "The amount of histogram points to return (0 to return all the points). Optional, the default value is 6000."
                    type: integer
                }
                key {
                    description: """
                    Histogram x axis to use:
                    iter - iteration number
                    iso_time - event time as ISO formatted string
                    timestamp - event timestamp as milliseconds since epoch
                    """
                    "$ref": "#/definitions/scalar_key_enum"
                }
            }
        }
        response {
            type: object
            properties {
                images {
                    type: array
                    items {
                        type: object
                    }
                }

            }
        }
    }
    "2.14": ${scalar_metrics_iter_histogram."2.1"} {
        request {
            properties {
                metrics {
                    type: array
                    description: List of metrics and variants
                    items { "$ref": "#/definitions/metric_variants" }
                }
            }
        }
    }
    "2.22": ${scalar_metrics_iter_histogram."2.14"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
multi_task_scalar_metrics_iter_histogram {
    "2.1" {
        description: "Used to compare scalar stats histogram of multiple tasks"
        request {
            type: object
            required: [
                tasks
            ]
            properties {
                tasks {
                    description: "List of task Task IDs. Maximum amount of tasks is 100"
                    type: array
                    items {
                        type: string
                        description: "Task ID"
                    }
                }
                samples {
                    description: "The amount of histogram points to return. Optional, the default value is 6000"
                    type: integer
                }
                key {
                    description: """
                    Histogram x axis to use:
                    iter - iteration number
                    iso_time - event time as ISO formatted string
                    timestamp - event timestamp as milliseconds since epoch
                    """
                    "$ref": "#/definitions/scalar_key_enum"
                }
            }
        }
        response {
            type: object
            // properties {}
            additionalProperties: true
        }
    }
    "2.22": ${multi_task_scalar_metrics_iter_histogram."2.1"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
    "2.28": ${multi_task_scalar_metrics_iter_histogram."2.22"} {
        request.properties.metrics {
            type: array
            description: List of metrics and variants
            items { "$ref": "#/definitions/metric_variants" }
        }
    }
}
get_task_single_value_metrics {
    "2.20" {
        description: Get single value metrics for the passed tasks
        request {
            type: object
            required: [tasks]
            properties {
                tasks {
                    description: "List of task Task IDs"
                    type: array
                    items {
                        type: string
                        description: "Task ID"
                    }
                }
            }
        }
        response {"$ref": "#/definitions/single_value_metrics_response"}
    }
    "2.22": ${get_task_single_value_metrics."2.20"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
    "2.28": ${get_task_single_value_metrics."2.22"} {
        request.properties.metrics {
            type: array
            description: List of metrics and variants
            items { "$ref": "#/definitions/metric_variants" }
        }
    }
}
get_task_latest_scalar_values {
    "2.1" {
        description: "Get the tasks's latest scalar values"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
            }
        }
        response {
            type: object
            properties {
                metrics {
                    type: array
                    items {
                        type: object
                        properties {
                            name {
                                type: string
                                description: "Metric name"
                            }
                            variants {
                                type: array
                                items {
                                    type: object
                                    properties {
                                        name {
                                            type: string
                                            description: "Variant name"
                                        }
                                        last_value {
                                            type: number
                                            description: "Last reported value"
                                        }
                                        last_100_value {
                                            type: number
                                            description: "Average of 100 last reported values"
                                        }

                                    }
                                }
                            }
                        }
                    }
                 }
             }
        }
    }
}
get_scalar_metrics_and_variants {
    "2.1" {
        description: get task scalar metrics and variants
        request {
            type: object
            required: [ task ]
            properties {
                task {
                    description: task ID
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                metrics {
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.22": ${get_scalar_metrics_and_variants."2.1"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
get_scalar_metric_data {
    "2.1" {
        description: "get scalar metric data for task"
        request {
            type: object
            properties {
                task {
                    type: string
                    description: task ID
                }
                metric {
                    type: string
                    description: type of metric
                }
                scroll_id {
                    type: string
                    description: "Scroll ID of previous call (used for getting more results)"
                }
            }
        }
        response {
            type: object
            properties {
                events {
                    description: "task scalar metric events"
                    type: array
                    items {
                        type: object
                    }
                }
                returned {
                    type: integer
                    description: "amount of events returned"
                }
                total {
                    type: integer
                    description: "amount of events in task"
                }
                scroll_id {
                    type: string
                    description: "Scroll ID for getting more results"
                }
            }
        }
    }
    "2.16": ${get_scalar_metric_data."2.1"} {
        request.properties.no_scroll {
            description: If true then no scroll is created. Suitable for one time calls
            type: boolean
            default: false
        }
    }
    "2.22": ${get_scalar_metric_data."2.16"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
scalar_metrics_iter_raw {
    "2.16" {
        description: "Get raw data for a specific metric variants in the task"
        request {
            type: object
            required: [
                task, metric
            ]
            properties {
                task {
                    type: string
                    description: "Task ID"
                }
                metric {
                    description: "Metric and variants for which to return data points"
                    "$ref": "#/definitions/metric_variants"
                }
                key {
                    description: """Array of x axis to return. Supported values:
                    iter - iteration number
                    timestamp - event timestamp as milliseconds since epoch
                    """
                    "$ref": "#/definitions/scalar_key_enum"
                }
                batch_size {
                    description: "The number of data points to return for this call. Optional, the default value is 10000. Maximum batch size is 200000"
                    type: integer
                    default: 10000
                }
                count_total {
                    description: "Count the total number of data points. If false, total number of data points is not counted and null is returned"
                    type: boolean
                    default: false
                }
                scroll_id {
                    description: "Optional Scroll ID. Use to get more data points following a previous call"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                variants {
                    description: "Raw data points for each variant"
                    type: object
                    additionalProperties: true
                }
                total {
                    description: "Total data points count. If count_total is false, null is returned"
                    type: integer
                }
                returned {
                    description: "Number of data points returned in this call. If 0 results were returned, no more results are avilable"
                    type: integer
                }
                scroll_id {
                    description: "Scroll ID. Use to get more data points when calling this endpoint again"
                    type: string
                }
            }
        }
    }
    "2.22": ${scalar_metrics_iter_raw."2.16"} {
        request.properties.model_events {
            type: boolean
            description: If set then the retrieving model events. Otherwise task events
            default: false
        }
    }
}
clear_scroll {
    "2.18" {
        description: "Clear an open Scroll ID"
        request {
            type: object
            required: [
                scroll_id
            ]
            properties {
                scroll_id {
                    description: "Scroll ID as returned by previous events service calls"
                    type: string
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}

clear_task_log {
    "2.19" {
        description: Remove old logs from task
        request {
            type: object
            required: [task]
            properties {
                task {
                    description: Task ID
                    type: string
                }
                allow_locked {
                    type: boolean
                    description: Allow deleting events even if the task is locked
                    default: false
                }
                threshold_sec {
                    description: The amount of seconds ago to retain the log records. The older log records will be deleted. If not passed or 0 then all the log records for the task will be deleted
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: The number of deleted log records
                    type: integer
                }
            }
        }
    }
    "2.30": ${clear_task_log."2.19"} {
        request.properties {
            include_metrics {
                type: array
                description: If passed then only events for these metrics are deleted
                items: {type: string}
            }
            exclude_metrics {
                type: array
                description: If passed then events for these metrics are retained
                items: {type: string}
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/login.conf">
_description: """This service provides an administrator management interface to the company's users login information."""

_default {
    internal: false
    allow_roles: ["system", "root", "admin"]
}

supported_modes {
    authorize: null
    "2.9" {
        description:  """ Return supported login modes."""
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                basic {
                    type: object
                    properties {
                        enabled {
                            description: "Basic aothentication (fixed users mode) mode enabled"
                            type: boolean
                        }
                        guest {
                            type: object
                            properties {
                                enabled {
                                    description: "Basic aothentication guest mode enabled"
                                    type: boolean
                                }
                                name {
                                    description: "Guest name"
                                    type: string
                                }
                                username {
                                    description: "Guest username"
                                    type: string
                                }
                                password {
                                    description: "Guest password"
                                    type: string
                                }
                            }
                        }
                    }
                }
                sso {
                    description: "SSO authentication providers"
                    type: object
                    additionalProperties {
                        description: "Provider redirect URL"
                        type: string
                    }
                }
                sso_providers {
                    description: "The list of SSO authentication providers"
                    type: array
                    items {
                        type: object
                        additionalProperties: true
                    }
                }
                server_errors {
                    description: "Server initialization errors"
                    type: object
                    properties {
                        missed_es_upgrade {
                            description: "Indicate that Elasticsearch database was not upgraded from version 5"
                            type: boolean
                        }
                        es_connection_error {
                            description: "Indicate an error communicating to Elasticsearch"
                            type: boolean
                        }
                    }
                }
                authenticated {
                    description: "Is user authenticated"
                    type: boolean
                }
            }
        }
    }
}

logout {
    authorize: null
    allow_roles = [ "*" ]
    "2.13" {
        description: """ Logout (including SSO, if used)) """
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}
</file>

<file path="apiserver/schema/services/models.conf">
_description: """This service provides a management interface for models (results of training tasks) stored in the system."""
_definitions {
    include "_tasks_common.conf"
    model {
        type: object
        properties {
            id {
                description: "Model id"
                type: string
            }
            name {
                description: "Model name"
                type: string
            }
            user {
                description: "Associated user id"
                type: string
            }
            company {
                description: "Company id"
                type: string
            }
            created {
                description: "Model creation time"
                type: string
                format: "date-time"
            }
            last_update {
                description: "Model last update time"
                type: string
                format: "date-time"
            }
            task {
                description: "Task ID of task in which the model was created"
                type: string
            }
            parent {
                description: "Parent model ID"
                type: string
            }
            project {
                description: "Associated project ID"
                type: string
            }
            comment {
                description: "Model comment"
                type: string
            }
            tags {
                type: array
                description: "User-defined tags"
                items { type: string }
            }
            system_tags {
                type: array
                description: "System tags. This field is reserved for system use, please don't use it."
                items { type: string }
            }
            framework {
                description: "Framework on which the model is based. Should be identical to the framework of the task which created the model"
                type: string
            }
            design {
                description: "Json object representing the model design. Should be identical to the network design of the task which created the model"
                type: object
                additionalProperties: true
            }
            labels {
                description: "Json object representing the ids of the labels in the model. The keys are the layers' names and the values are the ids."
                type: object
                additionalProperties { type: integer }
            }
            uri {
                description: "URI for the model, pointing to the destination storage."
                type: string
            }
            ready {
                description: "Indication if the model is final and can be used by other tasks"
                type: boolean
            }
            ui_cache {
                description: "UI cache for this model"
                type: object
                additionalProperties: true
            }
            metadata {
                description: "Model metadata"
                type: object
                additionalProperties {
                    "$ref": "#/definitions/metadata_item"
                }
            }
            last_iteration {
                description: "Last iteration reported for this model"
                type: integer
            }
            last_metrics {
                description: "Last metric variants (hash to events), one for each metric hash"
                type: object
                additionalProperties {
                    "$ref": "#/definitions/last_metrics_variants"
                }
            }
            stats {
                description: "Model statistics"
                type: object
                properties {
                    labels_count {
                        description: Number of the model labels
                        type: integer
                    }
                }
            }
        }
    }
    published_task_item {
        description: "Result of publishing of the model's associated task (if exists). Returned only if the task was published successfully as part of the model publishing."
        type: object
        properties {
            id {
                description: "Task id"
                type: string
            }
            data {
                description: "Data returned from the task publishing operation."
                type: object
                properties {
                    updated {
                        description: "Number of tasks updated (0 or 1)"
                        type: integer
                        enum: [ 0, 1 ]
                    }
                    fields {
                        description: "Updated fields names and values"
                        type: object
                        additionalProperties: true
                    }
                }
            }
        }
    }
}

get_by_id {
    "2.1" {
        description: "Gets model information"
        request {
            type: object
            required: [ model ]
            properties {
                model {
                    description: "Model id"
                    type: string
                }
            }
        }

        response {
            type: object
            properties {
                model {
                    description: "Model info"
                    "$ref": "#/definitions/model"
                }
            }
        }
    }
}

get_by_task_id {
    "2.1" {
        description: "Gets model information"
        request {
            type: object
            properties {
                task {
                    description: "Task id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                model {
                    description: "Model info"
                    "$ref": "#/definitions/model"
                }
            }
        }
    }
}
get_by_id_ex {
    internal: true
    "2.11": ${get_all_ex."2.1"}
}
get_all_ex {
    internal: true
    "2.1": ${get_all."2.1"}
    "2.13": ${get_all_ex."2.1"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and project field is set then models from the subprojects are searched too"
                    type: boolean
                    default: false
                }
            }
        }
    }
    "2.15": ${get_all_ex."2.13"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all_ex"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of models to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all_ex to retrieve more data"
        }
    }
    "2.20": ${get_all_ex."2.15"} {
        request.properties.include_stats {
            description: "If true, include models statistic in response"
            type: boolean
            default: false
        }
    }
    "2.23": ${get_all_ex."2.20"} {
        request.properties {
            allow_public {
                description: "Allow public models to be returned in the results"
                type: boolean
                default: true
            }
        }
    }
    "2.27": ${get_all_ex."2.23"} {
        request.properties {
            filters {
                type: object
                additionalProperties: ${_definitions.field_filter}
            }
        }
    }
}
get_all {
    "2.1" {
        description: "Get all models"
        request {
            type: object
            properties {
                name {
                    description: "Get only models whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                user {
                    description: "List of user IDs used to filter results by the model's creating user"
                    type: array
                    items { type: string }
                }
                ready {
                    description: "Indication whether to retrieve only models that are marked ready If not supplied returns both ready and not-ready projects."
                    type: boolean
                }
                tags {
                    description: "User-defined tags list used to filter results. Prepend '-' to tag name to indicate exclusion"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list used to filter results. Prepend '-' to system tag name to indicate exclusion"
                    type: array
                    items { type: string }
                }
                only_fields {
                    description: "List of model field names (if applicable, nesting is supported using '.'). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
                page {
                    description: "Page number, returns a specific page out of the resulting list of models"
                    type: integer
                    minimum: 0
                }
                page_size {
                    description: "Page size, specifies the number of results returned in each page (last page may contain fewer results)"
                    type: integer
                    minimum: 1
                }
                project {
                    description: "List of associated project IDs"
                    type: array
                    items { type: string }
                }
                order_by {
                    description: "List of field names to order by. When search_text is used, '@text_score' can be used as a field representing the text score of returned documents. Use '-' prefix to specify descending order. Optional, recommended when using page"
                    type: array
                    items { type: string }
                }
                task {
                    description: "List of associated task IDs"
                    type: array
                    items { type: string }
                }
                id {
                    description: "List of model IDs"
                    type: array
                    items { type: string }
                }
                search_text {
                    description: "Free text search query"
                    type: string
                }
                framework {
                    description: "List of frameworks"
                    type: array
                    items { type: string }
                }
                uri {
                    description: "List of model URIs"
                    type: array
                    items { type: string }
                }
                last_update {
                    description: "List of last_update constraint strings (utcformat, epoch) with an optional prefix modifier (\>, \>=, \<, \<=)"
                    type: array
                    items {
                        type: string
                        pattern: "^(>=|>|<=|<)?.*$"
                    }
                }
                _all_ {
                    description: "Multi-field pattern condition (all fields match pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
                _any_ {
                    description: "Multi-field pattern condition (any field matches pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
            }
        }
        response {
            type: object
            properties {
                models: {
                    description: "Models list"
                    type: array
                    items { "$ref": "#/definitions/model" }
                }
            }
        }
    }
    "2.15": ${get_all."2.1"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of models to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all to retrieve more data"
        }
    }
    "2.26": ${get_all."2.15"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and project field is set then models from the subprojects are searched too"
                    type: boolean
                    default: false
                }
            }
        }
    }
}
get_frameworks {
    "2.8" {
        description: "Get the list of frameworks used in the company models"
        request {
            type: object
            properties {
                projects {
                    description: "The list of projects which models will be analyzed. If not passed or empty then all the company and public models will be analyzed"
                    type: array
                    items: {type: string}
                }
            }
        }
        response {
            type: object
            properties {
                frameworks {
                    description: "Unique list of the frameworks used in the company models"
                    type: array
                    items: {type: string}
                }
            }
        }
    }
}
update_for_task {
    "2.1" {
        description: "Create or update a new model for a task"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "Task id"
                    type: string
                }
                uri {
                    description: "URI for the model. Exactly one of uri or override_model_id is a required."
                    type: string
                }
                name {
                    description: "Model name Unique within the company."
                    type: string
                }
                comment {
                    description: "Model comment"
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                override_model_id {
                    description: "Override model ID. If provided, this model is updated in the task. Exactly one of override_model_id or uri is required."
                    type: string
                }
                iteration {
                    description: "Iteration (used to update task statistics)"
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "ID of the model"
                    type: string
                }
                created {
                    description: "Was the model created"
                    type: boolean
                }
                updated {
                    description: "Number of models updated (0 or 1)"
                    type: integer
                }
                fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
}
create {
    "2.1" {
        description: "Create a new model not associated with a task"
        request {
            type: object
            required: [
                uri
                name
            ]
            properties {
                uri {
                    description: "URI for the model"
                    type: string
                }
                name {
                    description: "Model name Unique within the company."
                    type: string
                }
                comment {
                    description: "Model comment"
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                framework {
                    description: "Framework on which the model is based. Case insensitive. Should be identical to the framework of the task which created the model."
                    type: string
                }
                design {
                    description: "Json[d] object representing the model design. Should be identical to the network design of the task which created the model"
                    type: object
                    additionalProperties: true
                }
                labels {
                    description: "Json object"
                    type: object
                    additionalProperties { type: integer }
                }
                ready {
                    description: "Indication if the model is final and can be used by other tasks. Default is false."
                    type: boolean
                    default: false
                }
                public {
                    description: "Create a public model Default is false."
                    type: boolean
                    default: false
                }
                project {
                    description: "Project to which to model belongs"
                    type: string
                }
                parent {
                    description: "Parent model"
                    type: string
                }
                task {
                    description: "Associated task ID"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "ID of the model"
                    type: string
                }
                created {
                    description: "Was the model created"
                    type: boolean
                }
            }
        }
    }
    "2.13": ${create."2.1"} {
        metadata {
            description: "Model metadata"
            type: object
            additionalProperties {
                "$ref": "#/definitions/metadata_item"
            }
        }
    }
}
edit {
    "2.1" {
        description: "Edit an existing model"
        request {
            type: object
            required: [
                model
            ]
            properties {
                model {
                    description: "Model ID"
                    type: string
                }
                uri {
                    description: "URI for the model"
                    type: string
                }
                name {
                    description: "Model name Unique within the company."
                    type: string
                }
                comment {
                    description: "Model comment"
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                framework {
                    description: "Framework on which the model is based. Case insensitive. Should be identical to the framework of the task which created the model."
                    type: string
                }
                design {
                    description: "Json[d] object representing the model design. Should be identical to the network design of the task which created the model"
                    type: object
                    additionalProperties: true
                }
                labels {
                    description: "Json object"
                    type: object
                    additionalProperties { type: integer }
                }
                ready {
                    description: "Indication if the model is final and can be used by other tasks"
                    type: boolean
                }
                project {
                    description: "Project to which to model belongs"
                    type: string
                }
                parent {
                    description: "Parent model"
                    type: string
                }
                task {
                    description: "Associated task ID"
                    type: string
                }
                iteration {
                    description: "Iteration (used to update task statistics)"
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
                fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.13": ${edit."2.1"} {
        metadata {
            description: "Model metadata"
            type: object
            additionalProperties {
                "$ref": "#/definitions/metadata_item"
            }
        }
    }
}
update {
    "2.1" {
        description: "Update a model"
        request {
            type: object
            required: [ model ]
            properties {
                model {
                    description: "Model id"
                    type: string
                }
                name {
                    description: "Model name Unique within the company."
                    type: string
                }
                comment {
                    description: "Model comment"
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                ready {
                    description: "Indication if the model is final and can be used by other tasks Default is false."
                    type: boolean
                    default: false
                }
                created {
                    description: "Model creation time (UTC) "
                    type: string
                    format: "date-time"
                }
                ui_cache {
                    description: "UI cache for this model"
                    type: object
                    additionalProperties: true
                }
                project {
                    description: "Project to which to model belongs"
                    type: string
                }
                task {
                    description: "Associated task ID"
                    type: "string"
                }
                iteration {
                    description: "Iteration (used to update task statistics if an associated task is reported)"
                    type: integer
                }

            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
                fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.13": ${update."2.1"} {
        metadata {
            description: "Model metadata"
            type: object
            additionalProperties {
                "$ref": "#/definitions/metadata_item"
            }
        }
    }
}
publish_many {
    "2.13": ${_definitions.batch_operation} {
        description: Publish models
        request {
            properties {
                ids.description: "IDs of the models to publish"
                force_publish_task {
                    description: "Publish the associated tasks (if exist) even if they are not in the 'stopped' state. Optional, the default value is False."
                    type: boolean
                }
                publish_tasks {
                    description: "Indicates that the associated tasks (if exist) should be published. Optional, the default value is True."
                    type: boolean
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.updated {
                    description: "Indicates whether the model was updated"
                    type: boolean
                }
                succeeded.items.properties.published_task: ${_definitions.published_task_item}
            }
        }
    }
}
set_ready {
    "2.1" {
        description: "Set the model ready flag to True. If the model is an output model of a task then try to publish the task."
        request {
            type: object
            required: [ model ]
            properties {
                model {
                    description: "Model id"
                    type: string
                }
                force_publish_task {
                    description: "Publish the associated task (if exists) even if it is not in the 'stopped' state. Optional, the default value is False."
                    type: boolean
                }
                publish_task {
                    description: "Indicates that the associated task (if exists) should be published. Optional, the default value is True."
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
                published_task: ${_definitions.published_task_item}
            }
        }
    }
}
archive_many {
    "2.13": ${_definitions.batch_operation} {
        description: Archive models
        request {
            properties {
                ids.description: "IDs of the models to archive"
            }
        }
        response {
            properties {
                succeeded.items.properties.archived {
                    description: "Indicates whether the model was archived"
                    type: boolean
                }
            }
        }
    }
}
unarchive_many {
    "2.13": ${_definitions.batch_operation} {
        description: Unarchive models
        request {
            properties {
                ids.description: "IDs of the models to unarchive"
            }
        }
        response {
            properties {
                succeeded.items.properties.unarchived {
                    description: "Indicates whether the model was unarchived"
                    type: boolean
                }
            }
        }
    }
}
delete_many {
    "2.13": ${_definitions.batch_operation} {
        description: Delete models
        request {
            properties {
                ids.description: "IDs of the models to delete"
                force {
                    description: "Force. Required if there are tasks that use the model as an execution model, or if the model's creating task is published."
                    type: boolean
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.deleted {
                    description: "Indicates whether the model was deleted"
                    type: boolean
                }
                succeeded.items.properties.url {
                    description: "The url of the model file"
                    type: string
                }
            }
        }
    }
}
delete {
    "2.1" {
        description: "Delete a model."
        request {
            required: [
                model
            ]
            type: object
            properties {
                model {
                    description: "Model ID"
                    type: string
                }
                force {
                    description: """Force. Required if there are tasks that use the model as an execution model, or if the model's creating task is published.
                    """
                    type: boolean
                }

            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Indicates whether the model was deleted"
                    type: boolean
                }

            }
        }
    }
    "2.13": ${delete."2.1"} {
        response {
            properties {
                url {
                    description: "The url of the model file"
                    type: string
                }
            }
        }
    }
}

make_public {
    "2.9" {
        description: """Convert company models to public"""
        request {
            type: object
            properties {
                ids {
                    description: "Ids of the models to convert"
                    type: array
                    items { type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated"
                    type: integer
                }
            }
        }
    }
}

make_private {
    "2.9" {
        description: """Convert public models to private"""
        request {
            type: object
            properties {
                ids {
                    description: "Ids of the models to convert. Only the models originated by the company can be converted"
                    type: array
                    items { type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated"
                    type: integer
                }
            }
        }
    }
}

move {
    "2.12" {
        description: "Move models to a project"
        request {
            type: object
            required: [ids]
            properties {
                ids {
                    description: "Models to move"
                    type: array
                    items { type: string }
                }
                project {
                    description: "Target project ID. If not provided, `project_name` must be provided. Use null for the root project"
                    type: string
                }
                project_name {
                    description: "Target project name. If provided and a project with this name does not exist, a new project will be created. If not provided, `project` must be provided."
                    type: string
                }
            }
        }
        response {
            type: object
            additionalProperties: true
        }
    }
}
add_or_update_metadata {
    "2.13" {
        description: "Add or update model metadata"
        request {
            type: object
            required: [model, metadata]
            properties {
                model {
                    description: "ID of the model"
                    type: string
                }
                metadata {
                    type: array
                    description: "Metadata items to add or update"
                    items {"$ref": "#/definitions/metadata_item"}
                }
                replace_metadata {
                    description: "If set then the all the metadata items will be replaced with the provided ones. Otherwise only the provided metadata items will be updated or added"
                    type: boolean
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
delete_metadata {
    "2.13" {
        description: "Delete metadata from model"
        request {
            type: object
            required: [ model, keys ]
            properties {
                model {
                    description: "ID of the model"
                    type: string
                }
                keys {
                    description: "The list of metadata keys to delete"
                    type: array
                    items {type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of models updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
update_tags {
    "2.27" {
        description: Add or remove tags from multiple models
        request {
            type: object
            properties {
                ids {
                    type: array
                    description: IDs of the models to update
                    items {type: string}
                }
                add_tags {
                    type: array
                    description: User tags to add
                    items {type: string}
                }
                remove_tags {
                    type: array
                    description: User tags to remove
                    items {type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    type: integer
                    description: The number of updated models
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/organization.conf">
_description: "This service provides organization level operations"
_definitions {
    value_mapping {
        type: object
        required: [key, value]
        properties {
            key {
                description: Original value
                type: object
            }
            value {
                description: Translated value
                type: object
            }
        }
    }
    field_mapping {
        type: object
        required: [field]
        properties {
            field {
                description: The source field name as specified in the only_fields
                type: string
            }
            name {
                description: The column name in the exported csv file
                type: string
            }
            values {
                type: array
                items { "$ref": "#/definitions/value_mapping"}
            }
        }
    }
    usages {
        type: object
        properties {
            total {
                type: array
                items {
                    type: object
                    properties {
                        name {type: string}
                        id {type: string}
                        duration {
                            type: integer
                            description: Tasks running time in seconds
                        }
                        cpu_usage {
                            type: number
                            description: Tasks running task multiplied by cpu resource
                        }
                        gpu_usage {
                            type: number
                            description: Tasks running task multiplied by gpu resource
                        }
                        cpu_artificial_weights {
                            description: Indicate whether artificial cpu resource weights were used
                            type: boolean
                        }
                        gpu_artificial_weights {
                            description: Indicate whether artificial gpu resource weights were used
                            type: boolean
                        }
                    }
                }
            }
            series {
                type: array
                items {
                    type: object
                    properties {
                        name {type: string}
                        id {type: string}
                        dates {
                            type: array
                            description: Date timestamp in seconds
                            items: {type: number}
                        }
                        duration {
                            type: array
                            description: Tasks running time in seconds
                            items: {type: number}
                        }
                        cpu_usage {
                            type: array
                            description: Tasks running task multiplied by cpu resource
                            items: {type: number}
                        }
                        gpu_usage {
                            type: array
                            description: Tasks running task multiplied by gpu resource
                            items: {type: number}
                        }
                        cpu_usage_artifical_weights {
                            description: Indicate whether artificial cpu resource weights were used
                            type: boolean
                        }
                        gpu_usage_artifical_weights {
                            description: Indicate whether artificial gpu resource weights were used
                            type: boolean
                        }
                    }
                }
            }
        }
    }
}
get_tags {
    "2.8" {
        description: "Get all the user and system tags used for the company tasks and models"
        request {
            type: object
            properties {
                include_system {
                    description: "If set to 'true' then the list of the system tags is also returned. The default value is 'false'"
                    type: boolean
                    default: false
                }
                filter {
                    description: "Filter on entities to collect tags from"
                    type: object
                    properties {
                        tags {
                            description: "The list of tag values to filter by. Use 'null' value to specify empty tags. Use '__Snot' value to specify that the following value should be excluded"
                            type: array
                            items {type: string}
                        }
                        system_tags {
                            description: "The list of system tag values to filter by. Use 'null' value to specify empty system tags. Use '__Snot' value to specify that the following value should be excluded"
                            type: array
                            items {type: string}
                        }
                    }
                }
            }
        }
        response {
            type: object
            properties {
                tags {
                    description: "The list of unique tag values"
                    type: array
                    items {type: string}
                }
                system_tags {
                    description: "The list of unique system tag values. Returned only if 'include_system' is set to 'true' in the request"
                    type: array
                    items {type: string}
                }
            }
        }
    }
}
get_user_companies {
    "2.12" {
        description: "Get details for all companies associated with the current user"
        request {
            type: object
            properties {}
            additionalProperties: false
        }
        response {
            type: object
            properties {
                companies {
                    description: "List of company information entries. First company is the user's own company"
                    type: array
                    items {
                        type: object
                        properties {
                            id {
                                description: "Company ID"
                                type: string
                            }
                            name {
                                description: "Company name"
                                type: string
                            }
                            allocated {
                                description: "Number of users allocated for company"
                                type: integer
                            }
                            owners {
                                description: "Company owners"
                                type: array
                                items {
                                    type: object
                                    properties {
                                        id {
                                            description: "User ID"
                                            type: string
                                        }
                                        name {
                                            description: "User Name"
                                            type: string
                                        }
                                        avatar {
                                            description: "User avatar (URL or base64-encoded data)"
                                            type: string
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
get_entities_count {
    "2.20": {
        description: "Get counts for the company entities according to the passed search criteria"
        request {
            type: object
            properties {
                projects {
                    type: object
                    additionalProperties: true
                    description: Search criteria for projects
                }
                tasks {
                    type: object
                    additionalProperties: true
                    description: Search criteria for experiments
                }
                models {
                    type: object
                    additionalProperties: true
                    description: Search criteria for models
                }
                pipelines {
                    type: object
                    additionalProperties: true
                    description: Search criteria for pipelines
                }
                pipeline_runs {
                    type: object
                    additionalProperties: true
                    description: Search criteria for pipeline runs
                }
                datasets {
                    type: object
                    additionalProperties: true
                    description: Search criteria for datasets
                }
                dataset_versions {
                    type: object
                    additionalProperties: true
                    description: Search criteria for dataset versions
                }
            }
        }
        response {
            type: object
            properties {
                projects {
                    type: integer
                    description: The number of projects matching the criteria
                }
                tasks {
                    type: integer
                    description: The number of experiments matching the criteria
                }
                models {
                    type: integer
                    description: The number of models matching the criteria
                }
                pipelines {
                    type: integer
                    description: The number of pipelines matching the criteria
                }
                pipeline_runs {
                    type: integer
                    description: The number of pipeline runs matching the criteria
                }
                datasets {
                    type: integer
                    description: The number of datasets matching the criteria
                }
                dataset_versions {
                    type: integer
                    description: The number of dataset versions matching the criteria
                }
            }
        }
    }
    "2.22": ${get_entities_count."2.20"} {
        request.properties {
            search_hidden {
                description: "If set to 'true' then hidden projects and tasks are included in the search results"
                type: boolean
                default: false
            }
            active_users {
                description: "The list of users that were active in the project. If passes then the resulting projects are filtered to the ones that have tasks created by these users"
                type: array
                items: {type: string}
            }
        }
    }
    "2.23": ${get_entities_count."2.22"} {
        request.properties {
            reports {
                type: object
                additionalProperties: true
                description: Search criteria for reports
            }
            allow_public {
                description: "Allow public entities to be counted in the results"
                type: boolean
                default: true
            }
        }
        response.properties {
            reports {
                type: integer
                description: The number of reports matching the criteria
            }
        }
    }
    "2.33": ${get_entities_count."2.23"} {
        request.properties {
            limit {
                type: integer
                description: If specified then items only up to this limit are counted
            }
        }
    }
    "2.34": ${get_entities_count."2.33"} {
        response.properties {
            errors {
                type: object
                properties {
                    projects { type: string }
                    tasks { type: string }
                    models { type: string }
                    pipelines { type: string }
                    pipeline_runs { type: string }
                    datasets { type: string }
                    dataset_versions { type: string }
                    reports { type: string}
                }
            }
        }
    }
}
prepare_download_for_get_all {
    "2.26": {
        description: Prepares download from get_all_ex parameters
        request {
            type: object
            required: [ entity_type,  only_fields, field_mappings]
            properties {
                only_fields {
                    description: "List of task field names (nesting is supported using '.', e.g. execution.model_labels). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items {type: string}
                }
                allow_public {
                    description: "Allow public entities to be returned in the results"
                    type: boolean
                    default: true
                }
                search_hidden {
                    description: "If set to 'true' then hidden entities are included in the search results"
                    type: boolean
                    default: false
                }
                entity_type {
                    description: "The type of the entity to retrieve"
                    type: string
                    enum: [
                        task
                        model
                    ]
                }
                field_mappings {
                    description: The name and value mappings for the exported fields. The fields that are not in the mappings will not be exported
                    type: array
                    items { "$ref": "#/definitions/field_mapping"}
                }
            }
        }
        response {
            type: object
            properties {
                prepare_id {
                    description: "Prepare ID (use when calling 'download_for_get_all')"
                    type: string
                }
            }
        }
    }
}
download_for_get_all {
    "2.26": {
        description: Generates a file for the download
        request {
            type: object
            required: [ prepare_id ]
            properties {
                prepare_id {
                    description: "Call ID returned by a call to prepare_download_for_get_all"
                    type: string
                }
            }
        }
        response {
            type: string
        }
    }
}
get_project_usages {
    "2.34": {
        description: "Get projects usage breakdown"
        request {
            type: object
            required: [projects, from_date, to_date]
            properties {
                projects {
                    description: Project IDs
                    type: array
                    items: {type: string}
                }
                from_date {
                    description: "Starting time (in seconds from epoch) for collecting statistics"
                    type: string
                    format: "date-time"
                }
                to_date {
                    description: "Ending time (in seconds from epoch) for collecting statistics. Pass null for now"
                    type: string
                    format: "date-time"
                }
                include_development {
                    type: boolean
                    description: "If set to 'true' then development tasks are also returned"
                    default: false
                }
                breakdown_keys {
                    type: array
                    items {
                        type: string
                        enum: [project, user, queue]
                    }
                }
                usage_fields {
                    type: array
                    items {
                        type: string
                        enum: [duration, cpu_usage, gpu_usage]
                    }
                }
            }
        }
        response {
            type: object
            properties {
                projects: {
                    description: Usages by project
                    "$ref": "#/definitions/usages"
                }
                users: {
                    description: Usages by user
                    "$ref": "#/definitions/usages"
                }
                queues: {
                    description: Usages by user
                    "$ref": "#/definitions/usages"
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/pipelines.conf">
_description: "Provides a management API for pipelines in the system."
_definitions {
    include "_common.conf"
}
delete_runs {
    "2.26": ${_definitions.batch_operation} {
        description: Delete pipeline runs
        request {
            required: [ids, project]
            properties {
                ids.description: "IDs of the pipeline runs to delete. Should be the ids of pipeline controller tasks"
                project {
                    description: "Pipeline project ids. When deleting at least one run should be left"
                    type: string
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.deleted {
                    description: "Indicates whether the task was deleted"
                    type: boolean
                }
                succeeded.items.properties.updated_children {
                    description: "Number of child tasks whose parent property was updated"
                    type: integer
                }
                succeeded.items.properties.updated_models {
                    description: "Number of models whose task property was updated"
                    type: integer
                }
                succeeded.items.properties.deleted_models {
                    description: "Number of deleted output models"
                    type: integer
                }
            }
        }
    }
}
start_pipeline {
    "2.17" {
        description: "Start a pipeline"
        request {
            type: object
            required: [ task ]
            properties {
                task {
                    description: "ID of the task on which the pipeline will be based"
                    type: string
                }
                queue {
                    description: "Queue ID in which the created pipeline task will be enqueued"
                    type: string
                }
                args {
                    description: "Task arguments, name/value to be placed in the hyperparameters Args section"
                    type: array
                    items {
                        type: object
                        properties {
                            name: { type: string }
                            value: { type: string }
                        }
                    }
                }
            }
        }
        response {
            type: object
            properties {
                pipeline {
                    description: "ID of the new pipeline task"
                    type: string
                }
                enqueued {
                    description: "True if the task was successfuly enqueued"
                    type: boolean
                }
            }
        }
    }
    "2.28": ${start_pipeline."2.17"} {
        request.properties.verify_watched_queue {
            description: If passed then check wheter there are any workers watiching the queue
            type: boolean
            default: false
        }
        response.properties.queue_watched {
            description: Returns true if there are workers or autscalers working with the queue
            type: boolean
        }
    }
}
</file>

<file path="apiserver/schema/services/projects.conf">
_description: "Provides support for defining Projects containing Tasks, Models and Dataset Versions."
_definitions {
    include "_common.conf"
    project {
        type: object
        properties {
            id {
                description: "Project id"
                type: string
            }
            name {
                description: "Project name"
                type: string
            }
            basename {
                description: "Project base name"
                type: string
            }
            description {
                description: "Project description"
                type: string
            }
            user {
                description: "Associated user id"
                type: string
            }
            company {
                description: "Company id"
                type: string
            }
            created {
                description: "Creation time"
                type: string
                format: "date-time"
            }
            tags {
                description: "User-defined tags"
                type: array
                items { type: string }
            }
            system_tags {
                description: "System tags. This field is reserved for system use, please don't use it."
                type: array
                items { type: string }
            }
            default_output_destination {
                description: "The default output destination URL for new tasks under this project"
                type: string
            }
            last_update {
                description: "Last project update time. Reflects the last time the project metadata was changed or a task in this project has changed status"
                type: string
                format: "date-time"
            }
        }
    }
    stats_datasets {
        type: object
        properties {
            count {
                description: Number of datasets
                type: integer
            }
            tags {
                description: Dataset tags
                type: array
                items {type: string}
            }
        }
    }
    stats_status_count {
        type: object
        properties {
            total_runtime {
                description: "Total run time of all tasks in project (in seconds)"
                type: integer
            }
            total_tasks {
                description: "Number of tasks"
                type: integer
            }
            completed_tasks_24h {
                description: "Number of tasks completed in the last 24 hours"
                type: integer
            }
            last_task_run {
                description: "The most recent started time of a task"
                type: integer
            }
            status_count {
                description: "Status counts"
                type: object
                properties {
                    created {
                        description: "Number of 'created' tasks in project"
                        type: integer
                    }
                    completed {
                        description: "Number of 'completed' tasks in project"
                        type: integer
                    }
                    queued {
                        description: "Number of 'queued' tasks in project"
                        type: integer
                    }
                    in_progress {
                        description: "Number of 'in_progress' tasks in project"
                        type: integer
                    }
                    stopped {
                        description: "Number of 'stopped' tasks in project"
                        type: integer
                    }
                    published {
                        description: "Number of 'published' tasks in project"
                        type: integer
                    }
                    closed {
                        description: "Number of 'closed' tasks in project"
                        type: integer
                    }
                    failed {
                        description: "Number of 'failed' tasks in project"
                        type: integer
                    }
                    unknown {
                        description: "Number of 'unknown' tasks in project"
                        type: integer
                    }
                }
            }
        }
    }
    stats  {
        type: object
        properties {
            active {
                description: "Stats for active tasks"
                "$ref": "#/definitions/stats_status_count"
            }
            archived {
                description: "Stats for archived tasks"
                "$ref": "#/definitions/stats_status_count"
            }
            datasets {
                description: "Stats for childrent datasets"
                "$ref": "#/definitions/stats_datasets"
            }
        }
    }
    projects_get_all_response_single {
        // copy-paste from project definition
        type: object
        properties {
            id {
                description: "Project id"
                type: string
            }
            name {
                description: "Project name"
                type: string
            }
            basename {
                description: "Project base name"
                type: string
            }
            description {
                description: "Project description"
                type: string
            }
            user {
                description: "Associated user id"
                type: string
            }
            company {
                description: "Company id"
                type: string
            }
            created {
                description: "Creation time"
                type: string
                format: "date-time"
            }
            tags {
                description: "User-defined tags"
                type: array
                items { type: string }
            }
            system_tags {
                description: "System tags. This field is reserved for system use, please don't use it."
                type: array
                items { type: string }
            }
            default_output_destination {
                description: "The default output destination URL for new tasks under this project"
                type: string
            }
            last_update {
                description: "Last project update time. Reflects the last time the project metadata was changed or a task in this project has changed status"
                type: string
                format: "date-time"
            }
            // extra properties
            hidden {
                description: "Returned if the search_hidden flag was specified in the get_all_ex call and the project is hidden"
                type: boolean
            }
            stats {
                description: "Additional project stats"
                "$ref": "#/definitions/stats"
            }
            sub_projects {
                description: "The list of sub projects"
                type: array
                items {
                    type: object
                    properties {
                        id {
                            description: "Subproject ID"
                            type: string
                        }
                        name {
                            description: "Subproject name"
                            type: string
                        }
                    }
                }
            }
            own_datasets {
                description: "The amount of datasets/hyperdatasers under this project (without children projects). Returned if 'check_own_contents' flag is set in the request and children_type is set to 'dataset' or 'hyperdataset'"
                type: integer
            }
            own_tasks {
                description: "The amount of tasks under this project (without children projects). Returned if 'check_own_contents' flag is set in the request"
                type: integer
            }
            own_models {
                description: "The amount of models under this project (without children projects). Returned if 'check_own_contents' flag is set in the request"
                type: integer
            }
            dataset_stats {
                description: Project dataset statistics
                type: object
                properties {
                    file_count {
                        type: integer
                        description: The number of files stored in the dataset
                    }
                    total_size {
                        type: integer
                        description: The total dataset size in bytes
                    }
                }
            }
        }
    }
    metric_variant_result {
        type: object
        properties {
            metric {
                description: "Metric name"
                type: string
            }
            metric_hash {
                description: """Metric name hash. Used instead of the metric name when categorizing
                last metrics events in task objects."""
                type: string
            }
            variant {
                description: "Variant name"
                type: string
            }
            variant_hash {
                description: """Variant name hash. Used instead of the variant name when categorizing
                last metrics events in task objects."""
                type: string
            }
        }
    }
    tags_request {
        type: object
        properties {
            include_system {
                description: "If set to 'true' then the list of the system tags is also returned. The default value is 'false'"
                type: boolean
                default: false
            }
            projects {
                description: "The list of projects under which the tags are searched. If not passed or empty then all the projects are searched"
                type: array
                items { type: string }
            }
            filter {
                description: "Filter on entities to collect tags from"
                type: object
                properties {
                    tags {
                        description: "The list of tag values to filter by. Use 'null' value to specify empty tags. Use '__Snot' value to specify that the following value should be excluded"
                        type: array
                        items {type: string}
                    }
                    system_tags {
                        description: "The list of system tag values to filter by. Use 'null' value to specify empty system tags. Use '__Snot' value to specify that the following value should be excluded"
                        type: array
                        items {type: string}
                    }
                }
            }
        }
    }
    tags_response {
        type: object
        properties {
            tags {
                description: "The list of unique tag values"
                type: array
                items {type: string}
            }
            system_tags {
                description: "The list of unique system tag values. Returned only if 'include_system' is set to 'true' in the request"
                type: array
                items {type: string}
            }
        }
    }
    urls {
        type: object
        properties {
            model_urls {
                type: array
                items {type: string}
            }
            event_urls {
                type: array
                items {type: string}
            }
            artifact_urls {
                type: array
                items {type: string}
            }
        }
    }
}

create {
    "2.1" {
        description: "Create a new project"
        request {
            type: object
            required :[name]
            properties {
                name {
                    description: "Project name Unique within the company."
                    type: string
                }
                description {
                    description: "Project description."
                    type: string
                }
                tags {
                    description: "User-defined tags"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                default_output_destination  {
                    description: "The default output destination URL for new tasks under this project"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "Project id"
                    type: string
                }
            }
        }
    }
}
get_by_id {
    "2.1" {
        description: ""
        request {
            type: object
            required: [ project ]
            properties {
                project {
                    description: "Project id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                project {
                    description: "Project info"
                    "$ref": "#/definitions/project"
                }
            }
        }
    }
}
get_all {
    "2.1" {
        description: "Get all the company's projects and all public projects"
        request {
            type: object
            properties {
                id {
                    description: "List of IDs to filter by"
                    type: array
                    items { type: string }
                }
                name {
                    description: "Get only projects whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                basename {
                    description: "Project base name"
                    type: string
                }
                description {
                    description: "Get only projects whose description matches this pattern (python regular expression syntax)"
                    type: string
                }
                tags {
                    description: "User-defined tags list used to filter results. Prepend '-' to tag name to indicate exclusion"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list used to filter results. Prepend '-' to system tag name to indicate exclusion"
                    type: array
                    items { type: string }
                }
                order_by {
                    description: "List of field names to order by. When search_text is used, '@text_score' can be used as a field representing the text score of returned documents. Use '-' prefix to specify descending order. Optional, recommended when using page"
                    type: array
                    items { type: string }
                }
                page {
                    description: "Page number, returns a specific page out of the resulting list of projects"
                    type: integer
                    minimum: 0
                }
                page_size {
                    description: "Page size, specifies the number of results returned in each page (last page may contain fewer results)"
                    type: integer
                    minimum: 1
                }
                search_text {
                    description: "Free text search query"
                    type: string
                }
                only_fields {
                    description: "List of document's field names (nesting is supported using '.', e.g. execution.model_labels). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
                _all_ {
                    description: "Multi-field pattern condition (all fields match pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
                _any_ {
                    description: "Multi-field pattern condition (any field matches pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
            }
        }
        response {
            type: object
            properties {
                projects {
                    description: "Projects list"
                    type: array
                    items { "$ref": "#/definitions/projects_get_all_response_single" }
                }
            }
        }
    }
    "2.13": ${get_all."2.1"} {
        request {
            properties {
                shallow_search {
                    description: "If set to 'true' then the search with the specified criteria is performed among top level projects only (or if parents specified, among the direct children of the these parents). Otherwise the search is performed among all the company projects (or among all of the descendants of the specified parents)."
                    type: boolean
                    default: false
                }
            }
        }
    }
    "2.14": ${get_all."2.13"} {
        request.properties.search_hidden {
            description: "If set to 'true' then hidden projects are included in the search results"
            type: boolean
            default: false
        }
    }
    "2.15": ${get_all."2.14"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all_ex"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of projects to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all_ex to retrieve more data"
        }
    }
}
get_all_ex {
    internal: true
    "2.1": ${get_all."2.1"} {
        request {
            properties {
                include_stats {
                    description: "If true, include project statistic in response."
                    type: boolean
                    default: false
                }
                stats_for_state {
                    description: "Obsolete! Statistics is calculated always for the 'active' state"
                    type: string
                    enum: [ active, archived ]
                    default: active
                }
                non_public {
                    description: "Return only non-public projects"
                    type: boolean
                    default: false
                }
            }
        }
    }
    "2.13": ${get_all_ex."2.1"} {
        request {
            properties {
                active_users {
                    description: "The list of users that were active in the project. If passes then the resulting projects are filtered to the ones that have tasks created by these users"
                    type: array
                    items: {type: string}
                }
                shallow_search {
                    description: "If set to 'true' then the search with the specified criteria is performed among top level projects only (or if parents specified, among the direct children of the these parents). Otherwise the search is performed among all the company projects (or among all of the descendants of the specified parents)."
                    type: boolean
                    default: false
                }
                check_own_contents {
                    description: "If set to 'true' and project ids are passed to the query then for these projects their own tasks and models are counted"
                    type: boolean
                    default: false
                }
            }
        }
    }
    "2.14": ${get_all_ex."2.13"} {
        request.properties.search_hidden {
            description: "If set to 'true' then hidden projects are included in the search results"
            type: boolean
            default: false
        }
    }
    "2.15": ${get_all_ex."2.14"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of projects to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all to retrieve more data"
        }
    }
    "2.16": ${get_all_ex."2.15"} {
        request.properties.stats_with_children {
            description: "If include_stats flag is set then this flag contols whether the child projects tasks are taken into statistics or not"
            type: boolean
            default: true
        }
    }
    "2.17": ${get_all_ex."2.16"} {
        request.properties.include_stats_filter {
            description: The filter for selecting entities that participate in statistics calculation. For each task field that you want to filter on pass the list of allowed values. Prepend the value with '-' to exclude
            type: object
            additionalProperties: true
        }
    }
    "2.20": ${get_all_ex."2.17"} {
        request.properties.include_dataset_stats {
            description: "If true, include project dataset statistic in response"
            type: boolean
            default: false
        }
    }
    "2.23": ${get_all_ex."2.20"} {
        request.properties {
            allow_public {
                description: "Allow public projects to be returned in the results"
                type: boolean
                default: true
            }
        }
    }
    "2.24": ${get_all_ex."2.23"} {
        request.properties.children_type {
            description: If specified that only the projects under which the entities of this type can be found will be returned
            type: string
            enum: [pipeline, report, dataset]
        }
    }
    "2.25": ${get_all_ex."2.24"} {
        request.properties.children_tags {
            description: "The list of tag values to filter children by. Takes effect only if children_type is set. Use 'null' value to specify empty tags. Use '__Snot' value to specify that the following value should be excluded"
            type: array
            items {type: string}
        }
    }
    "2.27": ${get_all_ex."2.25"} {
        request.properties {
            filters {
                type: object
                additionalProperties: ${_definitions.field_filter}
            }
            children_tags_filter: ${_definitions.field_filter}
        }
    }
}
update {
    "2.1" {
        description: "Update project information"
        request {
            type: object
            required: [ project ]
            properties {
                project {
                    description: "Project id"
                    type: string
                }
                name {
                    description: "Project name. Unique within the company."
                    type: string
                }
                description {
                    description: "Project description"
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                default_output_destination {
                    description: "The default output destination URL for new tasks under this project"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of projects updated (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
                fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
}
move {
    "2.13" {
        description: "Moves a project and all of its subprojects under the different location"
        request {
            type: object
            required: [project]
            properties {
                project {
                    description: "Project id"
                    type: string
                }
                new_location {
                    description: "The name location for the project"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                moved {
                    description: "The number of projects moved"
                    type: integer
                }
            }
        }
    }
}
merge {
    "2.13" {
        description: "Moves all the source project's contents to the destination project and remove the source project"
        request {
            type: object
            required: [project]
            properties {
                project {
                    description: "Project id"
                    type: string
                }
                destination_project {
                    description: "The ID of the destination project"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                moved_entities {
                    description: "The number of tasks and models moved from the merged project into the destination"
                    type: integer
                }
                moved_projects {
                    description: "The number of child projects moved from the merged project into the destination"
                    type: integer
                }
            }
        }
    }
}
validate_delete {
    "2.14" {
        description: "Validates that the project existis and can be deleted"
        request {
            type: object
            required: [ project ]
            properties {
                project {
                    description: "Project ID"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                tasks {
                    description: "The total number of tasks under the project and all its children"
                    type: integer
                }
                non_archived_tasks {
                    description: "The total number of non-archived tasks under the project and all its children"
                    type: integer
                }
                models {
                    description: "The total number of models under the project and all its children"
                    type: integer
                }
                non_archived_models {
                    description: "The total number of non-archived models under the project and all its children"
                    type: integer
                }
            }
        }
    }
    "2.26": ${validate_delete."2.14"} {
        response.properties {
            reports {
                description: "The total number of reports under the project and all its children"
                type: integer
            }
            non_archived_reports {
                description: "The total number of non-archived reports under the project and all its children"
                type: integer
            }
            pipelines {
                description: "The total number of pipelines with active controllers under the project and all its children"
                type: integer
            }
            datasets {
                description: "The total number of non-empty datasets under the project and all its children"
                type: integer
            }
        }
    }
}
delete {
    "2.1" {
        description: "Deletes a project"
        request {
            type: object
            required: [ project ]
            properties {
                project {
                    description: "Project ID"
                    type: string
                }
                force {
                    description: """If not true, fails if project has tasks.
                    If true, and project has tasks, they will be unassigned"""
                    type: boolean
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Number of projects deleted (0 or 1)"
                    type: integer
                }
                disassociated_tasks {
                    description: "Number of tasks disassociated from the deleted project"
                    type: integer
                }
            }
        }
    }
    "2.13": ${delete."2.1"} {
        request {
            properties {
                delete_contents {
                    description: "If set to 'true' then the project tasks and models will be deleted. Otherwise their project property will be unassigned. Default value is 'false'"
                    type: boolean
                }
            }
        }
        response {
            properties {
                urls {
                    description: "The urls of the files that were uploaded by the project tasks and models. Returned if the 'delete_contents' was set to 'true'"
                    "$ref": "#/definitions/urls"
                }
                deleted_models {
                    description: "Number of models deleted"
                    type: integer
                }
                deleted_tasks {
                    description: "Number of tasks deleted"
                    type: integer
                }
            }
        }
    }
    "2.26": ${delete."2.13"} {
        request.properties.delete_external_artifacts {
            description: "If set to 'true' then BE will try to delete the extenal artifacts associated with the project tasks and models from the fileserver (if configured to do so)"
            type: boolean
            default: true
        }
    }
}
get_unique_metric_variants {
    "2.1" {
        description: """Get all metric/variant pairs reported for tasks in a specific project.
        If no project is specified, metrics/variant paris reported for all tasks will be returned.
        If the project does not exist, an empty list will be returned."""
        request {
            type: object
            properties {
                project {
                    description: "Project ID"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                metrics {
                    description: "A list of metric variants reported for tasks in this project"
                    type: array
                    items { "$ref": "#/definitions/metric_variant_result" }
                }
            }
        }
    }
    "2.13": ${get_unique_metric_variants."2.1"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and the project field is set then the result includes metrics/variants from the subproject tasks"
                    type: boolean
                    default: true
                }
            }
        }
    }
    "2.25": ${get_unique_metric_variants."2.13"} {
        request.properties.model_metrics {
            description: If set to true then bring unique metric and variant names from the project models otherwise from the project tasks
            type: boolean
            default: false
        }
    }
    "2.28": ${get_unique_metric_variants."2.25"} {
        request.properties.ids {
            description: IDs of the tasks or models to get metrics from
            type: array
            items {type: string}
        }
    }
}
get_hyperparam_values {
    "2.13" {
        description: """Get a list of distinct values for the chosen hyperparameter"""
        request {
            type: object
            required: [section, name]
            properties {
                projects {
                    description: "Project IDs"
                    type: array
                    items {type: string}
                }
                section {
                    description: "Hyperparameter section name"
                    type: string
                }
                name {
                    description: "Hyperparameter name"
                    type: string
                }
                allow_public {
                    description: "If set to 'true' then collect values from both company and public tasks otherwise company tasks only. The default is 'true'"
                    type: boolean
                }
                include_subprojects {
                    description: "If set to 'true' and the project field is set then the result includes hyper parameters values from the subproject tasks"
                    type: boolean
                    default: true
                }
            }
        }
        response {
            type: object
            properties {
                total {
                    description: "Total number of distinct parameter values"
                    type: integer
                }
                values {
                    description: "The list of the unique values for the parameter"
                    type: array
                    items {type: string}
                }
            }
        }
    }
    "2.26": ${get_hyperparam_values."2.13"} {
        request.properties {
            page {
                description: "Page number"
                default: 0
                type: integer
            }
            page_size {
                description: "Page size"
                default: 500
                type: integer
            }
        }
    }
    "2.27": ${get_hyperparam_values."2.26"} {
        request.properties.pattern {
            type: string
            description: The search pattern regex
        }
    }
}
get_hyper_parameters {
    "2.9" {
        description: """Get a list of all hyper parameter sections and names used in tasks within the given project."""
        request {
            type: object
            required: [project]
            properties {
                project {
                    description: "Project ID"
                    type: string
                }
                page {
                    description: "Page number"
                    default: 0
                    type: integer
                }
                page_size {
                    description: "Page size"
                    default: 500
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                parameters {
                    description: "A list of parameter sections and names"
                    type: array
                    items {type: object}
                }
                remaining {
                    description: "Remaining results"
                    type: integer
                }
                total {
                    description: "Total number of results"
                    type: integer
                }
            }
        }
    }
    "2.13": ${get_hyper_parameters."2.9"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and the project field is set then the result includes hyper parameters from the subproject tasks"
                    type: boolean
                    default: true
                }
            }
        }
    }
}
get_model_metadata_values {
    "2.17" {
        description: """Get a list of distinct values for the chosen model metadata key"""
        request {
            type: object
            required: [key]
            properties {
                projects {
                    description: "Project IDs"
                    type: array
                    items {type: string}
                }
                key {
                    description: "Metadata key"
                    type: string
                }
                allow_public {
                    description: "If set to 'true' then collect values from both company and public models otherwise company modeels only. The default is 'true'"
                    type: boolean
                }
                include_subprojects {
                    description: "If set to 'true' and the project field is set then the result includes metadata values from the subproject models"
                    type: boolean
                    default: true
                }
            }
        }
        response {
            type: object
            properties {
                total {
                    description: "Total number of distinct values"
                    type: integer
                }
                values {
                    description: "The list of the unique values"
                    type: array
                    items {type: string}
                }
            }
        }
    }
    "2.26": ${get_model_metadata_values."2.17"} {
        request.properties {
            page {
                description: "Page number"
                default: 0
                type: integer
            }
            page_size {
                description: "Page size"
                default: 500
                type: integer
            }
        }
    }
}
get_model_metadata_keys {
    "2.17" {
        description: """Get a list of all metadata keys used in models within the given project."""
        request {
            type: object
            required: [project]
            properties {
                project {
                    description: "Project ID"
                    type: string
                }
                include_subprojects {
                    description: "If set to 'true' and the project field is set then the result includes metadate keys from the subproject models"
                    type: boolean
                    default: true
                }

                page {
                    description: "Page number"
                    default: 0
                    type: integer
                }
                page_size {
                    description: "Page size"
                    default: 500
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                keys {
                    description: "A list of model keys"
                    type: array
                    items {type: string}
                }
                remaining {
                    description: "Remaining results"
                    type: integer
                }
                total {
                    description: "Total number of results"
                    type: integer
                }
            }
        }
    }
}
get_project_tags {
    "2.17" {
        description: "Get user and system tags used for the specified projects and their children"
        request = ${_definitions.tags_request}
        response = ${_definitions.tags_response}
    }
}
get_task_tags {
    "2.8" {
        description: "Get user and system tags used for the tasks under the specified projects"
        request = ${_definitions.tags_request}
        response = ${_definitions.tags_response}
    }
}
get_model_tags {
    "2.8" {
        description: "Get user and system tags used for the models under the specified projects"
        request = ${_definitions.tags_request}
        response = ${_definitions.tags_response}
    }
}

make_public {
    "2.9" {
        description: """Convert company projects to public"""
        request {
            type: object
            properties {
                ids {
                    description: "Ids of the projects to convert"
                    type: array
                    items { type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of projects updated"
                    type: integer
                }
            }
        }
    }
}

make_private {
    "2.9" {
        description: """Convert public projects to private"""
        request {
            type: object
            properties {
                ids {
                    description: "Ids of the projects to convert. Only the projects originated by the company can be converted"
                    type: array
                    items { type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of projects updated"
                    type: integer
                }
            }
        }
    }
}
get_task_parents {
    "2.12" {
        description: "Get unique parent tasks for the tasks in the specified projects"
        request {
            type: object
            properties {
                projects {
                    description: "The list of projects which task parents are retieved. If not passed or empty then all the projects are searched"
                    type: array
                    items { type: string }
                }
                tasks_state {
                    description: "Return parents for tasks in the specified state. If Null is provided, parents for all task states will be returned."
                    type: string
                    enum: [ active, archived ]
                    default: active
                }
            }
        }
        response {
            type: object
            properties {
                parents {
                    description: "The list of unique task parents sorted by their names"
                    type: array
                    items {
                        type: object
                        properties {
                            id {
                                description: "The ID of the parent task"
                                type: string
                            }
                            name {
                                description: "The name of the parent task"
                                type: string
                            }
                            project {
                                type: object
                                properties {
                                    id {
                                        description: "The ID of the parent task project"
                                        type: string
                                    }
                                    name {
                                        description: "The name of the parent task project"
                                        type: string
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    "2.13": ${get_task_parents."2.12"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and the projects field is not empty then the result includes tasks parents from the subproject tasks"
                    type: boolean
                    default: true
                }
            }
        }
    }
    "2.25": ${get_task_parents."2.13"} {
        request.properties.task_name {
            description: Task name pattern for the returned parent tasks
            type: string
        }
    }
}
get_user_names {
    "2.26" {
        description: "Get names and ids of the users who created child entitites under the passed projects"
        request {
            type: object
            properties {
                projects {
                    description: "The list of projects. If not passed or empty then all the projects are searched"
                    type: array
                    items { type: string }
                }
                include_subprojects {
                    description: "If set to 'true' and the projects field is not empty then the result includes user name from the subprojects children"
                    type: boolean
                    default: true
                }
                entity {
                    description: The type of the child entity to look for
                    type: string
                    enum: [task, model]
                    default: task
                }
            }
        }
        response {
            type: object
            properties {
                users {
                    description: "The list of users sorted by their names"
                    type: array
                    items {
                        type: object
                        properties {
                            id {
                                description: "The ID of the user"
                                type: string
                            }
                            name {
                                description: "The name of the user"
                                type: string
                            }
                        }
                    }
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/queues.conf">
_description: "Provides a management API for queues of tasks waiting to be executed by workers deployed anywhere (see Workers Service)."
_definitions {
    include "_common.conf"
    queue_metrics {
        type: object
        properties: {
            queue: {
                type: string
                description: "ID of the queue"
            }
            dates {
                type: array
                description: "List of timestamps (in seconds from epoch) in the acceding order. The timestamps are separated by the requested interval. Timestamps where no queue status change was recorded are omitted."
                items { type: integer }
            }
            avg_waiting_times {
                type: array
                description: "List of average waiting times for tasks in the queue. The points correspond to the timestamps in the dates list. If more than one value exists for the given interval then the maximum value is taken."
                items { type: number }
            }
            queue_lengths {
                type: array
                description: "List of tasks counts in the queue. The points correspond to the timestamps in the dates list. If more than one value exists for the given interval then the count that corresponds to the maximum average value is taken."
                items { type: integer }
            }
        }
    }
    entry {
        type: object
        properties: {
            task {
                description: "Queued task ID"
                type: string
            }
            added {
                description: "Time this entry was added to the queue"
                type: string
                format: "date-time"
            }
        }
    }
    queue {
        type: object
        properties: {
            id {
                description: "Queue id"
                type: string
            }
            name {
                description: "Queue name"
                type: string
            }
            display_name {
                description: "Display name"
                type: string
            }
            user {
                description: "Associated user id"
                type: string
            }
            company {
                description: "Company id"
                type: string
            }
            created {
                description: "Queue creation time"
                type: string
                format: "date-time"
            }
            tags {
                description: "User-defined tags"
                type: array
                items { type: string }
            }
            system_tags {
                description: "System tags. This field is reserved for system use, please don't use it."
                type: array
                items { type: string }
            }
            entries {
                description: "List of ordered queue entries"
                type: array
                items { "$ref": "#/definitions/entry" }
            }
            metadata {
                description: "Queue metadata"
                type: object
                additionalProperties {
                    "$ref": "#/definitions/metadata_item"
                }
            }
        }
    }
}

get_by_id {
    "2.4" {
        description: "Gets queue information"
        request {
            type: object
            required: [ queue ]
            properties {
                queue {
                    description: "Queue ID"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                queue {
                    description: "Queue info"
                    "$ref": "#/definitions/queue"
                }
            }
        }
    }
    "2.20": ${get_by_id."2.4"} {
        request.properties.max_task_entries {
            description: Max number of queue task entries to return
            type: integer
        }
    }
}
// typescript generation hack
get_all_ex {
    internal: true
    "2.4": ${get_all."2.4"}
    "2.15": ${get_all_ex."2.4"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all_ex"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of queues to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all_ex to retrieve more data"
        }
    }
    "2.20": ${get_all_ex."2.15"} {
        request.properties.max_task_entries {
            description: Max number of queue task entries to return
            type: integer
        }
    }
    "2.21": ${get_all_ex."2.20"} {
        request.properties.search_hidden {
            description: "If set to 'true' then hidden queues are included in the search results"
            type: boolean
            default: false
        }
    }
    "2.27": ${get_all_ex."2.21"} {
        request.properties {
            filters {
                type: object
                additionalProperties: ${_definitions.field_filter}
            }
        }
    }
}
get_all {
    "2.4" {
        description: "Get all queues"
        request {
            type: object
            properties {
                name {
                    description: "Get only queues whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                id {
                    description: "List of Queue IDs used to filter results"
                    type: array
                    items { type: string }
                }
                tags {
                    description: "User-defined tags list used to filter results. Prepend '-' to tag name to indicate exclusion"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list used to filter results. Prepend '-' to system tag name to indicate exclusion"
                    type: array
                    items { type: string }
                }
                page {
                    description: "Page number, returns a specific page out of the result list of results."
                    type: integer
                    minimum: 0
                }
                page_size {
                    description: "Page size, specifies the number of results returned in each page (last page may contain fewer results)"
                    type: integer
                    minimum: 1
                }
                order_by {
                    description: "List of field names to order by. When search_text is used, '@text_score' can be used as a field representing the text score of returned documents. Use '-' prefix to specify descending order. Optional, recommended when using page"
                    type: array
                    items { type: string }
                }
                search_text {
                    description: "Free text search query"
                    type: string
                }
                only_fields {
                    description: "List of document field names (nesting is supported using '.', e.g. execution.model_labels). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                queues {
                    description: "Queues list"
                    type: array
                    items { "$ref": "#/definitions/queue"}
                }
            }
        }
    }
    "2.15": ${get_all."2.4"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of queues to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all to retrieve more data"
        }
    }
    "2.20": ${get_all."2.15"} {
        request.properties.max_task_entries {
            description: Max number of queue task entries to return
            type: integer
        }
    }
    "2.21": ${get_all."2.20"} {
        request.properties.search_hidden {
            description: "If set to 'true' then hidden queues are included in the search results"
            type: boolean
            default: false
        }
    }
}
get_default {
    "2.4" {
        description: ""
        request {
            type: object
            properties {}
            additionalProperties: false
        }
        response {
            type: object
            properties {
                id {
                    description: "Queue id"
                    type: string
                }
                name {
                    description: "Queue name"
                    type: string
                }
            }
        }
    }
}
create {
    "2.4" {
        description: "Create a new queue"
        request {
            type: object
            required: [ name ]
            properties {
                name {
                    description: "Queue name Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "New queue ID"
                    type: string
                }
            }
        }
    }
    "2.13": ${create."2.4"} {
        request.properties.metadata {
            description: "Queue metadata"
            type: object
            additionalProperties {
                "$ref": "#/definitions/metadata_item"
            }
        }
    }
    "2.31": ${create."2.13"} {
        request.properties.display_name {
            description: "Display name"
            type: string
        }
    }
}
update {
    "2.4" {
        description: "Update queue information"
        request {
            type: object
            required: [ queue ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                name {
                    description: "Queue name Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of queues updated (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
                fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.13": ${update."2.4"} {
        request.properties.metadata {
            description: "Queue metadata"
            type: object
            additionalProperties {
                "$ref": "#/definitions/metadata_item"
            }
        }
    }
    "2.31": ${update."2.13"} {
        request.properties.display_name {
            description: "Display name"
            type: string
        }
    }
}
delete {
    "2.4" {
        description: "Deletes a queue. If the queue is not empty and force is not set to true, queue will not be deleted."
        request {
            type: object
            required: [ queue ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                force {
                    description: "Force delete of non-empty queue. Defaults to false"
                    type: boolean
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Number of queues deleted (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
}
add_task {
    "2.4" {
        description: "Adds a task entry to the queue."
        request {
            type: object
            required: [
                queue
                task
            ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                task {
                    description: "Task id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                added {
                    description: "Number of tasks added (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
    "2.31": ${add_task."2.4"} {
        request.properties.update_execution_queue {
            description: If set to false then the task 'execution.queue' is not updated
            type: boolean
            default: true
        }
    }
}
get_next_task {
    "2.4" {
        description: "Gets the next task from the top of the queue (FIFO). The task entry is removed from the queue."
        request {
            type: object
            required: [ queue ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                entry {
                    description: "Entry information"
                    "$ref": "#/definitions/entry"
                }
            }
        }
    }
    "2.14": ${get_next_task."2.4"} {
        request.properties.get_task_info {
            description: "If set then additional task info is returned"
            type: boolean
            default: false
        }
        response.properties.task_info {
            description: "Info about the returned task. Returned only if get_task_info is set to True"
            type: object
            properties {
                company {
                    description: Task company ID
                    type: string
                }
                user {
                    description: ID of the user who created the task
                    type: string
                }
            }
        }
    }
    "2.21": ${get_next_task."2.14"} {
        request.properties.task {
            description: Task company ID
            type: string
        }
    }
}
remove_task {
    "2.4" {
        description: "Removes a task entry from the queue."
        request {
            type: object
            required: [
                queue
                task
            ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                task {
                    description: "Task id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                removed {
                    description: "Number of tasks removed (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
    "2.31": ${remove_task."2.4"} {
        request.properties {
            update_task_status {
                type: boolean
                default: false
                description: If set to 'true' then change the removed task status to the one it had prior to enqueuing or 'created'
            }
        }
    }
}
clear_queue {
    "2.31" {
        description: Remove all tasks from the queue and change their statuses to what they were prior to enqueuing or 'created'
        request {
            type: object
            required: [queue]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                removed_tasks {
                    description: IDs of the removed tasks
                    type: array
                    items {type: string}
                }
            }
        }
    }
}
move_task_forward: {
    "2.4" {
        description: "Moves a task entry one step forward towards the top of the queue."
        request {
            type: object
            required: [
                queue
                task
            ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                task {
                    description: "Task id"
                    type: string
                }
                count {
                    description: "Number of positions in the queue to move the task forward relative to the current position. Optional, the default value is 1."
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                position {
                    description: "The new position of the task entry in the queue (index, -1 represents bottom of queue)"
                    type: integer
                }
            }
        }
    }
}
move_task_backward: {
    "2.4" {
        description: ""
        request {
            type: object
            required: [
                queue
                task
            ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                task {
                    description: "Task id"
                    type: string
                }
                count {
                    description: "Number of positions in the queue to move the task forward relative to the current position. Optional, the default value is 1."
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                position {
                    description: "The new position of the task entry in the queue (index, -1 represents bottom of queue)"
                    type: integer
                }
            }
        }
    }
}
move_task_to_front: {
    "2.4" {
        description: ""
        request {
            type: object
            required: [
                queue
                task
            ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                task {
                    description: "Task id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                position {
                    description: "The new position of the task entry in the queue (index, -1 represents bottom of queue)"
                    type: integer
                }
            }
        }
    }
}
move_task_to_back: {
    "2.4" {
        description: ""
        request {
            type: object
            required: [
                queue
                task
            ]
            properties {
                queue {
                    description: "Queue id"
                    type: string
                }
                task {
                    description: "Task id"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                position {
                    description: "The new position of the task entry in the queue (index, -1 represents bottom of queue)"
                    type: integer
                }
            }
        }
    }
}
get_queue_metrics : {
    "2.4" {
        description: "Returns metrics of the company queues. The metrics are avaraged in the specified interval."
        request {
            type: object
            required: [from_date, to_date, interval]
            properties: {
                from_date {
                    description: "Starting time (in seconds from epoch) for collecting metrics"
                    type: number
                }
                to_date {
                    description: "Ending time (in seconds from epoch) for collecting metrics"
                    type: number
                }
                interval {
                    description: "Time interval in seconds for a single metrics point. The minimal value is 1"
                    type: integer
                }
                queue_ids {
                    description: "List of queue ids to collect metrics for. If not provided or empty then all then average metrics across all the company queues will be returned."
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties: {
                queues {
                    type: array
                    description: "List of the requested queues with their metrics. If no queue ids were requested then 'all' queue is returned with the metrics averaged accross all the company queues."
                    items { "$ref": "#/definitions/queue_metrics" }
                }
            }
        }
    }
    "2.20": ${get_queue_metrics."2.4"} {
        request.properties.refresh {
            type: boolean
            default: false
            description: If set then the new queue metrics is taken
        }
    }
}
add_or_update_metadata {
    "2.13" {
        description: "Add or update queue metadata"
        request {
            type: object
            required: [queue, metadata]
            properties {
                queue {
                    description: "ID of the queue"
                    type: string
                }
                metadata {
                    type: array
                    description: "Metadata items to add or update"
                    items {"$ref": "#/definitions/metadata_item"}
                }
                replace_metadata {
                    description: "If set then the all the metadata items will be replaced with the provided ones. Otherwise only the provided metadata items will be updated or added"
                    type: boolean
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of queues updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
delete_metadata {
    "2.13" {
        description: "Delete metadata from queue"
        request {
            type: object
            required: [ queue, keys ]
            properties {
                queue {
                    description: "ID of the queue"
                    type: string
                }
                keys {
                    description: "The list of metadata keys to delete"
                    type: array
                    items {type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of queues updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
peek_task {
    "2.15" {
        description: "Peek the next task from a given queue"
        request {
            type: object
            required: [ queue ]
            properties {
                queue {
                    description: "ID of the queue"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
            }
        }
    }
}
get_num_entries {
    "2.15" {
        description: "Get the number of task entries in the given queue"
        request {
            type: object
            required: [ queue ]
            properties {
                queue {
                    description: "ID of the queue"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                num {
                    description: "Number of entries"
                    type: integer
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/README.md">
# Writing descriptions
There are two options for writing parameters descriptions. Mixing between the two
will result in output which is not Sphinx friendly.
Whatever you choose, lines are subject to wrapping.

- non-strict whitespace - Break the string however you like.
  Newlines and sequences of tabs/spaces are replaced by one space.
  Example:
    ```
    get_all {
        "1.5" {
            description: """This will all appear
            as one long
            sentence.
            Break lines wherever you
            like.
            """
        }
    }
    ```
    Becomes:
    ```
    class GetAllRequest(...):
        """
        This will all appear as one long sentence. Break lines wherever you
        like.
        """
    ```
- strict whitespace - Single newlines will be replaced by spaces.
  Double newlines become a single newline WITH INDENTATION PRESERVED,
  so if uniform indentation is requried for all lines you MUST start new lines
  at the first column.
  Example:
    ```
    get_all {
        "1.5" {
            description: """
    Some general sentence.

    - separate lines must have double newlines between them

    - must begin at first column even though the "description" key is indented

    - you can use single newlines, the lines will be
      joined

        -- sub bullet: this line's leading spaces are preserved
    """
        }
    }
    ```
  Becomes:
    ```
    class GetAllRequest(...):
        """
        Some general sentence.
        - separate lines must have double newlines between them
        - must begin at first column even though the "description" key is indented
        - you can use single newlines, the lines will be joined
            -- sub bullet: this line's leading spaces are preserved
        """
    ```
</file>

<file path="apiserver/schema/services/reports.conf">
_description: "Provides a management API for reports in the system."
_definitions {
    include "_tasks_common.conf"
    include "_events_common.conf"
    update_response {
        type: object
        properties {
            updated {
                description: "Number of reports updated (0 or 1)"
                type: integer
                enum: [ 0, 1 ]
            }
            fields {
                description: "Updated fields names and values"
                type: object
                additionalProperties: true
            }
        }
    }
    report_status_enum {
        type: string
        enum: [
            created
            published
        ]
    }
    report {
        type: object
        properties {
            id {
                description: "Report id"
                type: string
            }
            name {
                description: "Report Name"
                type: string
            }
            user {
                description: "Associated user id"
                type: string
            }
            company {
                description: "Company ID"
                type: string
            }
            status {
                description: ""
                "$ref": "#/definitions/report_status_enum"
            }
            comment {
                description: "Free text comment"
                type: string
            }
            report {
                description: "Report template"
                type: string
            }
            report_assets {
                description: "List of the external report assets"
                type: array
                items { type: string }
            }
            created {
                description: "Report creation time (UTC) "
                type: string
                format: "date-time"
            }
            project {
                description: "Project ID of the project to which this report is assigned"
                type: string
            }
            tags {
                description: "User-defined tags list"
                type: array
                items { type: string }
            }
            system_tags {
                description: "System tags list. This field is reserved for system use, please don't use it."
                type: array
                items { type: string }
            }
            status_changed {
                description: "Last status change time"
                type: string
                format: "date-time"
            }
            status_message {
                description: "free text string representing info about the status"
                type: string
            }
            status_reason {
                description: "Reason for last status change"
                type: string
            }
            published {
                description: "Report publish time"
                type: string
                format: "date-time"
            }
            last_update {
                description: "Last time this report was created, edited, changed"
                type: string
                format: "date-time"
            }
        }
    }
}
create {
    "2.23" {
        description: "Create a new report"
        request {
            type: object
            required: [
                name
            ]
            properties {
                name {
                    description: "Report name. Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                comment {
                    description: "Free text comment "
                    type: string
                }
                report {
                    description: "Report template"
                    type: string
                }
                project {
                    description: "Project ID of the project to which this report is assigned Must exist[ab]"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "ID of the report"
                    type: string
                }
                project_id {
                    description: "ID of the project that the report belongs to"
                    type: string
                }
            }
        }
    }
    "2.24": ${create."2.23"} {
        request.properties.report_assets {
            description: "List of the external report assets"
            type: array
            items { type: string }
        }
    }
}
update {
    "2.23" {
        description: "Create a new report"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "The ID of the report task to update"
                    type: string
                }
                name {
                    description: "Report name. Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                comment {
                    description: "Free text comment "
                    type: string
                }
                report {
                    description: "Report template"
                    type: string
                }
            }
        }
        response: ${_definitions.update_response}
    }
    "2.24": ${update."2.23"} {
        request.properties.report_assets {
            description: "List of the external report assets"
            type: array
            items { type: string }
        }
    }
}
move {
    "2.23" {
        description: "Move reports to a project"
        request {
            type: object
            required: [task]
            properties {
                task {
                    description: "ID of the report to move"
                    type: string
                }
                project {
                    description: "Target project ID. If not provided, `project_name` must be provided. Use null for the root project"
                    type: string
                }
                project_name {
                    description: "Target project name. If provided and a project with this name does not exist, a new project will be created. If not provided, `project` must be provided."
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                project_id: {
                    description: The ID of the target project
                    type: string
                }
            }
        }
    }
}
publish {
    "2.23" {
        description: "Publish report"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "The ID of the report task to publish"
                    type: string
                }
                comment {
                    description: "The client message"
                    type: string
                }
            }
        }
        response: ${_definitions.update_response}
    }
}
archive {
    "2.23" {
        description: "Archive report"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "The ID of the report task to archive"
                    type: string
                }
                comment {
                    description: "The client message"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                archived {
                    description: "Number of reports archived (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
unarchive {
    "2.23" {
        description: "Unarchive report"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "The ID of the report task to unarchive"
                    type: string
                }
                comment {
                    description: "The client message"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                unarchived {
                    description: "Number of reports unarchived (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
//share {
//    "999.0" {
//        description: "Share or unshare report"
//        request {
//            type: object
//            required: [
//                task
//            ]
//            properties {
//                task {
//                    description: "The ID of the report task to share/unshare"
//                    type: string
//                }
//                share {
//                    description: "If set to 'true' then the report will be shared. Otherwise unshared."
//                    type: boolean
//                    default: true
//                }
//            }
//        }
//        response {
//            type: object
//            properties {
//                changed {
//                    description: "Number of changed reports (0 or 1)"
//                    type: integer
//                    enum: [0, 1]
//                }
//            }
//        }
//    }
//}
delete {
    "2.23" {
        description: "Delete report"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "The ID of the report task to delete"
                    type: string
                }
                force {
                    description: "If not set then published or unarchived reports cannot be deleted"
                    type: boolean
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Number of deleted reports (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
get_task_data {
    "2.23" {
        description: "Get the tasks data according the passed search criteria + requested events"
        request {
            type: object
            properties {
                id {
                    description: "List of IDs to filter by"
                    type: array
                    items { type: string }
                }
                name {
                    description: "Get only tasks whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                user {
                    description: "List of user IDs used to filter results by the task's creating user"
                    type: array
                    items { type: string }
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of tasks to retrieve"
                }
                order_by {
                    description: "List of field names to order by. When search_text is used, '@text_score' can be used as a field representing the text score of returned documents. Use '-' prefix to specify descending order. Optional, recommended when using page"
                    type: array
                    items { type: string }
                }
                type {
                    description: "List of task types. One or more of: 'import', 'annotation', 'training' or 'testing' (case insensitive)"
                    type: array
                    items { type: string }
                }
                tags {
                    description: "List of task user-defined tags. Use '-' prefix to exclude tags"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "List of task system tags. Use '-' prefix to exclude system tags"
                    type: array
                    items { type: string }
                }
                status {
                    description: "List of task status."
                    type: array
                    items { "$ref": "#/definitions/task_status_enum" }
                }
                project {
                    description: "List of project IDs"
                    type: array
                    items { type: string }
                }
                only_fields {
                    description: "List of task field names (nesting is supported using '.', e.g. execution.model_labels). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
                parent {
                    description: "Parent ID"
                    type: string
                }
                status_changed {
                    description: "List of status changed constraint strings (utcformat, epoch) with an optional prefix modifier (\>, \>=, \<, \<=)"
                    type: array
                    items {
                        type: string
                        pattern: "^(>=|>|<=|<)?.*$"
                    }
                }
                search_text {
                    description: "Free text search query"
                    type: string
                }
                allow_public {
                    description: "Allow public tasks to be returned in the results"
                    type: boolean
                    default: true
                }
                _all_ {
                    description: "Multi-field pattern condition (all fields match pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
                _any_ {
                    description: "Multi-field pattern condition (any field matches pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
                "input.view.entries.dataset" {
                    description: "List of input dataset IDs"
                    type: array
                    items { type: string }
                }
                "input.view.entries.version" {
                    description: "List of input dataset version IDs"
                    type: array
                    items { type: string }
                }
                search_hidden {
                    description: "If set to 'true' then hidden tasks are included in the search results"
                    type: boolean
                    default: false
                }
                include_subprojects {
                    description: "If set to 'true' and project field is set then tasks from the subprojects are searched too"
                    type: boolean
                    default: false
                }
                plots {
                    type: object
                    properties {
                        iters {
                            type: integer
                            description: "Max number of latest iterations for which to return plots"
                        }
                        metrics {
                            type: array
                            description: List of metrics and variants
                            items { "$ref": "#/definitions/metric_variants" }
                        }
                    }
                }
                debug_images {
                    type: object
                    properties {
                        iters {
                            type: integer
                            description: "Max number of latest iterations for which to return debug images"
                        }
                        metrics {
                            type: array
                            description: List of metrics and variants
                            items { "$ref": "#/definitions/metric_variants" }
                        }
                    }
                }
                scalar_metrics_iter_histogram {
                    type: object
                    properties {
                        samples {
                            description: "The amount of histogram points to return (0 to return all the points). Optional, the default value is 6000."
                            type: integer
                        }
                        key {
                            description: """
                            Histogram x axis to use:
                            iter - iteration number
                            iso_time - event time as ISO formatted string
                            timestamp - event timestamp as milliseconds since epoch
                            """
                            "$ref": "#/definitions/scalar_key_enum"
                        }
                        metrics {
                            type: array
                            description: List of metrics and variants
                            items { "$ref": "#/definitions/metric_variants" }
                        }
                    }
                }
            }
        }
        response {
            type: object
            properties {
                tasks {
                    description: "List of tasks"
                    type: array
                    items { "$ref": "#/definitions/task" }
                }
                plots {
                    type: object
                    description: "Plots mapped by metric, variant, task and iteration"
                    additionalProperties: true
                }
                debug_images {
                    type: array
                    description: "Debug image events grouped by tasks and iterations"
                    items {"$ref": "#/definitions/debug_images_response_task_metrics"}
                }
                scalar_metrics_iter_histogram {
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.25": ${get_task_data."2.23"} {
        request.properties {
            model_events {
                type: boolean
                description: If set then the retrieving model events. Otherwise task events
                default: false
            }
            single_value_metrics {
                type: object
                description: If passed then task single value metrics are returned
                additionalProperties: false
            }
        }
        response.properties.single_value_metrics {
            type: array
            description: Single value metrics grouped by task
            items {"$ref": "#/definitions/single_value_task_metrics"}
        }
    }
    "2.26": ${get_task_data."2.25"} {
        request.properties.plots.properties.last_iters_per_task_metric {
            type: boolean
            description: If set to 'true' and iters passed then last iterations for each task metrics are retrieved. Otherwise last iterations for the whole task are retrieved
            default: true
        }
    }
}
get_all_ex {
    "2.23" {
        description: "Get all the company's and public report tasks"
        request {
            type: object
            properties {
                id {
                    description: "List of IDs to filter by"
                    type: array
                    items { type: string }
                }
                name {
                    description: "Get only reports whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                user {
                    description: "List of user IDs used to filter results by the reports's creating user"
                    type: array
                    items { type: string }
                }
                page {
                    description: "Page number, returns a specific page out of the resulting list of reports"
                    type: integer
                    minimum: 0
                }
                page_size {
                    description: "Page size, specifies the number of results returned in each page (last page may contain fewer results)"
                    type: integer
                    minimum: 1
                }
                order_by {
                    description: "List of field names to order by. When search_text is used, '@text_score' can be used as a field representing the text score of returned documents. Use '-' prefix to specify descending order. Optional, recommended when using page"
                    type: array
                    items { type: string }
                }
                tags {
                    description: "List of report user-defined tags. Use '-' prefix to exclude tags"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "List of report system tags. Use '-' prefix to exclude system tags"
                    type: array
                    items { type: string }
                }
                status {
                    description: "List of report status."
                    type: array
                    items { "$ref": "#/definitions/report_status_enum" }
                }
                project {
                    description: "List of project IDs"
                    type: array
                    items { type: string }
                }
                only_fields {
                    description: "List of report field names (nesting is supported using '.'). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
                status_changed {
                    description: "List of status changed constraint strings (utcformat, epoch) with an optional prefix modifier (\>, \>=, \<, \<=)"
                    type: array
                    items {
                        type: string
                        pattern: "^(>=|>|<=|<)?.*$"
                    }
                }
                search_text {
                    description: "Free text search query"
                    type: string
                }
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of tasks to retrieve"
                }
                allow_public {
                    description: "Allow public reports to be returned in the results"
                    type: boolean
                    default: true
                }
                _all_ {
                    description: "Multi-field pattern condition (all fields match pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
                _any_ {
                    description: "Multi-field pattern condition (any field matches pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
            }
        }
        response {
            type: object
            properties {
                tasks {
                    description: "List of report tasks"
                    type: array
                    items { "$ref": "#/definitions/report" }
                }
                scroll_id {
                    type: string
                    description: "Scroll ID that can be used with the next calls to get_all to retrieve more data"
                }
            }
        }
    }
    "2.26": ${get_all_ex."2.23"} {
        request.properties.include_subprojects {
            description: "If set to 'true' and project field is set then reports from the subprojects are searched too"
            type: boolean
            default: false
        }
    }
    "2.27": ${get_all_ex."2.26"} {
        request.properties {
            filters {
                type: object
                additionalProperties: ${_definitions.field_filter}
            }
        }
    }
}
get_tags {
    "2.23" {
        description: "Get all the user tags used for the company reports"
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                tags {
                    description: "The list of unique tag values"
                    type: array
                    items {type: string}
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/server.conf">
_description: "server utilities"
_default {
    internal: true
    allow_roles: ["root", "system"]
}
get_stats {
    "2.1" {
        description: "Get the server collected statistics."
        request {
            type: object
            properties {
                interval {
                    description: "The period for statistics collection in seconds."
                    type: integer
                }
            }
        }
        response {
            type: object
            properties: {
            }
        }
    }
}
config {
    "2.1" {
        description: "Get server configuration. Secure section is not returned."
        request {
            type: object
            properties {
                path {
                    description: "Path of config value. Defaults to root"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
            }
        }
    }
}
info {
    allow_roles = [ "*" ]
    "2.1" {
        description: "Get server information, including version and build number"
        request {
            type: object
            properties {
            }
        }
        response {
            type: object
            properties {
                version {
                    description: "Version string"
                    type: string
                }
                build {
                    description: "Build number"
                    type: string
                }
                commit {
                    description: "VCS commit number"
                    type: string
                }
            }
        }
    }
    "2.8": ${info."2.1"} {
        response {
            type: object
            properties {
                uid {
                    description: "Server UID"
                    type: string
                }
                api_version {
                    description: "Max API version supported"
                    type: string
                }
            }
        }
    }
    "2.12": ${info."2.8"} {
        response {
            type: object
            properties {
                api_version {
                    description: "Max API version supported"
                    type: string
                }
            }
        }
    }
}
endpoints {
    "2.1" {
        description: "Show available endpoints"
        request {
            type: object
            properties {
            }
        }
        response {
            type: object
            properties {
            }
        }
    }
}
report_stats_option {
    allow_roles = [ "*" ]
    "2.4" {
        description: "Get or set the report statistics option per-company"
        request {
            type: object
            properties {
                enabled {
                    description: "If provided, sets the report statistics option (true/false)"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                supported {
                    description: "Is this feature supported by the server"
                    type: boolean
                }
                enabled {
                    description: "Returns the current report stats option value"
                    type: boolean
                }
                enabled_time {
                    description: "If enabled, returns the time at which option was enabled"
                    type: string
                    format: date-time
                }
                enabled_version {
                    description: "If enabled, returns the server version at the time option was enabled"
                    type: string
                }
                enabled_user {
                    description: "If enabled, returns Id of the user who enabled the option"
                    type: string
                }
                current_version {
                    description: "Returns the current server version"
                    type: string
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/serving.conf">
_description: "Serving apis"
_definitions {
    include "_workers_common.conf"
    reference_item {
        type: object
        required = [type, value]
        properties {
            type {
                description: The type of the reference item
                type: string
                enum: [app_id, app_instance, model, task, url]
            }
            value {
                description: The reference item value
                type: string
            }
        }
    }
    reference {
        description: Array of reference items provided by the container instance. Can contain multiple reference items with the same type
        type: array
        items: ${_definitions.reference_item}
    }
    serving_model_report {
        type: object
        required: [container_id, endpoint_name, model_name]
        properties {
            container_id {
                type: string
                description: Container ID. Should uniquely identify a specific container instance
            }
            endpoint_name {
                type: string
                description: Endpoint name
            }
            endpoint_url {
                type: string
                description: Endpoint URL
            }
            model_name {
                type: string
                description: Model name
            }
            model_source {
                type: string
                description: Model source
            }
            model_version {
                type: string
                description: Model version
            }
            preprocess_artifact {
                type: string
                description: Preprocess Artifact
            }
            input_type {
                type: string
                description: Input type
            }
            input_size {
                type: string
                description: Input size
            }
            reference: ${_definitions.reference}
        }
    }
    endpoint_stats {
        type: object
        properties {
            endpoint {
                type: string
                description: Endpoint name
            }
            model {
                type: string
                description: Model name
            }
            url {
                type: string
                description: Model url
            }
            instances {
                type: integer
                description: The number of model serving instances
            }
            uptime_sec {
                type: integer
                description: Max of model instance uptime in seconds
            }
            requests {
                type: integer
                description: Total requests processed by model instances
            }
            requests_min {
                type: number
                description: Average of request rate of model instances per minute
            }
            latency_ms {
                type: integer
                description: Average of latency of model instances in ms
            }
            last_update {
                type: string
                format: "date-time"
                description: The latest time when one of the model instances was updated
            }
        }
    }
    container_instance_stats {
        type: object
        properties {
            id {
                type: string
                description: Container ID
            }
            uptime_sec {
                type: integer
                description: Uptime in seconds
            }
            requests {
                type: integer
                description: Number of requests
            }
            requests_min {
                type: number
                description: Average requests per minute
            }
            latency_ms {
                type: integer
                description: Average request latency in ms
            }
            last_update {
                type: string
                format: "date-time"
                description: The latest time when the container instance sent update
            }
            cpu_count {
                type: integer
                description: CPU Count
            }
            gpu_count {
                type: integer
                description: GPU Count
            }
            reference: ${_definitions.reference}

        }
    }
    serving_model_info {
        type: object
        properties {
            endpoint {
                type: string
                description: Endpoint name
            }
            model {
                type: string
                description: Model name
            }
            url {
                type: string
                description: Model url
            }
            model_source {
                type: string
                description: Model source
            }
            model_version {
                type: string
                description: Model version
            }
            preprocess_artifact {
                type: string
                description: Preprocess Artifact
            }
            input_type {
                type: string
                description: Input type
            }
            input_size {
                type: string
                description: Input size
            }
        }
    }
    container_info: ${_definitions.serving_model_info} {
        properties {
            id {
                type: string
                description: Container ID
            }
            uptime_sec {
                type: integer
                description: Model instance uptime in seconds
            }
            last_update {
                type: string
                format: "date-time"
                description: The latest time when the container instance sent update
            }
            age_sec {
                type: integer
                description: Amount of seconds since the container registration
            }
        }
    }
    metrics_history_series {
        type: object
        properties {
            title {
                type: string
                description: "The title of the series"
            }
            dates {
                type: array
                description: "List of timestamps (in seconds from epoch) in the acceding order. The timestamps are separated by the requested interval."
                items {type: integer}
            }
            values {
                type: array
                description: "List of values corresponding to the timestamps in the dates list."
                items {type: number}
            }
        }
    }
}
register_container {
    "2.31" {
        description: Register container
        request = ${_definitions.serving_model_report} {
            properties {
                timeout {
                    description: "Registration timeout in seconds. If timeout seconds have passed since the service container last call to register or status_report, the container is automatically removed from the list of registered containers."
                    type: integer
                    default: 600
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}
unregister_container {
    "2.31" {
        description: Unregister container
        request {
            type: object
            required: [container_id]
            properties {
                container_id {
                    type: string
                    description: Container ID
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}
container_status_report {
    "2.31" {
        description: Container status report
        request = ${_definitions.serving_model_report} {
            properties {
                uptime_sec {
                    type: integer
                    description: Uptime in seconds
                }
                requests_num {
                    type: integer
                    description: Number of requests
                }
                requests_min {
                    type: number
                    description: Average requests per minute
                }
                latency_ms {
                    type: integer
                    description: Average request latency in ms
                }
                machine_stats {
                    description: "The machine statistics"
                    "$ref": "#/definitions/machine_stats"
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}
get_endpoints {
    "2.31" {
        description: Get all the registered endpoints
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                endpoints {
                    type: array
                    items { "$ref": "#/definitions/endpoint_stats" }
                }
            }
        }
    }
}
get_loading_instances {
    "2.31" {
        description: "Get loading instances (enpoint_url not set yet)"
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                instances {
                    type: array
                    items { "$ref": "#/definitions/container_info" }
                }
            }
        }
    }
}
get_endpoint_details {
    "2.31" {
        description: Get endpoint details
        request {
            type: object
            required: [endpoint_url]
            properties {
                endpoint_url {
                    type: string
                    description: Endpoint URL
                }
            }
        }
        response: ${_definitions.serving_model_info} {
            properties {
                uptime_sec {
                    type: integer
                    description: Max of model instance uptime in seconds
                }
                last_update {
                    type: string
                    format: "date-time"
                    description: The latest time when one of the model instances was updated
                }
                instances {
                    type: array
                    items {"$ref": "#/definitions/container_instance_stats"}
                }
            }
        }
    }
}
get_endpoint_metrics_history {
    "2.31" {
        description: Get endpoint charts
        request {
            type: object
            required: [endpoint_url, from_date, to_date, interval]
            properties {
                endpoint_url {
                    description: Endpoint Url
                    type: string
                }
                from_date {
                    description: "Starting time (in seconds from epoch) for collecting statistics"
                    type: number
                }
                to_date {
                    description: "Ending time (in seconds from epoch) for collecting statistics"
                    type: number
                }
                interval {
                    description: "Time interval in seconds for a single statistics point. The minimal value is 1"
                    type: integer
                }
                metric_type {
                    description: The type of the metrics to return on the chart
                    type: string
                    default: requests
                    enum: [
                        requests
                        requests_min
                        latency_ms
                        cpu_count
                        gpu_count
                        cpu_util
                        gpu_util
                        ram_total
                        ram_used
                        ram_free
                        gpu_ram_total
                        gpu_ram_used
                        gpu_ram_free
                        network_rx
                        network_tx
                    ]
                }
                instance_charts {
                    type: boolean
                    default: true
                    description: If set then return instance charts and total. Otherwise total only
                }
            }
        }
        response {
            type: object
            properties {
                computed_interval {
                    description: The inteval that was actually used for the histogram. May be larger then the requested one
                    type: integer
                }
                total: ${_definitions.metrics_history_series} {
                    properties {
                        description: The total histogram
                    }
                }
                instances {
                    description: Instance charts
                    type: object
                    additionalProperties: ${_definitions.metrics_history_series}
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/storage.conf">
_description: """This service provides storage settings managmement"""
_default {
    internal: true
}

_definitions {
    include "_common.conf"
    aws_bucket {
        type: object
        description: Settings per S3 bucket
        properties {
            bucket {
                description: The name of the bucket
                type: string
            }
            subdir {
                description: The path to match
                type: string
            }
            host {
                description: Host address (for minio servers)
                type: string
            }
            key {
                description: Access key
                type: string
            }
            secret {
                description: Secret key
                type: string
            }
            token {
                description: Access token
                type: string
            }
            multipart {
                description: Multipart upload
                type: boolean
                default: true
            }
            acl {
                description: ACL
                type: string
            }
            secure {
                description: Use SSL connection
                type: boolean
                default: true
            }
            region {
                description: AWS Region
                type: string
            }
            verify {
                description: Verify server certificate
                type: boolean
                default: true
            }
            use_credentials_chain {
                description: Use host configured credentials
                type: boolean
                default: false
            }
        }
    }
    aws {
        type: object
        description: AWS S3 storage settings
        properties {
            key {
                description: Access key
                type: string
            }
            secret {
                description: Secret key
                type: string
            }
            region {
                description: AWS region
                type: string
            }
            token {
                description: Access token
                type: string
            }
            use_credentials_chain {
                description: If set then use host credentials
                type: boolean
                default: false
            }
            buckets {
                description: Credential settings per bucket
                type: array
                items {"$ref": "#/definitions/aws_bucket"}
            }
        }
    }
    google_bucket {
        type: object
        description: Settings per Google storage bucket
        properties {
            bucket {
                description: The name of the bucket
                type: string
            }
            project {
                description: The name of the project
                type: string
            }
            subdir {
                description: The path to match
                type: string
            }
            credentials_json {
                description: The contents of the credentials json file
                type: string
            }
        }
    }
    google {
        type: object
        description: Google storage settings
        properties {
            project {
                description: Project name
                type: string
            }
            credentials_json {
                description: The contents of the credentials json file
                type: string
            }
            buckets {
                description: Credentials per bucket
                type: array
                items {"$ref": "#/definitions/google_bucket"}
            }
        }
    }
    azure_container {
        type: object
        description: Azure container settings
        properties {
            account_name {
                description: Account name
                type: string
            }
            account_key {
                description: Account key
                type: string
            }
            container_name {
                description: The name of the container
                type: string
            }
        }
    }
    azure {
        type: object
        description: Azure storage settings
        properties {
            containers {
                description: Credentials per container
                type: array
                items {"$ref": "#/definitions/azure_container"}
            }
        }
    }
}

set_settings {
    "2.31" {
        description: Set Storage settings
        request {
            type: object
            properties {
                aws {"$ref": "#/definitions/aws"}
                google {"$ref": "#/definitions/google"}
                azure {"$ref": "#/definitions/azure"}
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of settings documents updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
reset_settings {
    "2.31" {
        description: Reset selected storage settings
        request {
            type: object
            properties {
                keys {
                    description: The names of the settings to delete
                    type: array
                    items {
                        type: string
                        enum: ["azure", "aws", "google"]
                    }
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of settings documents updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
get_settings {
    "2.22" {
        description: Get storage settings
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                last_update {
                    description: "Settings last update time (UTC) "
                    type: string
                    format: "date-time"
                }
                aws {"$ref": "#/definitions/aws"}
                google {"$ref": "#/definitions/google"}
                azure {"$ref": "#/definitions/azure"}
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/tasks.conf">
_description: "Provides a management API for tasks in the system."
_references {
    status_change_request {
        type: object
        properties {
            task {
                description: Task ID
                type: string
            }
            status_reason {
                description: Reason for status change
                type: string
            }
            status_message {
                description: Extra information regarding status change
                type: string
            }
        }
    }
    // "force" field with default description
    force_arg {
        type: boolean
        default: false
        description: "Allows forcing state change even if transition is not supported"
    }
}
_definitions {
    include "_tasks_common.conf"
    change_many_request: ${_definitions.batch_operation} {
        request {
            properties {
                status_reason {
                    description: Reason for status change
                    type: string
                }
                status_message {
                    description: Extra information regarding status change
                    type: string
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.updated {
                    description: "Number of tasks updated (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
                succeeded.items.properties.fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    update_response {
        type: object
        properties {
            updated {
                description: "Number of tasks updated (0 or 1)"
                type: integer
                enum: [ 0, 1 ]
            }
            fields {
                description: "Updated fields names and values"
                type: object
                additionalProperties: true
            }
        }
    }
    param_key {
        type: object
        properties {
            section {
                description: "Section that the parameter belongs to"
                type: string
            }
            name {
                description: "Name of the parameter. If the name is ommitted then the corresponding operation is performed on the whole section"
                type: string
            }
        }
    }
    replace_hyperparams_enum {
        type: string
        enum: [
            none,
            section,
            all
        ]
    }
    task_urls {
        type: object
        properties {
            model_urls {
                type: array
                items {type: string}
            }
            event_urls {
                type: array
                items {type: string}
            }
            artifact_urls {
                type: array
                items {type: string}
            }
        }
    }
}

get_by_id {
    "2.1" {
        description: "Gets task information"
        request {
            type: object
            required: [ task ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                task {
                    description: "Task info"
                    "$ref": "#/definitions/task"
                }
            }
        }
    }
}
get_by_id_ex {
    internal: true
    "2.11": ${get_all_ex."2.1"}
}
get_all_ex {
    internal: true
    "2.1": ${get_all."2.1"}
    "2.13": ${get_all_ex."2.1"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and project field is set then tasks from the subprojects are searched too"
                    type: boolean
                    default: false
                }
            }
        }
    }
    "2.14": ${get_all_ex."2.13"} {
        request.properties.search_hidden {
            description: "If set to 'true' then hidden tasks are included in the search results"
            type: boolean
            default: false
        }
    }
    "2.15": ${get_all_ex."2.14"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all_ex"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of tasks to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all_ex to retrieve more data"
        }
    }
    "2.23": ${get_all_ex."2.15"} {
        request.properties {
            allow_public {
                description: "Allow public tasks to be returned in the results"
                type: boolean
                default: true
            }
        }
    }
    "2.27": ${get_all_ex."2.23"} {
        request.properties {
            filters {
                type: object
                additionalProperties: ${_definitions.field_filter}
            }
        }
    }
}
get_all {
    "2.1" {
        description: "Get all the company's tasks and all public tasks"
        request {
            type: object
            properties {
                id {
                    description: "List of IDs to filter by"
                    type: array
                    items { type: string }
                }
                name {
                    description: "Get only tasks whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                user {
                    description: "List of user IDs used to filter results by the task's creating user"
                    type: array
                    items { type: string }
                }
                project {
                    type: array
                    items { type: string }
                }
                page {
                    description: "Page number, returns a specific page out of the resulting list of tasks"
                    type: integer
                    minimum: 0
                }
                page_size {
                    description: "Page size, specifies the number of results returned in each page (last page may contain fewer results)"
                    type: integer
                    minimum: 1
                }
                order_by {
                    description: """List of field names to order by. When search_text is used,
                    '@text_score' can be used as a field representing the text score of returned documents.
                    Use '-' prefix to specify descending order. Optional, recommended when using page.
                    If the first order field is a hyper parameter or metric then string values are ordered
                    according to numeric ordering rules where applicable"""
                    type: array
                    items { type: string }
                }
                type {
                    description: "List of task types. One or more of: 'import', 'annotation', 'training' or 'testing' (case insensitive)"
                    type: array
                    items { type: string }
                }
                tags {
                    description: "List of task user-defined tags. Use '-' prefix to exclude tags"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "List of task system tags. Use '-' prefix to exclude system tags"
                    type: array
                    items { type: string }
                }
                status {
                    description: "List of task status."
                    type: array
                    items { "$ref": "#/definitions/task_status_enum" }
                }
                project {
                    description: "List of project IDs"
                    type: array
                    items { type: string }
                }
                only_fields {
                    description: "List of task field names (nesting is supported using '.', e.g. execution.model_labels). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
                parent {
                    description: "Parent ID"
                    type: string
                }
                status_changed {
                    description: "List of status changed constraint strings (utcformat, epoch) with an optional prefix modifier (\>, \>=, \<, \<=)"
                    type: array
                    items {
                        type: string
                        pattern: "^(>=|>|<=|<)?.*$"
                    }
                }
                search_text {
                    description: "Free text search query"
                    type: string
                }
                _all_ {
                    description: "Multi-field pattern condition (all fields match pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
                _any_ {
                    description: "Multi-field pattern condition (any field matches pattern)"
                    "$ref": "#/definitions/multi_field_pattern_data"
                }
            }
        }
        response {
            type: object
            properties {
                tasks {
                    description: "List of tasks"
                    type: array
                    items { "$ref": "#/definitions/task" }
                }
            }
        }
    }
    "2.14": ${get_all."2.1"} {
        request.properties.search_hidden {
            description: "If set to 'true' then hidden tasks are included in the search results"
            type: boolean
            default: false
        }
    }
    "2.15": ${get_all."2.14"} {
        request {
            properties {
                scroll_id {
                    type: string
                    description: "Scroll ID returned from the previos calls to get_all"
                }
                refresh_scroll {
                    type: boolean
                    description: "If set then all the data received with this scroll will be requeried"
                }
                size {
                    type: integer
                    minimum: 1
                    description: "The number of tasks to retrieve"
                }
            }
        }
        response.properties.scroll_id {
            type: string
            description: "Scroll ID that can be used with the next calls to get_all to retrieve more data"
        }
    }
    "2.26": ${get_all."2.15"} {
        request {
            properties {
                include_subprojects {
                    description: "If set to 'true' and project field is set then tasks from the subprojects are searched too"
                    type: boolean
                    default: false
                }
            }
        }
    }
}
get_types {
    "2.8" {
        description: "Get the list of task types used in the specified projects"
        request {
            type: object
            properties {
                projects {
                    description: "The list of projects which tasks will be analyzed. If not passed or empty then all the company and public tasks will be analyzed"
                    type: array
                    items: {type: string}
                }
            }
        }
        response {
            type: object
            properties {
                types {
                    description: "Unique list of the task types used in the requested projects"
                    type: array
                    items: {type: string}
                }
            }
        }
    }
}
clone {
    "2.5" {
        description: "Clone an existing task"
        request {
            type: object
            required: [ task ]
            properties {
                task {
                    description: "ID of the task"
                    type: string
                }
                new_task_name {
                    description: "The name of the cloned task. If not provided then taken from the original task"
                    type: string
                }
                new_task_comment {
                    description: "The comment of the cloned task. If not provided then taken from the original task"
                    type: string
                }
                new_task_tags {
                    description: "The user-defined tags of the cloned task. If not provided then taken from the original task"
                    type: array
                    items { type: string }
                }
                new_task_system_tags {
                    description: "The system tags of the cloned task. If not provided then empty"
                    type: array
                    items { type: string }
                }
                new_task_parent {
                    description: "The parent of the cloned task. If not provided then taken from the original task"
                    type: string
                }
                new_task_project {
                    description: "The project of the cloned task. If not provided then taken from the original task"
                    type: string
                }
                new_task_hyperparams {
                    description: "The hyper params for the new task. If not provided then taken from the original task"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/section_params"
                    }
                }
                new_task_configuration {
                    description: "The configuration for the new task. If not provided then taken from the original task"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/configuration_item"
                    }
                }
                execution_overrides {
                    description: "The execution params for the cloned task. The params not specified are taken from the original task"
                    "$ref": "#/definitions/execution"
                }
                validate_references {
                    description: "If set to 'false' then the task fields that are copied from the original task are not validated. The default is false."
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "ID of the new task"
                    type: string
                }
            }
        }
    }
    "2.12": ${clone."2.5"} {
        request {
            properties {
                new_project_name {
                    description: "Clone task to a new project by this name (only if `new_task_project` is not provided). If a project by this name already exists, task will be cloned to existing project."
                    type: string
                }
            }
        }
        response {
            properties {
                new_project {
                    description: "In case the new_project_name was specified returns the target project details"
                    type: object
                    properties {
                        id {
                            description: "The ID of the target project"
                            type: string
                        }
                        name {
                            description: "The name of the target project"
                            type: string
                        }
                    }
                }
            }
        }
    }
    "2.13": ${clone."2.12"}{
        request {
            properties {
                new_task_input_models {
                    description: "The list of input models for the cloned task. If not specifed then copied from the original task"
                    type: array
                    items {"$ref": "#/definitions/task_model_item"}
                }
                new_task_container {
                    description: "The docker container properties for the new task. If not provided then taken from the original task"
                    type: object
                    additionalProperties { type: string }
                }
            }
        }
    }
}
edit_runtime {
    "2.34" {
        description: "Edit task's runtime properties by adding, updating or removing existing fields"
        request {
            type: object
            required: [task]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                add_or_update {
                    description: "Runtime properties to be updated or added (key/value)"
                    type: object
                    additionalProperties { type: string }
                }
                remove {
                    description: "Runtime properties to be removed (list of keys)"
                    type: array
                    items { type: string }
                }
                force {
                    description: "If set to 'true' then both new and running task runtime can be edited. Otherwise only the new task ones. Default is 'false'"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}
add_or_update_model {
    "2.13" {
        description: "Add or update task model"
        request {
            type: object
            required: [task, name, model, type]
            properties {
                task {
                    description: "ID of the task"
                    type: string
                }
                name {
                    description: "The task model name"
                    type: string
                }
                model {
                    description: "The model ID"
                    type: string
                }
                type {
                    description: "The task model type"
                    "$ref": "#/definitions/model_type_enum"
                }
                iteration {
                    description: "Iteration (used to update task statistics)"
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of tasks updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
delete_models {
    "2.13" {
        description: "Delete models from task"
        request {
            type: object
            required: [ task, models ]
            properties {
                task {
                    description: "ID of the task"
                    type: string
                }
                models {
                    description: "The list of models to delete"
                    type: array
                    items {
                        type: object
                        required: [name, type]
                        properties {
                            name {
                                description: "The task model name"
                                type: string
                            }
                            type {
                                description: "The task model type"
                                "$ref": "#/definitions/model_type_enum"
                            }
                        }
                    }
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of tasks updated (0 or 1)"
                    type: integer
                    enum: [0, 1]
                }
            }
        }
    }
}
create {
    "2.1" {
        description: "Create a new task"
        request {
            type: object
            required: [
                name
                type
            ]
            properties {
                name {
                    description: "Task name. Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                type {
                    description: "Type of task"
                    "$ref": "#/definitions/task_type_enum"
                }
                comment {
                    description: "Free text comment "
                    type: string
                }
                parent {
                    description: "Parent task id Must be a completed task."
                    type: string
                }
                project {
                    description: "Project ID of the project to which this task is assigned Must exist[ab]"
                    type: string
                }
                output_dest {
                    description: "Output storage id Must be a reference to an existing storage."
                    type: string
                }
                execution {
                    description: "Task execution params"
                    "$ref": "#/definitions/execution"
                }
                script {
                    description: "Script info"
                    "$ref": "#/definitions/script"
                }
                hyperparams {
                    description: "Task hyper params per section"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/section_params"
                    }
                }
                configuration {
                    description: "Task configuration params"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/configuration_item"
                    }
                }
            }
        }
        response {
            type: object
            properties {
                id {
                    description: "ID of the task"
                    type: string
                }
            }
        }
    }
    "2.13": ${create."2.1"} {
        request {
            properties {
                models {
                    description: "Task models"
                    "$ref": "#/definitions/task_models"
                }
                container {
                    description: "Docker container parameters"
                    type: object
                    additionalProperties { type: string }
                }
            }
        }
    }
}
validate {
    "2.1" {
        description: "Validate task properties (before create)"
        request {
            type: object
            required: [
                name
                type
            ]
            properties {
                name {
                    description: "Task name. Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                type {
                    description: "Type of task"
                    "$ref": "#/definitions/task_type_enum"
                }
                comment {
                    description: "Free text comment "
                    type: string
                }
                parent {
                    description: "Parent task id Must be a completed task."
                    type: string
                }
                project {
                    description: "Project ID of the project to which this task is assigned Must exist[ab]"
                    type: string
                }
                output_dest {
                    description: "Output storage id Must be a reference to an existing storage."
                    type: string
                }
                execution {
                    description: "Task execution params"
                    "$ref": "#/definitions/execution"
                }
                script {
                    description: "Script info"
                    "$ref": "#/definitions/script"
                }
                hyperparams {
                    description: "Task hyper params per section"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/section_params"
                    }
                }
                configuration {
                    description: "Task configuration params"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/configuration_item"
                    }
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
    "2.13": ${validate."2.1"} {
        request {
            properties {
                models {
                    description: "Task models"
                    "$ref": "#/definitions/task_models"
                }
                container {
                    description: "Docker container parameters"
                    type: object
                    additionalProperties { type: string }
                }
            }
        }
    }
}
update {
    "2.1" {
        description: "Update task's runtime parameters"
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "ID of the task"
                    type: string
                }
                name {
                    description: "Task name Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                comment {
                    description: "Free text comment "
                    type: string
                }
                project {
                    description: "Project ID of the project to which this task is assigned"
                    type: string
                }
                output__error {
                    description: "Free text error"
                    type: string
                }
                created {
                    description: "Task creation time (UTC) "
                    type: string
                    format: "date-time"
                }
            }
        }
        response: ${_definitions.update_response}
    }
}
update_batch {
    "2.1" {
        description: """Updates a batch of tasks.
        Headers
        Content type should be 'application/json-lines'."""
        batch_request: {
            action: update
            version: 1.5
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of tasks updated (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
}
edit {
    "2.1" {
        description: "Edit task's details."
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "ID of the task"
                    type: string
                }
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is not 'created'"
                }
                name {
                    description: "Task name Unique within the company."
                    type: string
                }
                tags {
                    description: "User-defined tags list"
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: "System tags list. This field is reserved for system use, please don't use it."
                    type: array
                    items { type: string }
                }
                type {
                    description: "Type of task"
                    "$ref": "#/definitions/task_type_enum"
                }
                comment {
                    description: "Free text comment "
                    type: string
                }
                parent {
                    description: "Parent task id Must be a completed task."
                    type: string
                }
                project {
                    description: "Project ID of the project to which this task is assigned Must exist[ab]"
                    type: string
                }
                output_dest {
                    description: "Output storage id Must be a reference to an existing storage."
                    type: string
                }
                execution {
                    description: "Task execution params"
                    "$ref": "#/definitions/execution"
                }
                script {
                    description: "Script info"
                    "$ref": "#/definitions/script"
                }
                hyperparams {
                    description: "Task hyper params per section"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/section_params"
                    }
                }
                configuration {
                    description: "Task configuration params"
                    type: object
                    additionalProperties {
                        "$ref": "#/definitions/configuration_item"
                    }
                }
            }
        }
        response: ${_definitions.update_response}
    }
    "2.13": ${edit."2.1"} {
        request {
            properties {
                models {
                    description: "Task models"
                    "$ref": "#/definitions/task_models"
                }
                container {
                    description: "Docker container parameters"
                    type: object
                    additionalProperties { type: string }
                }
                runtime {
                    description: "Task runtime mapping"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
}
reset {
    "2.1" {
        description: "Reset a task to its initial state, along with any information stored for it (statistics, frame updates etc.)."
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg} {
                description: "If not true, call fails if the task status is 'completed'"
            }
            properties.clear_all {
                description: "Clear script and execution sections completely"
                type: boolean
                default: false
            }
        } ${_references.status_change_request}
        response: ${_definitions.update_response} {
            properties {
                dequeued {
                    description: "Response from queues.remove_task"
                    type: object
                    additionalProperties: true
                }
                events {
                    description: "Response from events.delete_for_task"
                    type: object
                    additionalProperties: true
                }
                deleted_models {
                    description: "Number of output models deleted by the reset"
                    type: integer
                }
            }
        }
    }
    "2.13": ${reset."2.1"} {
        request {
            properties {
                return_file_urls {
                    description: "If set to 'true' then return the urls of the files that were uploaded by this task. Default value is 'false'"
                    type: boolean
                }
                delete_output_models {
                    description: "If set to 'true' then delete output models of this task that are not referenced by other tasks. Default value is 'true'"
                    type: boolean
                }
            }
        }
        response {
            properties {
                urls {
                    description: "The urls of the files that were uploaded by this task. Returned if the 'return_file_urls' was set to 'true'"
                    "$ref": "#/definitions/task_urls"
                }
            }
        }
    }
    "2.21": ${reset."2.13"} {
        request.properties.delete_external_artifacts {
            description: "If set to 'true' then BE will try to delete the extenal artifacts associated with the task from the fileserver (if configured to do so)"
            type: boolean
            default: true
        }
    }
}
reset_many {
    "2.13": ${_definitions.batch_operation} {
        description: Reset tasks
        request {
            properties {
                ids.description: "IDs of the tasks to reset"
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is 'completed'"
                }
                clear_all {
                    description: "Clear script and execution sections completely"
                    type: boolean
                    default: false
                }
                return_file_urls {
                    description: "If set to 'true' then return the urls of the files that were uploaded by the tasks. Default value is 'false'"
                    type: boolean
                }
                delete_output_models {
                    description: "If set to 'true' then delete output models of the tasks that are not referenced by other tasks. Default value is 'true'"
                    type: boolean
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.dequeued {
                    description: "Indicates whether the task was dequeued"
                    type: boolean
                }
                succeeded.items.properties.updated {
                    description: "Number of tasks updated (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
                succeeded.items.properties.fields {
                    description: "Updated fields names and values"
                    type: object
                    additionalProperties: true
                }
                succeeded.items.properties.deleted_models {
                    description: "Number of output models deleted by the reset"
                    type: integer
                }
                succeeded.items.properties.urls {
                    description: "The urls of the files that were uploaded by the task. Returned if the 'return_file_urls' was set to 'true'"
                    "$ref": "#/definitions/task_urls"
                }
            }
        }
    }
    "2.21": ${reset_many."2.13"} {
        request.properties.delete_external_artifacts {
            description: "If set to 'true' then BE will try to delete the extenal artifacts associated with the tasks from the fileserver (if configured to do so)"
            type: boolean
            default: true
        }
    }
}
delete_many {
    "2.13": ${_definitions.batch_operation} {
        description: Delete tasks
        request {
            properties {
                ids.description: "IDs of the tasks to delete"
                move_to_trash {
                    description: "Move task to trash instead of deleting it. For internal use only, tasks in the trash are not visible from the API and cannot be restored!"
                    type: boolean
                    default: false
                }
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is 'in_progress'"
                }
                return_file_urls {
                    description: "If set to 'true' then return the urls of the files that were uploaded by the tasks. Default value is 'false'"
                    type: boolean
                }
                delete_output_models {
                    description: "If set to 'true' then delete output models of the tasks that are not referenced by other tasks. Default value is 'true'"
                    type: boolean
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.deleted {
                    description: "Indicates whether the task was deleted"
                    type: boolean
                }
                succeeded.items.properties.updated_children {
                    description: "Number of child tasks whose parent property was updated"
                    type: integer
                }
                succeeded.items.properties.updated_models {
                    description: "Number of models whose task property was updated"
                    type: integer
                }
                succeeded.items.properties.deleted_models {
                    description: "Number of deleted output models"
                    type: integer
                }
                succeeded.items.properties.urls {
                    description: "The urls of the files that were uploaded by the task. Returned if the 'return_file_urls' was set to 'true'"
                    "$ref": "#/definitions/task_urls"
                }
            }
        }
    }
    "2.21": ${delete_many."2.13"} {
        request.properties.delete_external_artifacts {
            description: "If set to 'true' then BE will try to delete the extenal artifacts associated with the tasks from the fileserver (if configured to do so)"
            type: boolean
            default: true
        }
    }
    "2.30": ${delete_many."2.21"} {
        request.properties.include_pipeline_steps {
            description: If set then for the passed pipeline controller tasks the pipeline steps will be also deleted
            type: boolean
            default: false
        }
    }
}
delete {
    "2.1" {
        description: """Delete a task along with any information stored for it (statistics, frame updates etc.)
        Unless Force flag is provided, operation will fail if task has objects associated with it - i.e. children tasks and projects.
        Models that refer to the deleted task will be updated with a task ID indicating a deleted task.
        """
        request = {
            type: object
            required: [
                task
            ]
            properties {
                move_to_trash {
                    description: "Move task to trash instead of deleting it. For internal use only, tasks in the trash are not visible from the API and cannot be restored!"
                    type: boolean
                    default: false
                }
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is 'in_progress'"
                }
            }
        } ${_references.status_change_request}
        response {
            type: object
            properties {
                deleted {
                    description: "Indicates whether the task was deleted"
                    type: boolean
                }
                updated_children {
                    description: "Number of child tasks whose parent property was updated"
                    type: integer
                }
                updated_models {
                    description: "Number of models whose task property was updated"
                    type: integer
                }
                events {
                    description: "Response from events.delete_for_task"
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.13": ${delete."2.1"} {
        request {
            properties {
                return_file_urls {
                    description: "If set to 'true' then return the urls of the files that were uploaded by this task. Default value is 'false'"
                    type: boolean
                }
                delete_output_models {
                    description: "If set to 'true' then delete output models of this task that are not referenced by other tasks. Default value is 'true'"
                    type: boolean
                }
            }
        }
        response {
            properties {
                urls {
                    description: "The urls of the files that were uploaded by this task. Returned if the 'return_file_urls' was set to 'true'"
                    "$ref": "#/definitions/task_urls"
                }
            }
        }
    }
    "2.21": ${delete."2.13"} {
        request.properties.delete_external_artifacts {
            description: "If set to 'true' then BE will try to delete the extenal artifacts associated with the task from the fileserver (if configured to do so)"
            type: boolean
            default: true
        }
    }
    "2.30": ${delete."2.21"} {
        request.properties.include_pipeline_steps {
            description: If set then and the passed task is a pipeline controller then delete the pipeline tasks too
            type: boolean
            default: false
        }
    }
}
archive {
    "2.12" {
        description: """Archive tasks.
        If a task is queued it will first be dequeued and then archived.
        """
        request = {
            type: object
            required: [
                tasks
            ]
            properties {
                tasks {
                    description: "List of task ids"
                    type: array
                    items { type: string }
                }
                status_reason {
                    description: Reason for status change
                    type: string
                }
                status_message {
                    description: Extra information regarding status change
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                archived {
                    description: "Indicates number of archived tasks"
                    type: integer
                }
            }
        }
    }
    "2.30": ${archive."2.12"} {
        request.properties.include_pipeline_steps {
            description: If set then for the passed pipeline controller tasks also archive the pipeline steps
            type: boolean
            default: false
        }
    }
}
archive_many {
    "2.13": ${_definitions.batch_operation} {
        description: Archive tasks
        request {
            properties {
                ids.description: "IDs of the tasks to archive"
                status_reason {
                    description: Reason for status change
                    type: string
                }
                status_message {
                    description: Extra information regarding status change
                    type: string
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.archived {
                    description: "Indicates whether the task was archived"
                    type: boolean
                }
            }
        }
    }
    "2.30": ${archive_many."2.13"} {
        request.properties.include_pipeline_steps {
            description: If set then for the passed pipeline controller tasks also archive the pipeline steps
            type: boolean
            default: false
        }
    }
}
unarchive_many {
    "2.13": ${_definitions.batch_operation} {
        description: Unarchive tasks
        request {
            properties {
                ids.description: "IDs of the tasks to unarchive"
                status_reason {
                    description: Reason for status change
                    type: string
                }
                status_message {
                    description: Extra information regarding status change
                    type: string
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.unarchived {
                    description: "Indicates whether the task was unarchived"
                    type: boolean
                }
            }
        }
    }
    "2.30": ${unarchive_many."2.13"} {
        request.properties.include_pipeline_steps {
            description: If set then for the passed pipeline controller tasks also archive the pipeline steps
            type: boolean
            default: false
        }
    }
}
started {
    "2.1" {
        description: "Mark a task status as in_progress. Optionally allows to set the task's execution progress."
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg} {
                description: "If not true, call fails if the task status is not 'not_started'"
            }
        } ${_references.status_change_request}
        response: ${_definitions.update_response} {
            properties {
                started {
                    description: "Number of tasks started (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
}
stop {
    "2.1" {
        description: "Request to stop a running task"
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg} {
                description: "If not true, call fails if the task status is not 'in_progress'"
            }
        } ${_references.status_change_request}
        response: ${_definitions.update_response}
    }
    "2.30": ${stop."2.1"} {
        request.properties.include_pipeline_steps {
            description: If set and the passed task is a pipeline controller then stop all its steps too
            type: boolean
            default: false
        }
    }
}
stop_many {
    "2.13": ${_definitions.change_many_request} {
        description: "Request to stop running tasks"
        request {
            properties {
                ids.description: "IDs of the tasks to stop"
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is not 'in_progress'"
                }
            }
        }
    }
    "2.30": ${stop_many."2.13"} {
        request.properties.include_pipeline_steps {
            description: If set then for all the passed pipeline controller tasks stop their steps too
            type: boolean
            default: false
        }
    }
}
stopped {
    "2.1" {
        description: "Signal a task has stopped"
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg} {
                description: "If not true, call fails if the task status is not 'stopped'"
            }
        } ${_references.status_change_request}
        response: ${_definitions.update_response}
    }
}
failed {
    "2.1" {
        description: "Indicates that task has failed"
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg}
        } ${_references.status_change_request}
        response: ${_definitions.update_response}
    }
}
close {
    "2.1" {
        description: "Indicates that task is closed"
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg}
        } ${_references.status_change_request}
        response: ${_definitions.update_response}
    }
}
publish {
    "2.1" {
        description: """Mark a task status as published.

        For Annotation tasks - if any changes were committed by this task, a new version in the dataset together with an output view are created.

        For Training tasks - if a model was created, it should be set to ready."""
        request = {
            type: object
            required: [
                task
            ]
            properties {
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is not 'stopped'"
                }
                publish_model {
                    description: "Indicates that the task output model (if exists) should be published. Optional, the default value is True."
                    type: boolean
                }
            }
        } ${_references.status_change_request}
        response: ${_definitions.update_response}
    }
}
publish_many {
    "2.13": ${_definitions.change_many_request} {
        description: Publish tasks
        request {
            properties {
                ids.description: "IDs of the tasks to publish"
                force = ${_references.force_arg} {
                    description: "If not true, call fails if the task status is not 'stopped'"
                }
                publish_model {
                    description: "Indicates that the task output model (if exists) should be published. Optional, the default value is True."
                    type: boolean
                }
            }
        }
    }
}
enqueue {
    "1.5" {
        description: """Adds a task into a queue.

Fails if task state is not 'created'.

Fails if the following parameters in the task were not filled:

* execution.script.repository

* execution.script.entrypoint
"""
        request = {
            type: object
            required: [
                task
            ]
            properties {
                queue {
                    description: "Queue id. If not provided and no queue name is passed then task is added to the default queue."
                    type: string
                }
            }

        } ${_references.status_change_request}
        response: ${_definitions.update_response} {
            properties {
                queued {
                    description: "Number of tasks queued (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
    "2.19": ${enqueue."1.5"} {
        request.properties.queue_name {
            description: The name of the queue. If the queue does not exist then it is auto-created. Cannot be used together with the queue id
            type: string
        }
    }
    "2.22": ${enqueue."2.19"} {
        request.properties.verify_watched_queue {
            description: If passed then check wheter there are any workers watiching the queue
            type: boolean
            default: false
        }
        response.properties.queue_watched {
            description: Returns true if there are workers or autscalers working with the queue
            type: boolean
        }
    }
    "2.31": ${enqueue."2.22"} {
        request.properties.update_execution_queue {
            description: If set to false then the task 'execution.queue' is not updated. This can be done only for the task that is already enqueued
            type: boolean
            default: true
        }
    }
}
enqueue_many {
    "2.13": ${_definitions.change_many_request} {
        description: Enqueue tasks
        request {
            properties {
                ids.description: "IDs of the tasks to enqueue"
                queue {
                    description: "Queue id. If not provided and no queue name is passed then tasks are added to the default queue."
                    type: string
                }
                validate_tasks {
                    description: "If set then tasks are validated before enqueue"
                    type: boolean
                    default: false
                }
            }
        }
        response {
            properties {
                succeeded.items.properties.queued {
                    description: "Indicates whether the task was queued"
                    type: boolean
                }
            }
        }
    }
    "2.19": ${enqueue_many."2.13"} {
        request.properties.queue_name {
            description: The name of the queue. If the queue does not exist then it is auto-created. Cannot be used together with the queue id
            type: string
        }
    }
    "2.22": ${enqueue_many."2.19"} {
        request.properties.verify_watched_queue {
            description: If passed then check wheter there are any workers watiching the queue
            type: boolean
            default: false
        }
        response.properties.queue_watched {
            description: Returns true if there are workers or autscalers working with the queue
            type: boolean
        }
    }
}
dequeue {
    "1.5" {
        description: """Remove a task from its queue.
        Fails if task status is not queued."""
        request = {
            type: object
            required: [
                task
            ]
        } ${_references.status_change_request}
        response: ${_definitions.update_response} {
            properties {
                dequeued {
                    description: "Number of tasks dequeued (0 or 1)"
                    type: integer
                    enum: [ 0, 1 ]
                }
            }
        }
    }
    "2.25": ${dequeue."1.5"} {
        request.properties.remove_from_all_queues {
            type: boolean
            description: If set to 'true' then the task is searched and removed from all the queues. Otherwise only from the queue stored in the task execution parameters
            default: false
        }
    }
    "2.26": ${dequeue."2.25"} {
        request.properties.new_status {
            type: string
            description: The new status to assign to the task after the dequeue instead of the default one
        }
    }
}
dequeue_many {
    "2.13": ${_definitions.change_many_request} {
        description: Dequeue tasks
        request {
            properties {
                ids.description: "IDs of the tasks to dequeue"
            }
        }
        response {
            properties {
                succeeded.items.properties.dequeued {
                    description: "Indicates whether the task was dequeued"
                    type: boolean
                }
            }
        }
    }
    "2.25": ${dequeue_many."2.13"} {
        request.properties.remove_from_all_queues {
            type: boolean
            description: If set to 'true' then the tasks are searched and removed from all the queues. Otherwise only from the queue stored in the task execution parameters
            default: false
        }
    }
    "2.26": ${dequeue_many."2.25"} {
        request.properties.new_status {
            type: string
            description: The new status to assign to the task after the dequeue instead of the default one
        }
    }
}
set_requirements {
    "2.1" {
        description: """Set the script requirements for a task"""
        request {
            type: object
            required: [
                task
                requirements
            ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                requirements {
                    description: "A JSON object containing requirements strings by key"
                    type: object
                }
            }
        }
        response: ${_definitions.update_response}
    }
}

completed {
    "2.2" {
        description: "Signal a task has completed"
        request = {
            type: object
            required: [
                task
            ]
            properties.force = ${_references.force_arg} {
                description: "If not true, call fails if the task status is not in_progress/stopped"
            }
        } ${_references.status_change_request}
        response: ${_definitions.update_response}
    }
    "2.20": ${completed."2.2"} {
        request.properties.publish {
            type: boolean
            default: false
            description: If set and the task is completed successfully then it is published
        }
        response.properties.published {
            description: "Number of tasks published (0 or 1)"
            type: integer
            enum: [0, 1]
        }
    }
}

ping {
    "2.1" {
        description: """ Refresh the task's last update time"""
        request {
            type: object
            required: [
                task
            ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}

add_or_update_artifacts {
    "2.10" {
        description: """Update existing artifacts (search by key/mode) and add new ones"""
        request {
            type: object
            required: [task, artifacts]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                artifacts {
                    description: "Artifacts to add or update"
                    type: array
                    items {"$ref": "#/definitions/artifact"}
                }
                force {
                    description: "If set to True then both new and running task artifacts can be edited. Otherwise only the new task ones. Default is False"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}
delete_artifacts {
    "2.10" {
        description: """Delete existing artifacts (search by key/mode)"""
        request {
            type: object
            required: [task, artifacts]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                artifacts {
                    description: "Artifacts to delete"
                    type: array
                    items {"$ref": "#/definitions/artifact_id"}
                }
                force {
                    description: "If set to True then both new and running task artifacts can be deleted. Otherwise only the new task ones. Default is False"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}

make_public {
    "2.9" {
        description: """Convert company tasks to public"""
        request {
            type: object
            properties {
                ids {
                    description: "Ids of the tasks to convert"
                    type: array
                    items { type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of tasks updated"
                    type: integer
                }
            }
        }
    }
}

make_private {
    "2.9" {
        description: """Convert public tasks to private"""
        request {
            type: object
            properties {
                ids {
                    description: "Ids of the tasks to convert. Only the tasks originated by the company can be converted"
                    type: array
                    items { type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Number of tasks updated"
                    type: integer
                }
            }
        }
    }
}

get_hyper_params {
    "2.9": {
        description: "Get the list of task hyper parameters"
        request {
            type: object
            required: [tasks]
            properties {
                tasks {
                    description: "Task IDs"
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                params {
                    description: "Hyper parameters (keyed by task ID)"
                    type: array
                    items {
                        type: object
                        properties {
                            "task": {
                                description: "Task ID"
                                type: string
                            }
                            "hyperparams": {
                                description: "Hyper parameters"
                                type: array
                                items {"$ref": "#/definitions/params_item"}
                            }
                        }
                    }
                }
            }
        }
    }
}
edit_hyper_params {
    "2.9" {
        description: "Add or update task hyper parameters"
        request {
            type: object
            required: [ task, hyperparams ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                hyperparams {
                    description: "Task hyper parameters. The new ones will be added and the already existing ones will be updated"
                    type: array
                    items {"$ref": "#/definitions/params_item"}
                }
                replace_hyperparams {
                    description: """Can be set to one of the following:
                     'all' - all the hyper parameters will be replaced with the provided ones
                     'section' - the sections that present in the new parameters will be replaced with the provided parameters
                     'none' (the default value) - only the specific parameters will be updated or added"""
                    "$ref": "#/definitions/replace_hyperparams_enum"
                }
                force {
                    description: "If set to True then both new and running task hyper params can be edited. Otherwise only the new task ones. Default is False"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}
delete_hyper_params {
    "2.9": {
        description: "Delete task hyper parameters"
        request {
            type: object
            required: [ task, hyperparams ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                hyperparams {
                    description: "List of hyper parameters to delete. In case a parameter with an empty name is passed all the section will be deleted"
                    type: array
                    items { "$ref": "#/definitions/param_key" }
                }
                force {
                    description: "If set to True then both new and running task hyper params can be deleted. Otherwise only the new task ones. Default is False"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}

get_configurations {
    "2.9": {
        description: "Get the list of task configurations"
        request {
            type: object
            required: [tasks]
            properties {
                tasks {
                    description: "Task IDs"
                    type: array
                    items { type: string }
                }
                names {
                    description: "Names of the configuration items to retreive. If not passed or empty then all the configurations will be retreived."
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                configurations {
                    description: "Configurations (keyed by task ID)"
                    type: array
                    items {
                        type: object
                        properties {
                            "task" {
                                description: "Task ID"
                                type: string
                            }
                            "configuration" {
                                description: "Configuration list"
                                type: array
                                items {"$ref": "#/definitions/configuration_item"}
                            }
                        }
                    }
                }
            }
        }
    }
}
get_configuration_names {
    "2.9": {
        description: "Get the list of task configuration items names"
        request {
            type: object
            required: [tasks]
            properties {
                tasks {
                    description: "Task IDs"
                    type: array
                    items { type: string }
                }
                skip_empty {
                    description: If set to 'true' then the names for configurations with missing values are not returned
                    type: boolean
                    default: true
                }
            }
        }
        response {
            type: object
            properties {
                configurations {
                    description: "Names of task configuration items (keyed by task ID)"
                    type: object
                    properties {
                        task {
                            description: "Task ID"
                            type: string
                        }
                        names {
                            description: "Configuration names"
                            type: array
                            items {type: string}
                        }
                    }
                }
            }
        }
    }
}
edit_configuration {
    "2.9" {
        description: "Add or update task configuration"
        request {
            type: object
            required: [ task, configuration ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                configuration {
                    description: "Task configuration items. The new ones will be added and the already existing ones will be updated"
                    type: array
                    items {"$ref": "#/definitions/configuration_item"}
                }
                replace_configuration {
                    description: "If set then the all the configuration items will be replaced with the provided ones. Otherwise only the provided configuration items will be updated or added"
                    type: boolean
                }
                force {
                    description: "If set to True then both new and running task configuration can be edited. Otherwise only the new task ones. Default is False"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}
delete_configuration {
    "2.9": {
        description: "Delete task configuration items"
        request {
            type: object
            required: [ task, configuration ]
            properties {
                task {
                    description: "Task ID"
                    type: string
                }
                configuration {
                    description: "List of configuration itemss to delete"
                    type: array
                    items { type: string }
                }
                force {
                    description: "If set to True then both new and running task configuration can be deleted. Otherwise only the new task ones. Default is False"
                    type: boolean
                }
            }
        }
        response {
            type: object
            properties {
                deleted {
                    description: "Indicates if the task was updated successfully"
                    type: integer
                }
            }
        }
    }
}
move {
    "2.12" {
        description: "Move tasks to a project"
        request {
            type: object
            required: [ids]
            properties {
                ids {
                    description: "Tasks to move"
                    type: array
                    items { type: string }
                }
                project {
                    description: "Target project ID. If not provided, `project_name` must be provided. Use null for the root project"
                    type: string
                }
                project_name {
                    description: "Target project name. If provided and a project with this name does not exist, a new project will be created. If not provided, `project` must be provided."
                    type: string
                }
            }
        }
        response {
            type: object
            additionalProperties: true
        }
    }
}
update_tags {
    "2.27" {
        description: Add or remove tags from multiple tasks
        request {
            type: object
            properties {
                ids {
                    type: array
                    description: IDs of the tasks to update
                    items {type: string}
                }
                add_tags {
                    type: array
                    description: User tags to add
                    items {type: string}
                }
                remove_tags {
                    type: array
                    description: User tags to remove
                    items {type: string}
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    type: integer
                    description: The number of updated tasks
                }
            }
        }
    }
}
</file>

<file path="apiserver/schema/services/users.conf">
_description: """This service provides a management interface to users information
and new users login restrictions."""
_default {
    internal: true
}
_definitions {
    user {
        type: object
        properties {
            id {
                description: User ID
                type: string
            }
            name {
                description: Full name
                type: string
            }
            given_name {
                description: Given name
                type: string
            }
            family_name {
                description: Family name
                type: string
            }
            avatar {
                description: Avatar URL
                type: string
            }
            company {
                description: Company ID
                type: string
            }
            # Admin only fields
            role {
                description: """User's role (admin only)"""
                type: string
            }
            providers {
                description: """Providers uses has logged-in with"""
                type: object
                additionalProperties: true
            }
            created {
                description: User creation date
                type: string
                format: date-time
            }
            email {
                description: User email
                type: string
                format: email
            }
        }
    }
    get_current_user_response_user_object {
        type: object
        description: "like user, but returns company object instead of ID"
        properties {
            id {
                type: string
            }
            name {
                type: string
            }
            given_name {
                type: string
            }
            family_name {
                type: string
            }
            role {
                type: string
            }
            avatar {
                type: string
            }
            company {
                type: object
                properties {
                    id {
                        type: string
                    }
                    name {
                        type: string
                    }
                }
            }
            preferences {
                description: User preferences
                type: object
                additionalProperties: true
            }
        }
    }
}

get_by_id {
    internal: false
    "2.1" {
        description: Gets user information
        request {
            type: object
            required: [ user ]
            properties {
                user {
                    description: User ID
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                user {
                    description: User info
                    "$ref": "#/definitions/user"
                }
            }
        }
    }
}

get_current_user {
    internal: false
    "2.1" {
        description: """Gets current user information, based on the authenticated user making the call."""
        request {
            type: object
            additionalProperties: false
        }
        response {
            type: object
            properties {
                user {
                    description: "User info"
                    "$ref": "#/definitions/get_current_user_response_user_object"
                }
            }
        }
    }
    "2.20": ${get_current_user."2.1"} {
        response {
            properties {
                getting_started {
                    type: object
                    description: Getting stated info
                    additionalProperties: true
                }
                user.properties.created {
                    type: string
                    description: User creation time
                    format: date-time
                }
            }
        }
    }
    "2.26": ${get_current_user."2.20"} {
        response.properties.settings {
            type: object
            properties {
                max_download_items {
                    type: string
                    description: The maximum items downloaded for this user in csv file downloads
                }
            }
        }
    }
    "2.31": ${get_current_user."2.26"} {
        response.properties {
            user.properties.created_in_version {
                type: string
                description: Server version at user creation time
            }
        }
    }
}

get_all_ex {
    internal: true
    "2.1": ${get_all."2.1"} {
    }
    "2.8": ${get_all."2.1"} {
        request {
            type: object
            properties {
                active_in_projects {
                    description: "List of project IDs. If provided, return only users that were active in these projects. If empty list is provided, return users that were active in all projects"
                    type: array
                    items { type: string }
                }
            }
        }
    }

}

get_all {
    "2.1" {
        description: Get all user objects
        request {
            type: object
            properties {
                name {
                    description: "Get only users whose name matches this pattern (python regular expression syntax)"
                    type: string
                }
                id {
                    description: "List of user IDs used to filter results"
                    type: array
                    items { type: string }
                }
                only_fields {
                    description: "List of user field names (if applicable, nesting is supported using '.'). If provided, this list defines the query's projection (only these fields will be returned for each result entry)"
                    type: array
                    items { type: string }
                }
                page {
                    description: "Page number, returns a specific page out of the resulting list of users"
                    type: integer
                    minimum: 0
                }
                page_size {
                    description: "Page size, specifies the number of results returned in each page (last page may contain fewer results)"
                    type: integer
                    minimum: 1
                }
                order_by {
                    description: "List of field names to order by. Use '-' prefix to specify descending order. Optional, recommended when using page"
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                users {
                    description: User list
                    type: array
                    items { "$ref": "#/definitions/user" }
                }
            }
        }
    }
}
delete {
    internal: true
    allow_roles: [ "system", "root" ]
    "2.1" {
        description: Delete user
        description: Delete a user
        request {
            type: object
            required: [ user ]
            properties {
                user {
                    description: ID of user to delete
                    type: string
                }
            }
        }
        response {
            type: object
            additionalProperties: false
        }
    }
}

create {
    allow_roles: [ "system", "root" ]
    "2.1" {
        description: Create a new user object. Reserved for internal use.
        request {
            type: object
            required: [
                company
                id
                name
            ]
            properties {
                id {
                    description: User ID
                    type: string
                }
                company {
                    description: Company ID
                    type: string
                }
                name {
                    description: Full name
                    type: string
                }
                given_name {
                    description: Given name
                    type: string
                }
                family_name {
                    description: Family name
                    type: string
                }
                avatar {
                    description: Avatar URL
                    type: string
                }
            }
        }
        response {
            type: object
            properties {}
            additionalProperties: false
        }
    }
}

update {
    internal: false
    "2.1" {
        description: Update a user object
        request {
            type: object
            required: [ user ]
            properties {
                user {
                    description: User ID
                    type: string
                }
                name {
                    description: Full name
                    type: string
                }
                given_name {
                    description: Given name
                    type: string
                }
                family_name {
                    description: Family name
                    type: string
                }
                avatar {
                    description: Avatar URL
                    type: string
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: Number of updated user objects (0 or 1)
                    type: integer
                }
                fields {
                    description: Updated fields names and values
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
}

get_preferences {
    internal: false
    "2.1" {
        description: Get user preferences
        request {
            type: object
            properties {}
        }
        response {
            type: object
            properties {
                preferences {
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
}

set_preferences {
    internal: false
    "2.1" {
        description: Set user preferences
        request {
            type: object
            required: [ preferences ]
            properties {
                preferences {
                    description: """Updates to user preferences. A mapping from keys in dot notation to values.
                    For example, `{"a.b": 0}` will set the key "b" in object "a" to 0."""
                    type: object
                    additionalProperties: true
                }
            }
        }
        response {
            type: object
            properties {
                updated {
                    description: Number of updated user objects (0 or 1)
                    type: integer
                }
                fields {
                    description: If the preferences were updated successfully then return the updated preferences
                    type: object
                    additionalProperties: true
                }
            }
        }
    }
    "2.33": ${set_preferences."2.1"} {
        request.properties.return_updated {
            description: If set to 'true' and update was succcessful then return updated preferences
            type: boolean
            default: true
        }
    }
}
</file>

<file path="apiserver/schema/services/workers.conf">
_description: "Provides an API for worker machines, allowing workers to report status and get tasks for execution"
_definitions {
    include "_workers_common.conf"
    metrics_category {
        type: object
        properties {
            name {
                type: string
                description: "Name of the metrics category."
            }
            metric_keys {
                type: array
                items { type: string }
                description: "The names of the metrics in the category."
            }
        }
    }
    worker_stat_key {
        type: string
        enum: [
            cpu_usage
            cpu_temperature
            memory_used
            memory_free
            gpu_usage
            gpu_temperature
            gpu_fraction
            gpu_memory_free
            gpu_memory_used
            network_tx
            network_rx
            disk_free_home
            disk_free_temp
            disk_read
            disk_write
        ]
    }
    aggregation_type {
        type: string
        enum: [ avg, min, max ]
        description: "Metric aggregation type"
    }
    stat_item {
        type: object
        properties {
            key: ${_definitions.worker_stat_key} {
                description: "Name of a metric"
            }
            category {
                "$ref": "#/definitions/aggregation_type"
            }
        }
    }
    aggregation_stats {
        type: object
        properties {
            aggregation {
                "$ref": "#/definitions/aggregation_type"
            }
            dates {
                type: array
                description: "List of timestamps (in seconds from epoch) in the acceding order. The timestamps are separated by the requested interval. Timestamps where no workers activity was recorded are omitted."
                items { type: integer }
            }
            values {
                type: array
                description: "List of values corresponding to the dates in metric statistics"
                items { type: number }
            }
            resource_series {
                type: array
                description: "Metric data per single resource. Return only if split_by_resource request parameter is set to True"
                items {"$ref": "#/definitions/metric_resource_series"}
            }
        }
    }
    metric_resource_series {
        type: object
        properties {
            name {
                type: string
                description: Resource name
            }
            values {
                type: array
                description: "List of values corresponding to the dates in metric statistics"
                items { type: number }
            }
        }
    }
    metric_stats {
        type: object
        properties {
            metric {
                type: string
                description: "Name of the metric ("cpu_usage", "memory_used" etc.)"
            }
            variant {
                type: string
                description: "Name of the metric component. Set only if 'split_by_variant' was set in the request"
            }
            stats {
                type: array
                description: "Statistics data by type"
                items { "$ref": "#/definitions/aggregation_stats" }
            }
        }
    }
    worker_stats {
        type: object
        properties {
            worker {
                type: string
                description: "ID of the worker"
            }
            metrics {
                type: array
                description: "List of the metrics statistics for the worker"
                items { "$ref": "#/definitions/metric_stats" }
            }
        }
    }
    activity_series {
        type: object
        properties {
            dates {
                type: array
                description: "List of timestamps (in seconds from epoch) in the acceding order. The timestamps are separated by the requested interval."
                items {type: integer}
            }
            counts {
                type: array
                description: "List of worker counts corresponding to the timestamps in the dates list. None values are returned for the dates with no workers."
                items {type: integer}
            }
        }
    }
    worker {
        type: object
        properties {
            id {
                description: "Worker ID"
                type: string
            }
            user {
                description: "Associated user (under whose credentials are used by the worker daemon)"
                "$ref": "#/definitions/id_name_entry"
            }
            company {
                description: "Associated company"
                "$ref": "#/definitions/id_name_entry"
            }
            ip {
                description: "IP of the worker"
                type: string
            }
            register_time {
                description: "Registration time"
                type: string
                format: "date-time"
            }
            last_activity_time {
                description: "Last activity time (even if an error occurred)"
                type: string
                format: "date-time"
            }
            last_report_time {
                description: "Last successful report time"
                type: string
                format: "date-time"
            }
            task {
                description: "Task currently being run by the worker"
                "$ref": "#/definitions/current_task_entry"
            }
            project {
                description: "Project in which currently executing task resides"
                "$ref": "#/definitions/id_name_entry"
            }
            queue {
                description: "Queue from which running task was taken"
                "$ref": "#/definitions/queue_entry"
            }
            queues {
                description: "List of queues on which the worker is listening"
                type: array
                items { "$ref": "#/definitions/queue_entry" }
            }
            tags {
                description: "User tags for the worker"
                type: array
                items: { type: string }
            }
            system_tags {
                description: "System tags for the worker"
                type: array
                items: { type: string }
            }
            key {
                description: "Worker entry key"
                type: string
            }
        }
    }

    id_name_entry  {
        type: object
        properties {
            id {
                description: "Worker ID"
                type: string
            }
            name {
                description: "Worker name"
                type: string
            }
        }
    }

    current_task_entry = ${_definitions.id_name_entry} {
        properties {
            running_time {
                description: "Task running time"
                type: integer
            }
            last_iteration {
                description: "Last task iteration"
                type: integer
            }
        }
    }

    queue_entry = ${_definitions.id_name_entry} {
        properties {
            display_name {
                description: "Display name for the queue (if defined)"
                type: string
            }
            next_task {
                description: "Next task in the queue"
                "$ref": "#/definitions/id_name_entry"
            }
            num_tasks {
                description: "Number of task entries in the queue"
                type: integer
            }
        }
    }
}
get_all {
    "2.4" {
        description: "Returns information on all registered workers."
        request {
            type: object
            properties {
                last_seen {
                    description: """Filter out workers not active for more than last_seen seconds.
                        A value or 0 or 'none' will disable the filter."""
                    type: integer
                    default: 3600
                }
            }
        }
        response {
            type: object
            properties {
                workers {
                    type: array
                    items { "$ref": "#/definitions/worker" }
                }
            }
        }
    }
    "2.20": ${get_all."2.4"} {
        request.properties.tags {
            description: The list of allowed worker tags. Prepend tag value with '-' in order to exclude
            type: array
            items { type: string }
        }
    }
    "2.22": ${get_all."2.20"} {
        request.properties.system_tags {
            description: The list of allowed worker system tags. Prepend tag value with '-' in order to exclude
            type: array
            items { type: string }
        }
    }
    "2.30": ${get_all."2.22"} {
        request.properties.worker_pattern {
            description: The worker name pattern. If specified then only matching keys returned
            type: string
        }
    }
}
get_count {
    "2.26": {
        description: "Returns the number of registered workers."
        request {
            type: object
            properties {
                last_seen {
                    description: """Filter out workers not active for more than last_seen seconds.
                        A value or 0 or 'none' will disable the filter."""
                    type: integer
                    default: 0
                }
                tags {
                    description: The list of allowed worker tags. Prepend tag value with '-' in order to exclude
                    type: array
                    items { type: string }
                }
                system_tags {
                    description: The list of allowed worker system tags. Prepend tag value with '-' in order to exclude
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                count {
                    description: Workers count
                    type: integer
                }
            }
        }
    }
    "2.30": ${get_count."2.26"} {
        request.properties.worker_pattern {
            description: The worker name pattern. If specified then only matching keys are counted
            type: string
        }
    }
}
register {
    "2.4" {
        description: "Register a worker in the system. Called by the Worker Daemon."
        request {
            required: [ worker ]
            type: object
            properties {
                worker {
                    description: "Worker id. Must be unique in company."
                    type: string
                }
                timeout {
                    description: "Registration timeout in seconds. If timeout seconds have passed since the worker's last call to register or status_report, the worker is automatically removed from the list of registered workers."
                    type: integer
                    default: 600
                }
                queues {
                    description: "List of queue IDs on which the worker is listening."
                    type: array
                    items { type: string }
                }
                tags {
                    description: "User tags for the worker"
                    type: array
                    items: { type: string }
                }
            }
        }
        response {
            type: object
            properties {}
        }
    }
    "2.22": ${register."2.4"} {
        request.properties.system_tags {
            description: "System tags for the worker"
            type: array
            items: { type: string }
        }
    }
    "2.34": ${register."2.22"} {
        request.properties.resources {
            type: object
            description: Resources that this worker uses
            properties {
                cpu_usage {
                    type: number
                    description: CPU Usage
                }
                gpu_usage {
                    type: number
                    description: GPU Usage
                }
            }
        }
    }
}
unregister {
    "2.4" {
        description: "Unregister a worker in the system. Called by the Worker Daemon."
        request {
            required: [ worker ]
            type: object
            properties {
                worker {
                    description: "Worker id. Must be unique in company."
                    type: string
                }
            }
        }
        response {
            type: object
            properties {}
        }
    }
}
status_report {
    "2.4" {
        description: "Called periodically by the worker daemon to report machine status"
        request {
            required: [
                worker
                timestamp
            ]
            type: object
            properties {
                worker {
                    description: "Worker id."
                    type: string
                }
                task {
                    description: "ID of a task currently being run by the worker. If no task is sent, the worker's task field will be cleared."
                    type: string
                }
                queue {
                    description: "ID of the queue from which task was received. If no queue is sent, the worker's queue field will be cleared."
                    type: string
                }
                queues {
                    description: "List of queue IDs on which the worker is listening. If null, the worker's queues list will not be updated."
                    type: array
                    items { type: string }
                }
                timestamp {
                    description: "UNIX time in seconds since epoch."
                    type: integer
                }
                machine_stats {
                    description: "The machine statistics."
                    "$ref": "#/definitions/machine_stats"
                }
                tags {
                    description: "New user tags for the worker"
                    type: array
                    items: { type: string }
                }
            }
        }
        response {
            type: object
            properties {}
        }
    }
    "2.22": ${status_report."2.4"} {
        request.properties.system_tags {
            description: "New system tags for the worker"
            type: array
            items: { type: string }
        }
    }
}
get_metric_keys {
    "2.4" {
        description: "Returns worker statistics metric keys grouped by categories."
        request {
            type: object
            properties {
                worker_ids {
                    description: "List of worker ids to collect metrics for. If not provided or empty then all the company workers metrics are analyzed."
                    type: array
                    items { type: string }
                }
            }
        }
        response {
            type: object
            properties {
                categories {
                    type: array
                    description: "List of unique metric categories found in the statistics of the requested workers."
                    items { "$ref": "#/definitions/metrics_category" }
                }
            }
        }
    }
}
get_stats {
    "2.4" {
        description: "Returns statistics for the selected workers and time range aggregated by date intervals."
        request {
            type: object
            required: [ from_date, to_date, interval, items ]
            properties {
                worker_ids {
                    description: "List of worker ids to collect metrics for. If not provided or empty then all the company workers metrics are analyzed."
                    type: array
                    items { type: string }
                }
                from_date {
                    description: "Starting time (in seconds from epoch) for collecting statistics"
                    type: number
                }
                to_date {
                    description: "Ending time (in seconds from epoch) for collecting statistics"
                    type: number
                }
                interval {
                    description: "Time interval in seconds for a single statistics point. The minimal value is 1"
                    type: integer
                }
                items {
                    description: "List of metric keys and requested statistics"
                    type: array
                    items { "$ref": "#/definitions/stat_item" }
                }
                split_by_variant {
                    description: "If true then break statistics by hardware sub types"
                    type: boolean
                    default: false
                }
            }
        }
        response {
            type: object
            properties {
                workers {
                    type: array
                    description: "List of the requested workers with their statistics"
                    items { "$ref": "#/definitions/worker_stats" }
                }
            }
        }
    }
    "2.32": ${get_stats."2.4"} {
        request.properties {
            split_by_variant {
                description: "Obsolete, please do not use"
                type: boolean
                default: false
            }
            split_by_resource {
                type: boolean
                default: false
                description: If set then for GPU related keys return the per GPU charts in addition to the aggregated one
            }
        }
    }
}
get_activity_report {
    "2.4" {
        description: "Returns count of active company workers in the selected time range."
        request {
            type: object
            required: [ from_date, to_date, interval ]
            properties {
                from_date {
                    description: "Starting time (in seconds from epoch) for collecting statistics"
                    type: number
                }
                to_date {
                    description: "Ending time (in seconds from epoch) for collecting statistics"
                    type: number
                }
                interval {
                    description: "Time interval in seconds for a single statistics point. The minimal value is 1"
                    type: integer
                }
            }
        }
        response {
            type: object
            properties {
                total {
                    description: "Activity series that include all the workers that sent reports in the given time interval."
                    "$ref": "#/definitions/activity_series"
                }
                active {
                    description: "Activity series that include only workers that worked on a task in the given time interval."
                    "$ref": "#/definitions/activity_series"
                }
            }
        }
    }
}
</file>

<file path="apiserver/server_init/app_sequence.py">
log = config.logger(__file__)
⋮----
class AppSequence
⋮----
def __init__(self, app: Flask)
⋮----
def start(self, request_handlers: RequestHandlers)
⋮----
def _attach_request_handlers(self, request_handlers: RequestHandlers)
⋮----
def _configure(self)
⋮----
@staticmethod
    def _get_db_instance_key() -> str
⋮----
"""build a key that uniquely identifies specific mongo instance"""
hosts_string = ";".join(sorted(db.get_hosts()))
⋮----
def _init_dbs(self)
⋮----
upgrade_monitoring = config.get(
⋮----
empty_es = check_elastic_empty()
⋮----
empty_db = check_mongo_empty()
⋮----
def _load_services(self)
⋮----
def _start_worker(self)
⋮----
def _on_worker_stop(self)
</file>

<file path="apiserver/server_init/request_handlers.py">
log = config.logger(__file__)
⋮----
class RequestHandlers
⋮----
_request_strip_prefix = config.get("apiserver.request.strip_prefix", None)
_server_header = config.get("apiserver.response.headers.server", "clearml")
_basic_cookie_settings = config.get("apiserver.auth.cookies")
_custom_cookie_settings = {
⋮----
def _get_cookie_settings(self, cookie_key=None)
⋮----
settings = (
⋮----
domain = first(
⋮----
def _get_identity_from_encoded_token(self, encoded: str)
⋮----
def _set_company_cookie_overrides(self, company_id: str, cookie_kwargs: dict)
⋮----
# use no default value to allow setting a null domain as well
⋮----
def before_request(self)
⋮----
call = self._create_api_call(request)
load_data_callback = partial(self._load_call_data, req=request)
⋮----
response = redirect(call.result.redirect.url, call.result.redirect.code)
⋮----
headers = None
disable_cache = False
⋮----
# make sure that downloaded files are not cached by the client
disable_cache = True
download_name = call.result.filename
⋮----
simple = unicodedata.normalize("NFKD", download_name)
simple = simple.encode("ascii", "ignore").decode("ascii")
# safe = RFC 5987 attr-char
quoted = urllib.parse.quote(download_name, safe="")
filenames = f"filename={quote_header_value(simple)}; filename*=UTF-8''{quoted}"
⋮----
filenames = f"filename={quote_header_value(download_name)}"
headers = {f"Content-Disposition": f"attachment; {filenames}"}
⋮----
response = Response(
⋮----
kwargs = self._get_cookie_settings(key)
⋮----
# Removing a cookie
⋮----
value = ""
⋮----
# Setting a cookie, let's try to figure out the company
# noinspection PyBroadException
⋮----
company = self._get_identity_from_encoded_token(value).company
⋮----
def after_request(self, response)
⋮----
@staticmethod
    def _apply_multi_dict(body: dict, md: ImmutableMultiDict)
⋮----
def convert_value(v: str)
⋮----
v = (
⋮----
def _update_call_data(self, call, req)
⋮----
"""Use request payload/form to fill call data or batched data"""
⋮----
items = []
⋮----
event = json.loads(line)
⋮----
msg = f"{e} in batch item #{i}"
⋮----
body = (req.get_json(force=True, silent=False) if req.data else None) or {}
⋮----
def _call_or_empty_with_error(self, call, req, msg, code=500, subcode=0)
⋮----
call = call or APICall(
⋮----
def _get_session_auth_cookie(self, req)
⋮----
def _create_api_call(self, req)
⋮----
call = None
⋮----
# Parse the request path
path = req.path
⋮----
path = path[len(self._request_strip_prefix) :]
⋮----
# Resolve authorization: if cookies contain an authorization token, use it as a starting point.
# in any case, request headers always take precedence.
auth_cookie = self._get_session_auth_cookie(req)
headers = (
⋮----
)  # add (possibly override with) the headers
⋮----
# Construct call instance
call = APICall(
⋮----
call = self._call_or_empty_with_error(call, req, ex.args[0], 400)
⋮----
call = self._call_or_empty_with_error(call, req, ex.description, 400)
⋮----
call = self._call_or_empty_with_error(
⋮----
def _load_call_data(self, call: APICall, req)
⋮----
"""Update call data from request"""
</file>

<file path="apiserver/server.py">
app = Flask(__name__, static_url_path="/static")
⋮----
# =================== MAIN =======================
⋮----
p = ArgumentParser(description=__doc__)
⋮----
args = p.parse_args()
⋮----
# logging.info("Starting API Server at %s:%s and env '%s'" % (args.ip, args.port, config.env))
</file>

<file path="apiserver/service_repo/__init__.py">
__all__ = ["ServiceRepo", "APICall", "endpoint"]
⋮----
LegacyEndpointFunc = Callable[[APICall], None]
⋮----
""" Endpoint decorator, used to declare a method as an endpoint handler """
⋮----
def decorator(f: Union[EndpointFunc, LegacyEndpointFunc]) -> EndpointFunc
⋮----
# Backwards compatibility: support endpoints with both old-style signature (call) and new-style signature
#  (call, company, request_model)
func = f
sig = signature(f)
⋮----
last_arg = list(sig.parameters.items())[-1][1]
⋮----
request_data_model = last_arg.annotation
⋮----
# old-style
def adapter(call, *_, **__)
⋮----
func = adapter
</file>

<file path="apiserver/service_repo/apicall.py">
JSON_CONTENT_TYPE = "application/json"
⋮----
@attr.s
class Redirect
⋮----
url = attr.ib(type=str)
code = attr.ib(
⋮----
def empty(self) -> bool
⋮----
class DataContainer(object)
⋮----
""" Data container that supports raw data (dict or a list of batched dicts) and a data model """
⋮----
null_schema_validator: SchemaValidator = SchemaValidator(None)
⋮----
def __init__(self, data=None, batched_data=None)
⋮----
# use setter to properly initialize data
⋮----
@property
    def schema_validator(self)
⋮----
@schema_validator.setter
    def schema_validator(self, value)
⋮----
@property
    def data(self) -> dict
⋮----
@data.setter
    def data(self, value)
⋮----
""" Set the data using a raw dict. If a model cls is defined, validate the raw data """
⋮----
@property
    def batched_data(self)
⋮----
@batched_data.setter
    def batched_data(self, value)
⋮----
@property
    def raw_data(self)
⋮----
@raw_data.setter
    def raw_data(self, value)
⋮----
@property
    def content_type(self)
⋮----
@content_type.setter
    def content_type(self, value)
⋮----
def _update_data_model(self)
⋮----
cls = self.data_model_cls
⋮----
# handle batched items
⋮----
data_model = [cls(**item) for item in self._batched_data]
⋮----
data_model = cls(**self.data)
⋮----
@property
    def data_model(self)
⋮----
# @property
# def get_partial_update(self, data_model_class):
#     return {k: v for k, v in self.data_model.to_struct().iteritems() if k in self.data}
⋮----
@property
    def data_model_for_partial_update(self)
⋮----
"""
        Return only data model fields that we actually passed by the user
        :return:
        """
⋮----
@data_model.setter
    def data_model(self, value)
⋮----
""" Set the data using a model instance. NOTE: batched_data is never updated. """
⋮----
# instance of the data model class - just take it
⋮----
# instance of a subclass of the data model class - create the expected class instance and use the instance
# we received to initialize it
⋮----
@property
    def data_model_cls(self) -> Optional[Type[models.Base]]
⋮----
@data_model_cls.setter
    def data_model_cls(self, value: Type[models.Base])
⋮----
class APICallResult(DataContainer)
⋮----
def get_log_entry(self)
⋮----
def copy_from(self, result)
⋮----
@property
    def msg(self)
⋮----
@msg.setter
    def msg(self, value)
⋮----
@property
    def code(self)
⋮----
@code.setter
    def code(self, value)
⋮----
@property
    def subcode(self)
⋮----
@subcode.setter
    def subcode(self, value)
⋮----
@property
    def traceback(self)
⋮----
@traceback.setter
    def traceback(self, value)
⋮----
@property
    def extra_log(self)
⋮----
""" Extra data to be logged into ES """
⋮----
@extra_log.setter
    def extra_log(self, value)
⋮----
@property
    def filename(self)
⋮----
@filename.setter
    def filename(self, value)
⋮----
@property
    def error_data(self)
⋮----
@error_data.setter
    def error_data(self, value)
⋮----
@property
    def cookies(self)
⋮----
def set_auth_cookie(self, value)
⋮----
@property
    def redirect(self)
⋮----
@redirect.setter
    def redirect(self, value: Union[Redirect, str, Tuple[str, int], list])
⋮----
class MissingIdentity(Exception)
⋮----
def _get_headers(name: str) -> Tuple[str, ...]
⋮----
class APICall(DataContainer)
⋮----
HEADER_AUTHORIZATION = "Authorization"
HEADER_REAL_IP = "X-Real-IP"
HEADER_FORWARDED_FOR = "X-Forwarded-For"
""" Standard headers """
⋮----
_transaction_headers = _get_headers("Trx")
""" Transaction ID """
⋮----
_redacted_headers = {
""" Headers whose value should be redacted. Maps header name to partition char """
⋮----
@property
    def HEADER_TRANSACTION(self)
⋮----
_client_headers = _get_headers("Client")
""" Client """
⋮----
@property
    def HEADER_CLIENT(self)
⋮----
_worker_headers = _get_headers("Worker")
""" Worker (machine) ID """
⋮----
@property
    def HEADER_WORKER(self)
⋮----
_impersonate_as_headers = _get_headers("Impersonate-As")
""" Impersonate as someone else (using his identity and permissions) """
⋮----
@property
    def HEADER_IMPERSONATE_AS(self)
⋮----
_act_as_headers = _get_headers("Act-As")
""" Act as someone else (using his identity, but with your own role and permissions) """
⋮----
@property
    def HEADER_ACT_AS(self)
⋮----
_async_headers = _get_headers("Async")
""" Specifies that this call should be done asynchronously """
⋮----
@property
    def HEADER_ASYNC(self)
⋮----
self._files = files  # currently dic of key to flask's FileStorage)
⋮----
@property
    def files(self)
⋮----
@property
    def id(self)
⋮----
@property
    def requires_authorization(self)
⋮----
@requires_authorization.setter
    def requires_authorization(self, value)
⋮----
@property
    def log_api(self)
⋮----
@log_api.setter
    def log_api(self, value)
⋮----
def assign_new_id(self)
⋮----
def get_header(self, header, default=None)
⋮----
"""
        Get header value
        :param header: Header name options (more than on supported, listed by priority)
        :param default: Default value if no such headers were found
        """
⋮----
def clear_header(self, header)
⋮----
"""
        Clear header value
        :param header: Header name options (more than on supported, all will be cleared)
        """
⋮----
def set_header(self, header, value)
⋮----
"""
        Set header value
        :param header: header name (if a list is provided, first item is used)
        :param value: Value to set
        :return:
        """
⋮----
@property
    def real_ip(self)
⋮----
""" Obtain visitor's IP address """
⋮----
@property
    def failed(self)
⋮----
@property
    def duration(self)
⋮----
@property
    def endpoint_name(self)
⋮----
@property
    def requested_endpoint_version(self) -> PartialVersion
⋮----
@property
    def auth(self)
⋮----
""" Authenticated payload (Token or Basic) """
⋮----
@auth.setter
    def auth(self, value)
⋮----
@property
    def impersonation_headers(self)
⋮----
@property
    def impersonate_as(self)
⋮----
@property
    def act_as(self)
⋮----
@property
    def impersonation(self)
⋮----
@impersonation.setter
    def impersonation(self, value)
⋮----
@property
    def identity(self)
⋮----
@property
    def actual_endpoint_version(self)
⋮----
@actual_endpoint_version.setter
    def actual_endpoint_version(self, value)
⋮----
@property
    def headers(self)
⋮----
@property
    def kpis(self)
⋮----
"""
        Key Performance Indicators, holding things like number of returned frames/rois, etc.
        :return:
        """
⋮----
@property
    def trx(self)
⋮----
@trx.setter
    def trx(self, value)
⋮----
@property
    def client(self)
⋮----
@property
    def worker(self)
⋮----
def get_worker(self, default=None)
⋮----
@property
    def authorization(self)
⋮----
""" Call authorization data used to authenticate the call """
⋮----
@property
    def result(self)
⋮----
@property
    def exec_async(self)
⋮----
@exec_async.setter
    def exec_async(self, value)
⋮----
@property
    def host(self)
⋮----
@property
    def auth_cookie(self)
⋮----
@property
    def json_flags(self)
⋮----
@property
    def extra_meta_fields(self)
⋮----
def mark_end(self)
⋮----
def get_response(self, include_stack: bool = None) -> Tuple[Union[dict, str], str]
⋮----
"""
        Get the response for this call.
        :param include_stack: If True, stack trace stored in this call's result should
        be included in the response (default follows configuration)
        :return: Response data (encoded according to self.content_type) and the data's content type
        """
include_stack = (
⋮----
def make_version_number(version: PartialVersion) -> Union[None, float, str]
⋮----
"""
            Client versions <=2.0 expect expect endpoint versions in float format, otherwise throwing an exception
            """
⋮----
# endpoint returned raw data and no error was detected, return raw data, no fancy dicts
⋮----
res = {
⋮----
func = (
res = func(res, **(self._json_flags or {}))
⋮----
# JSON serialization may fail, probably problem with data or error_data so pop it and try again
⋮----
msg = "Error serializing response data: " + str(ex)
⋮----
tb = format_exc() if include_stack else None
⋮----
def get_redacted_headers(self, fields=None)
⋮----
headers = (
⋮----
# We won't log the authorization header if call shouldn't be authorized, or if it was successfully
#  authorized. This means we'll only log authorization header for calls that failed to authorize (hopefully
#  this will allow us to debug authorization errors).
⋮----
prefix = sep = ""
redact = headers[header]
</file>

<file path="apiserver/service_repo/auth/__init__.py">

</file>

<file path="apiserver/service_repo/auth/auth.py">
log = config.logger(__file__)
entity_keys = set(get_options(Entities))
verify_user_tokens = config.get("apiserver.auth.verify_user_tokens", True)
_revoked_tokens_key = "revoked_tokens"
redis = redman.connection("apiserver")
⋮----
def get_auth_func(auth_type)
⋮----
def authorize_token(jwt_token, service, action, call)
⋮----
"""Validate token against service/endpoint and requests data (dicts).
    Returns a parsed token object (auth payload)
    """
call_info = {"ip": call.real_ip}
⋮----
def log_error(msg)
⋮----
info = ", ".join(f"{k}={v}" for k, v in call_info.items())
⋮----
token = Token.from_encoded_token(jwt_token)
⋮----
def authorize_credentials(auth_data, service, action, call)
⋮----
"""Validate credentials against service/action and request data (dicts).
    Returns a new basic object (auth payload)
    """
⋮----
query = Q(credentials__match=Credentials(key=access_key, secret=secret_key))
⋮----
fixed_user = None
⋮----
fixed_user = FixedUser.get_by_username(access_key)
⋮----
query = Q(id=fixed_user.id)
⋮----
user = User.objects(query).first()
⋮----
# In case these are proper credentials, update last used time
⋮----
company = Company.objects(id=user.company).only("id", "name").first()
⋮----
identity = Identity(
⋮----
basic = Basic(user_key=access_key, identity=identity)
⋮----
def authorize_impersonation(user, identity, service, action, call)
⋮----
""" Returns a new basic object (auth payload)"""
⋮----
def compare_secret_key_hash(secret_key: str, hashed_secret: str) -> bool
⋮----
"""
    Compare hash for the passed secret key with the passed hash
    :return: True if equal. Otherwise False
    """
⋮----
def is_token_revoked(token: Token) -> bool
⋮----
def revoke_auth_token(token: Token)
⋮----
timestamp_now = int(time())
expiration_timestamp = token.exp
⋮----
expiration_timestamp = timestamp_now + Token.default_expiration_sec
</file>

<file path="apiserver/service_repo/auth/dictable.py">
class Dictable(object)
⋮----
_cached_props = None
⋮----
@classmethod
    def _get_cached_props(cls)
⋮----
props = set()
⋮----
def to_dict(self, **extra)
⋮----
props = self._get_cached_props()
d = {k: getattr(self, k) for k in props if getattr(self, k)}
res = {k: (v.to_dict() if isinstance(v, Dictable) else v) for k, v in d.items()}
⋮----
# add the extra items to our result, make sure not to overwrite existing properties (claims etc)
⋮----
@classmethod
    def from_dict(cls, d)
</file>

<file path="apiserver/service_repo/auth/fixed_user.py">
class FixedUsersError(Exception)
⋮----
@attr.s(auto_attribs=True)
class FixedUser
⋮----
username: str
password: str
name: str
company: str = get_default_company()
id: str = attr.field(init=False)
email: str = attr.field(init=False)
role: str = attr.field(init=False)
⋮----
is_guest: bool = False
⋮----
def __attrs_post_init__(self)
⋮----
@classmethod
    def enabled(cls)
⋮----
@classmethod
    def guest_enabled(cls)
⋮----
@classmethod
    def pass_hashed(cls)
⋮----
@classmethod
    def validate(cls)
⋮----
users = cls.from_config()
⋮----
# @lru_cache()
def from_config(cls) -> Sequence["FixedUser"]
⋮----
users = [
⋮----
@classmethod
@lru_cache()
    def get_by_username(cls, username) -> "FixedUser"
⋮----
@classmethod
@lru_cache()
    def is_guest_endpoint(cls, service, action)
⋮----
"""
        Validate a potential guest user,
        This method will verify the user is indeed the guest user,
         and that the guest user may access the service/action using its username/password
        """
⋮----
@classmethod
    def get_guest_user(cls) -> Optional["FixedUser"]
⋮----
def __hash__(self)
</file>

<file path="apiserver/service_repo/auth/identity.py">
class Identity(Dictable)
⋮----
def __init__(self, user, company, role, user_name=None, company_name=None)
⋮----
@property
    def user(self)
⋮----
@property
    def company(self)
⋮----
@property
    def role(self)
⋮----
@property
    def user_name(self)
⋮----
@property
    def company_name(self)
</file>

<file path="apiserver/service_repo/auth/payload/__init__.py">

</file>

<file path="apiserver/service_repo/auth/payload/auth_type.py">
class AuthType(object)
⋮----
basic = 'Basic'
bearer_token = 'Bearer'
</file>

<file path="apiserver/service_repo/auth/payload/basic.py">
class Basic(Payload)
⋮----
def __init__(self, user_key, identity=None, entities=None, **_)
⋮----
@property
    def user_key(self)
⋮----
def get_log_entry(self)
⋮----
d = super(Basic, self).get_log_entry()
</file>

<file path="apiserver/service_repo/auth/payload/payload.py">
class Payload(Dictable)
⋮----
def __init__(self, auth_type, identity=None, entities=None)
⋮----
@property
    def auth_type(self)
⋮----
@property
    def identity(self)
⋮----
@identity.setter
    def identity(self, value)
⋮----
value = Identity(**value)
⋮----
@property
    def entities(self)
⋮----
@entities.setter
    def entities(self, value)
⋮----
def get_log_entry(self)
⋮----
def validate_entities(self, **entities)
⋮----
""" Validate entities. key/value represents entity_name/entity_id(s) """
⋮----
constraints = self.entities.get(entity_name)
ids = set(entity_id if isinstance(entity_id, (tuple, list)) else (entity_id,))
</file>

<file path="apiserver/service_repo/auth/payload/token.py">
token_secret = config.get("secure.auth.token_secret")
⋮----
log = config.logger(__file__)
⋮----
class Token(Payload)
⋮----
default_expiration_sec = config.get("apiserver.auth.default_expiration_sec")
⋮----
@property
    def session_id(self)
⋮----
@property
    def env(self)
⋮----
@property
    def exp(self)
⋮----
@exp.setter
    def exp(self, value)
⋮----
@property
    def iat(self)
⋮----
@iat.setter
    def iat(self, value)
⋮----
@property
    def nbf(self)
⋮----
@nbf.setter
    def nbf(self, value)
⋮----
def get_log_entry(self)
⋮----
d = super(Token, self).get_log_entry()
⋮----
def encode(self, **extra_payload)
⋮----
payload = self.to_dict(**extra_payload)
⋮----
@classmethod
    def decode(cls, encoded_token, verify=True)
⋮----
options = (
⋮----
@classmethod
    def from_encoded_token(cls, encoded_token, verify=True)
⋮----
decoded = cls.decode(encoded_token, verify=verify)
⋮----
token = Token.from_dict(decoded)
⋮----
# limit expiration time for all roles but an internal service
expiration_sec = expiration_sec or cls.default_expiration_sec
⋮----
now = datetime.utcnow()
session_id = uuid4().hex
⋮----
token = cls(
⋮----
# add 'expiration' claim
⋮----
@classmethod
    def decode_identity(cls, encoded_token)
⋮----
# noinspection PyBroadException
⋮----
decoded = cls.decode(encoded_token, verify=False)
</file>

<file path="apiserver/service_repo/auth/utils.py">
def get_random_string(length)
⋮----
"""
    Create a random crypto-safe sequence of 'length' or more characters
    Possible characters: alphanumeric, '-' and '_'
    Make sure that it starts from alphanumeric for better compatibility with yaml files
    """
token = secrets.token_urlsafe(length)
⋮----
"""
    Create a random client id composed of 'length' upper case characters or digits
    """
⋮----
def get_secret_key(length: int = 50) -> str
⋮----
"""
    Create a random secret key
    """
</file>

<file path="apiserver/service_repo/endpoint.py">
EndpointFunc = Callable[[APICall, Text, models.Base], Any]
⋮----
class Endpoint(object)
⋮----
_endpoint_config_cache = {}
"""
    Endpoints configuration cache, in the format of {full endpoint name: dict}
    """
⋮----
"""
        Endpoint configuration
        :param name: full endpoint name
        :param func: endpoint implementation
        :param min_version: minimum supported version
        :param required_fields: required request fields, can not be used with validate_schema
        :param request_data_model: request jsonschema model, will be validated if validate_schema=False
        :param response_data_model: response jsonschema model, will be validated if validate_schema=False
        :param validate_schema: whether request and response schema should be validated
        """
⋮----
endpoint = self.endpoint_group.get_for_version(self.min_version)
request_schema = endpoint.request_schema
response_schema = endpoint.response_schema
⋮----
request_schema = None
response_schema = None
⋮----
def __repr__(self)
⋮----
def to_dict(self)
⋮----
"""
        Used by `server.endpoints` endpoint.
        Provided endpoints and their schemas on a best-effort basis.
        """
d = {
⋮----
def safe_to_json_schema(data_model: models.Base)
⋮----
"""
            Provided data_model schema if available
            """
⋮----
res = data_model.to_json_schema()
⋮----
def visit(path, key, value)
⋮----
value = str(value)
⋮----
@property
    def authorize(self)
⋮----
@property
    def allow_roles(self)
⋮----
def allows(self, role)
⋮----
@property
    def is_internal(self)
</file>

<file path="apiserver/service_repo/errors.py">
class PathParsingError(Exception)
⋮----
def __init__(self, msg)
⋮----
class MalformedPathError(PathParsingError)
⋮----
class InvalidVersionError(PathParsingError)
⋮----
class CallParsingError(Exception)
⋮----
class CallFailedError(Exception)
</file>

<file path="apiserver/service_repo/schema_validator.py">
log = config.logger(__file__)
⋮----
@attr.s(auto_attribs=True, cmp=False)
class FastValidationError(Exception)
⋮----
error: fastjsonschema.JsonSchemaException
data: dict
⋮----
class SchemaValidator
⋮----
def __init__(self, schema: Optional[dict])
⋮----
"""
        Utility for different schema validation strategies
        :param schema: jsonschema to validate against
        """
⋮----
@property
    def enabled(self) -> bool
⋮----
def fast_validate(self, data: dict) -> None
⋮----
"""
        Perform a quick validate with laconic error messages
        :param data: data to validate
        :raises: fastjsonschema.JsonSchemaException
        """
⋮----
data = remap(data, lambda path, key, value: value is not None)
⋮----
def detailed_validate(self, data: dict) -> None
⋮----
"""
        Perform a slow validate with detailed error messages
        :param data: data to validate
        :raises: errors.bad_request.ValidationError
        """
</file>

<file path="apiserver/service_repo/service_repo.py">
log = config.logger(__file__)
⋮----
class ServiceRepo(object)
⋮----
_endpoints: MutableMapping[str, List[Endpoint]] = {}
""" 
    Registered endpoints, in the format of {endpoint_name: Endpoint)}
    the list of endpoints is sorted by min_version
    """
⋮----
_version_required = config.get("apiserver.version.required")
""" If version is required, parsing will fail for endpoint paths that do not contain a valid version """
⋮----
_check_max_version = config.get("apiserver.version.check_max_version")
"""If the check is set, parsing will fail for endpoint request with the version that is grater than the current 
    maximum """
⋮----
_max_version = PartialVersion("2.34")
""" Maximum version number (the highest min_version value across all endpoints) """
⋮----
_endpoint_exp = (
""" 
        Endpoint structure expressions. We have two expressions, one with optional version part.
        Constraints for the first (strict) expression:
        1. May start with a leading '/'
        2. Followed by a version number (int or float) preceded by a leading 'v'
        3. Followed by a '/'
        4. Followed by a service name, which must start with an english letter (lower or upper case) or underscore,
            and followed by any number of alphanumeric or underscore characters
        5. Followed by a '.'  
        6. Followed by an action name, which must start with an english letter (lower or upper case) or underscore,
            and followed by any number of alphanumeric or underscore characters  
        7. May end with a leading '/' 
        
        The second (optional version) expression does not require steps 2 and 3. 
    """
⋮----
_return_stack = config.get("apiserver.return_stack")
""" return stack trace on error """
⋮----
_return_stack_on_code = parse_return_stack_on_code(
""" if 'return_stack' is true and error contains a return code, return stack trace only for these error codes """
⋮----
_credentials = config["secure.credentials.apiserver"]
""" Api Server credentials used for intra-service communication """
⋮----
_token = None
""" Token for internal calls """
⋮----
sub_module = None
⋮----
# leave no trace of the 'sub_module' local
⋮----
@classmethod
    def load(cls, root_module="services")
⋮----
@classmethod
    def register(cls, endpoint: Endpoint)
⋮----
@classmethod
    def endpoint_names(cls)
⋮----
@classmethod
    def endpoints_summary(cls)
⋮----
@classmethod
    def max_endpoint_version(cls) -> PartialVersion
⋮----
@classmethod
    def _get_endpoint(cls, name, version) -> Optional[Endpoint]
⋮----
versions = cls._endpoints.get(name)
⋮----
# no appropriate version found
⋮----
@classmethod
    def _resolve_endpoint_from_call(cls, call: APICall) -> Optional[Endpoint]
⋮----
endpoint = cls._get_endpoint(
⋮----
@classmethod
    def parse_endpoint_path(cls, path: str) -> Tuple[PartialVersion, str]
⋮----
""" Parse endpoint version, service and action from request path. """
m = cls._endpoint_exp.match(path)
⋮----
endpoint_name = m.group("endpoint_name")
version = m.group("endpoint_version")
⋮----
# If endpoint is available, use the max version
version = cls._max_version
⋮----
version = PartialVersion(version)
⋮----
@classmethod
    def _should_return_stack(cls, code: int, subcode: int) -> bool
⋮----
# Code in dict, but no subcode. We'll allow it.
⋮----
subcode_list = cls._return_stack_on_code.get(code)
⋮----
# if the code is there but we don't have any subcode list, always return stack
⋮----
authorize = endpoint and endpoint.authorize
⋮----
identity = cls._get_identity(call, endpoint=endpoint, ignore_error=ignore_error)
⋮----
@classmethod
    def handle_call(cls, call: APICall, load_data_callback: Callable = None)
⋮----
company = None
⋮----
endpoint = cls._resolve_endpoint_from_call(call)
⋮----
# if impersonating, validate role again
⋮----
# In case call does not require authorization, parsing the identity.company might raise an exception
company = cls._get_company(call, endpoint)
⋮----
ret = endpoint.func(call, company, call.data_model)
⋮----
# allow endpoints to return dict or model (instead of setting them explicitly on the call)
⋮----
# report stack trace only for gene
include_stack = cls._should_return_stack(
⋮----
# Do nothing, let 'finally' wrap up
⋮----
console_msg = f"Returned {call.result.code} for {call.endpoint_name} in {call.duration}ms"
⋮----
console_msg = f"{console_msg}, msg={call.result.msg}"
</file>

<file path="apiserver/service_repo/util.py">
def get_local_addr()
⋮----
""" Get the local IP address (that isn't localhost) """
⋮----
def resolve_addr(addr)
⋮----
""" Resolve address (IP string of host name) into an IP string. """
⋮----
def parse_return_stack_on_code(codes)
⋮----
def parse(e)
⋮----
subcodes = [subcodes]
</file>

<file path="apiserver/service_repo/validators.py">
log = config.logger(__file__)
⋮----
def validate_data(call: APICall, endpoint: Endpoint)
⋮----
""" Perform all required call/endpoint validation, update call result appropriately """
⋮----
# todo: remove validate_required_fields once all endpoints have json schema
⋮----
# set models. models will be validated automatically
⋮----
def validate_role(endpoint, call)
⋮----
def validate_auth(endpoint: Endpoint, call: "APICall")
⋮----
"""
    Validate authorization for this endpoint and call.
    If authentication has occurred, the call is updated with the authentication results.
    For the endpoints with authorize==False the validation is not performed to improve performance
    For the endpoints with authorize==True the validation should pass otherwise exception will be thrown
    For the endpoints with authorize==None the validation will be tried, but it does not have to succeed
    """
⋮----
# No auth data. Invalid if we need to authorize and valid otherwise
⋮----
# prepare arguments for validation
⋮----
# noinspection PyBroadException
⋮----
auth = call.authorization
⋮----
authorize_func = get_auth_func(auth_type)
⋮----
# if endpoint requires authorization, re-raise exception
⋮----
def validate_impersonation(endpoint, call)
⋮----
""" Validate impersonation headers and set impersonated identity and authorization data accordingly.
        :return: True if impersonating, False otherwise
    """
⋮----
act_as = call.act_as
impersonate_as = call.impersonate_as
⋮----
identity = call.auth.identity
⋮----
# verify this user is allowed to impersonate at all
⋮----
# get the impersonated user's info
user_id = act_as or impersonate_as
⋮----
# only root is allowed to impersonate users in other companies
query = dict(id=user_id)
⋮----
query = dict(id=user_id, company=identity.company)
user = User.objects(**query).first()
⋮----
company = Company.objects(id=user.company).only("name").first()
⋮----
# create impersonation payload
⋮----
# act as a user, using your own role and permissions
⋮----
# impersonate as a user, using his own identity and permissions (required additional validation to verify
# impersonated user is allowed to access the endpoint)
⋮----
def validate_required_fields(endpoint, call)
⋮----
missing = [val for val in endpoint.required_fields if val not in call.data]
</file>

<file path="apiserver/services_schema.py">
schema = SchemaReader().get_schema()
</file>

<file path="apiserver/services/__init__.py">

</file>

<file path="apiserver/services/auth.py">
log = config.logger(__file__)
⋮----
def login(call: APICall, *_, **__)
⋮----
"""Generates a token based on the authenticated user (intended for use with credentials)"""
⋮----
# Add authorization cookie
⋮----
@endpoint("auth.logout", min_version="2.2")
def logout(call: APICall, *_, **__)
⋮----
def get_token_for_user(call: APICall, _: str, request: GetTokenForUserRequest)
⋮----
"""Generates a token based on a requested user and company. INTERNAL."""
⋮----
def validate_token_endpoint(call: APICall, _, __)
⋮----
"""Validate a token and return identity if valid. INTERNAL."""
⋮----
# if invalid, decoding will fail
token = Token.from_encoded_token(call.data_model.token)
⋮----
def create_user(call: APICall, _, request: CreateUserRequest)
⋮----
"""Create a user from. INTERNAL."""
⋮----
user_id = AuthBLL.create_user(request=request, call=call)
⋮----
@endpoint("auth.create_credentials", response_data_model=CreateCredentialsResponse)
def create_credentials(call: APICall, _, request: CreateCredentialsRequest)
⋮----
credentials = AuthBLL.create_credentials(
⋮----
@endpoint("auth.edit_credentials")
def edit_credentials(call: APICall, company_id: str, request: EditCredentialsRequest)
⋮----
identity = call.identity
access_key = request.access_key
⋮----
updated = User.objects(
⋮----
def revoke_credentials(call: APICall, _, __)
⋮----
access_key = call.data_model.access_key
⋮----
query = dict(
updated = User.objects(**query).update_one(pull__credentials__key=access_key)
⋮----
@endpoint("auth.get_credentials", response_data_model=GetCredentialsResponse)
def get_credentials(call: APICall, _, __)
⋮----
query = dict(id=identity.user, company=identity.company)
user = User.objects(**query).first()
⋮----
# we return ONLY the key IDs, never the secrets (want a secret? create new credentials)
⋮----
def update(call: APICall, company_id: str, _)
⋮----
fields = {
⋮----
result = User.objects(company=company_id, id=call.data_model.user).update(
⋮----
@endpoint("auth.fixed_users_mode")
def fixed_users_mode(call: APICall, *_, **__)
⋮----
server_errors = {
⋮----
data = {
guest_user = FixedUser.get_guest_user()
⋮----
def _is_protected_user(user_id)
</file>

<file path="apiserver/services/debug.py">
@endpoint("debug.ping")
def ping(call: APICall, _, __)
⋮----
res = {"msg": "ClearML server"}
</file>

<file path="apiserver/services/events.py">
task_bll = TaskBLL()
event_bll = EventBLL()
model_bll = ModelBLL()
⋮----
@endpoint("events.add")
def add(call: APICall, company_id, _)
⋮----
data = call.data.copy()
⋮----
@endpoint("events.add_batch")
def add_batch(call: APICall, company_id, _)
⋮----
events = call.batched_data
⋮----
@endpoint("events.get_task_log")
def get_task_log_v1_5(call, company_id, request: LegacyLogEventsRequest)
⋮----
task_id = request.task
task = task_bll.assert_exists(
order = request.order
scroll_id = request.scroll_id
batch_size = request.batch_size
⋮----
@endpoint("events.get_task_log", min_version="1.7")
def get_task_log_v1_7(call, company_id, request: LegacyLogEventsRequest)
⋮----
from_ = call.data.get("from") or "head"
⋮----
scroll_order = "asc" if (from_ == "head") else "desc"
⋮----
events = events[::-1]
⋮----
@endpoint("events.get_task_log", min_version="2.9", request_data_model=LogEventsRequest)
def get_task_log(call, company_id, request: LogEventsRequest)
⋮----
res = event_bll.events_iterator.get_task_events(
⋮----
@endpoint("events.download_task_log")
def download_task_log(call, company_id, request: TaskRequest)
⋮----
line_type = call.data.get("line_type", "json").lower()
line_format = str(call.data.get("line_format", "{asctime} {worker} {level} {msg}"))
⋮----
is_json = line_type == "json"
⋮----
# validate line format placeholders
valid_task_log_fields = {"asctime", "timestamp", "level", "worker", "msg"}
⋮----
invalid_placeholders = set()
⋮----
# make sure line_format has a trailing newline
line_format = line_format.rstrip("\n") + "\n"
⋮----
def generate()
⋮----
scroll_id = None
batch_size = 1000
⋮----
model_events = request.model_events
task_or_model = _assert_task_or_model_exists(
⋮----
# todo: !!! currently returning 10,000 records. should decide on a better way to control it
⋮----
metric = request.metric
variant = request.variant
⋮----
class GetTaskEventsScroll(Scroll)
⋮----
from_key_value = jsonmodels.fields.StringField()
total = jsonmodels.fields.IntField()
request: TaskEventsRequest = jsonmodels.fields.EmbeddedField(TaskEventsRequest)
⋮----
@endpoint("events.get_task_events", request_data_model=TaskEventsRequest)
def get_task_events(_, company_id, request: TaskEventsRequest)
⋮----
key = ScalarKeyEnum.iter
scalar_key = ScalarKey.resolve(key)
⋮----
from_key_value = None if (request.order == LogOrderEnum.desc) else 0
total = None
⋮----
scroll = GetTaskEventsScroll.from_scroll_id(request.scroll_id)
⋮----
from_key_value = scalar_key.cast_value(scroll.from_key_value)
total = scroll.total
⋮----
request = scroll.request
⋮----
navigate_earlier = request.order == LogOrderEnum.desc
metric_variants = _get_metric_variants_from_request(request.metrics)
⋮----
total = event_bll.events_iterator.count_task_events(
⋮----
batch_size = min(
⋮----
scroll = GetTaskEventsScroll(
⋮----
@endpoint("events.get_scalar_metric_data")
def get_scalar_metric_data(call, company_id, request: GetScalarMetricDataRequest)
⋮----
no_scroll = request.no_scroll
⋮----
result = event_bll.get_task_events(
⋮----
@endpoint("events.get_task_latest_scalar_values")
def get_task_latest_scalar_values(call, company_id, request: TaskRequest)
⋮----
index_company = task.get_index_company()
⋮----
last_iters = event_bll.get_last_iters(
⋮----
# todo: should not repeat iter (x-axis) for each metric/variant, JS client should get raw data and fill gaps if needed
⋮----
metrics = event_bll.metrics.get_scalar_metrics_average_per_iter(
⋮----
"""
    Returns lists of tasks grouped by company
    """
tasks_or_models = _assert_task_or_model_exists(
⋮----
unique_ids = set(task_ids)
⋮----
invalid = tuple(unique_ids - {t.id for t in tasks_or_models})
error_cls = (
⋮----
task_ids = request.tasks
⋮----
task_ids = [s.strip() for s in task_ids.split(",")]
⋮----
task_names = {
⋮----
companies = _get_task_or_model_index_companies(
⋮----
@endpoint("events.get_multi_task_plots")
def get_multi_task_plots_v1_7(call, company_id, request: LegacyMultiTaskEventsRequest)
⋮----
iters = request.iters
⋮----
companies = _get_task_or_model_index_companies(company_id, task_ids)
⋮----
# Get last 10K events by iteration and group them by unique metric+variant, returning top events for combination
⋮----
return_events = _get_top_iter_unique_events_per_task(
⋮----
metrics = _get_metric_variants_from_request(request_metrics)
⋮----
@endpoint("events.get_multi_task_plots", min_version="1.8")
def get_multi_task_plots(call, company_id, request: MultiTaskPlotsRequest)
⋮----
@endpoint("events.get_task_plots")
def get_task_plots_v1_7(call, company_id, request: LegacyMetricEventsRequest)
⋮----
# events, next_scroll_id, total_events = event_bll.get_task_events(
#     company, task_id,
#     event_type="plot",
#     sort=[{"iter": {"order": "desc"}}],
#     last_iter_count=iters,
#     scroll_id=scroll_id)
⋮----
# get last 10K events by iteration and group them by unique metric+variant, returning top events for combination
⋮----
return_events = _get_top_iter_unique_events(result.events, max_iters=iters)
⋮----
def get_task_plots(call, company_id, request: TaskPlotsRequest)
⋮----
result = event_bll.get_task_plots(
⋮----
return_events = result.events
⋮----
def _task_metrics_dict_from_request(req_metrics: Sequence[TaskMetric]) -> dict
⋮----
task_metrics = defaultdict(dict)
⋮----
def _get_metrics_response(metric_events: Sequence[tuple]) -> Sequence[MetricEvents]
⋮----
def task_plots(call, company_id, request: MetricEventsRequest)
⋮----
task_metrics = _task_metrics_dict_from_request(request.metrics)
task_ids = list(task_metrics)
task_or_models = _assert_task_or_model_exists(
result = event_bll.plots_iterator.get_task_events(
⋮----
@endpoint("events.debug_images")
def get_debug_images_v1_7(call, company_id, request: LegacyMetricEventsRequest)
⋮----
#     event_type="training_debug_image",
⋮----
@endpoint("events.debug_images", min_version="1.8")
def get_debug_images_v1_8(call, company_id, request: LegacyMetricEventsRequest)
⋮----
tasks_or_model = _assert_task_or_model_exists(
⋮----
def get_debug_images(call, company_id, request: MetricEventsRequest)
⋮----
result = event_bll.debug_images_iterator.get_task_events(
⋮----
def get_debug_image_sample(call, company_id, request: GetVariantSampleRequest)
⋮----
res = event_bll.debug_image_sample_history.get_sample_for_variant(
⋮----
def next_debug_image_sample(call, company_id, request: NextHistorySampleRequest)
⋮----
res = event_bll.debug_image_sample_history.get_next_sample(
⋮----
def get_plot_sample(call, company_id, request: GetMetricSamplesRequest)
⋮----
res = event_bll.plot_sample_history.get_samples_for_metric(
⋮----
def next_plot_sample(call, company_id, request: NextHistorySampleRequest)
⋮----
res = event_bll.plot_sample_history.get_next_sample(
⋮----
@endpoint("events.get_task_metrics", request_data_model=TaskMetricsRequest)
def get_task_metrics(call: APICall, company_id, request: TaskMetricsRequest)
⋮----
res = event_bll.metrics.get_task_metrics(
⋮----
@endpoint("events.get_multi_task_metrics")
def get_multi_task_metrics(call: APICall, company_id, request: MultiTaskMetricsRequest)
⋮----
metrics = event_bll.metrics.get_multi_task_metrics(
res = [
⋮----
task = get_task_with_write_access(
⋮----
@endpoint("events.delete_for_task")
def delete_for_task(call, company_id, request: TaskRequest)
⋮----
allow_locked = call.data.get("allow_locked", False)
⋮----
model = model_bll.assert_exists(company_id, model_id, only=("id", "ready"))[0]
⋮----
@endpoint("events.delete_for_model")
def delete_for_model(call: APICall, company_id: str, request: ModelRequest)
⋮----
model_id = request.model
⋮----
@endpoint("events.clear_task_log")
def clear_task_log(call: APICall, company_id: str, request: ClearTaskLogRequest)
⋮----
key_fields = ("metric", "variant", "task")
unique_events = itertools.chain.from_iterable(
⋮----
def collect(evs, fields)
⋮----
evs = list(evs)
⋮----
collect_fields = ("metric", "variant", "task", "iter")
⋮----
def _get_top_iter_unique_events(events, max_iters)
⋮----
top_unique_events = defaultdict(lambda: [])
⋮----
key = ev.get("metric", "") + ev.get("variant", "")
evs = top_unique_events[key]
⋮----
unique_events = list(
⋮----
class ScalarMetricsIterRawScroll(Scroll)
⋮----
request: ScalarMetricsIterRawRequest = jsonmodels.fields.EmbeddedField(
⋮----
key = request.key or ScalarKeyEnum.iter
⋮----
from_key_value = None
⋮----
scroll = ScalarMetricsIterRawScroll.from_scroll_id(request.scroll_id)
⋮----
metric_variants = _get_metric_variants_from_request([request.metric])
⋮----
events = []
⋮----
from_key_value = str(events[-1][scalar_key.field])
⋮----
key = str(key)
variants = {
⋮----
scroll = ScalarMetricsIterRawScroll(
⋮----
@endpoint("events.clear_scroll", min_version="2.18")
def clear_scroll(_, __, request: ClearScrollRequest)
</file>

<file path="apiserver/services/login/__init__.py">
@endpoint("login.supported_modes", response_data_model=GetSupportedModesResponse)
def supported_modes(call: APICall, _, __: GetSupportedModesRequest)
⋮----
guest_user = FixedUser.get_guest_user()
⋮----
guest = BasicGuestMode(
⋮----
guest = BasicGuestMode()
⋮----
@endpoint("login.logout", min_version="2.13")
def logout(call: APICall, _, __)
</file>

<file path="apiserver/services/models.py">
log = config.logger(__file__)
org_bll = OrgBLL()
project_bll = ProjectBLL()
event_bll = EventBLL()
⋮----
def conform_model_data(call: APICall, model_data: Union[Sequence[dict], dict])
⋮----
@endpoint("models.get_by_id")
def get_by_id(call: APICall, company_id, request: ModelRequest)
⋮----
model_id = request.model
call_data = Metadata.escape_query_parameters(call.data)
models = Model.get_many(
⋮----
@endpoint("models.get_by_task_id")
def get_by_task_id(call: APICall, company_id, request: TaskRequest)
⋮----
task_id = request.task
⋮----
query = dict(id=task_id, company=company_id)
task = Task.get(_only=["models"], **query)
⋮----
model_id = task.models.output[-1].model
model = Model.objects(
⋮----
model_dict = model.to_proper_dict()
⋮----
@endpoint("models.get_all_ex", request_data_model=ModelsGetRequest)
def get_all_ex(call: APICall, company_id, request: ModelsGetRequest)
⋮----
ret_params = {}
models = Model.get_many_with_join(
⋮----
model_ids = {model["id"] for model in models}
stats = ModelBLL.get_model_stats(
⋮----
@endpoint("models.get_by_id_ex", required_fields=["id"])
def get_by_id_ex(call: APICall, company_id, _)
⋮----
@endpoint("models.get_all")
def get_all(call: APICall, company_id, _)
⋮----
@endpoint("models.get_frameworks", request_data_model=GetFrameworksRequest)
def get_frameworks(call: APICall, company_id, request: GetFrameworksRequest)
⋮----
create_fields = {
⋮----
last_update_fields = (
⋮----
def parse_model_fields(call, valid_fields)
⋮----
task_id = call.data.get("task")
⋮----
fields = parse_from_call(call.data, valid_fields, Model.get_fields())
⋮----
def _update_cached_tags(company: str, project: str, fields: dict)
⋮----
def _reset_cached_tags(company: str, projects: Sequence[str])
⋮----
@endpoint("models.update_for_task")
def update_for_task(call: APICall, company_id, request: UpdateForTaskRequest)
⋮----
uri = request.uri
iteration = request.iteration
override_model_id = request.override_model_id
⋮----
task = get_task_with_write_access(
⋮----
allowed_states = [TaskStatus.created, TaskStatus.in_progress]
⋮----
model = ModelBLL.get_company_model_by_id(
⋮----
# use task name if name not provided
⋮----
# model exists, update
⋮----
res = _update_model(call, company_id, model_id=model_id).to_struct()
⋮----
# new model, create
fields = parse_model_fields(call, create_fields)
⋮----
# create and save model
now = datetime.utcnow()
model = Model(
⋮----
def create(call: APICall, company_id, req_model: CreateModelRequest)
⋮----
company_id = ""
⋮----
project = req_model.project
⋮----
task = req_model.task
req_data = req_model.to_struct()
⋮----
fields = filter_fields(Model, req_data)
⋮----
def prepare_update_fields(call, company_id, fields: dict)
⋮----
fields = fields.copy()
⋮----
# clear UI cache if URI is provided (model updated)
⋮----
labels = fields["labels"]
⋮----
def find_other_types(iterable, type_)
⋮----
res = [x for x in iterable if not isinstance(x, type_)]
⋮----
# Un-hashable, probably
⋮----
invalid_keys = find_other_types(labels.keys(), str)
⋮----
invalid_values = find_other_types(labels.values(), int)
⋮----
def validate_task(company_id: str, identity: Identity, fields: dict)
⋮----
task_id = fields["task"]
⋮----
@endpoint("models.edit", response_data_model=UpdateResponse)
def edit(call: APICall, company_id, request: UpdateModelRequest)
⋮----
model = ModelBLL.get_company_model_by_id(company_id=company_id, model_id=model_id)
⋮----
fields = prepare_update_fields(call, company_id, fields)
⋮----
field = getattr(model, key, None)
value = fields[key]
⋮----
d = field.to_mongo(use_db_field=False).to_dict()
⋮----
task_id = model.task or fields.get("task")
⋮----
updated = model.update(upsert=False, **fields)
⋮----
new_project = fields.get("project", model.project)
⋮----
def _update_model(call: APICall, company_id, model_id)
⋮----
data = prepare_update_fields(call, company_id, call.data)
task_id = data.get("task")
iteration = data.get("iteration")
⋮----
new_project = updated_fields.get("project", model.project)
⋮----
@endpoint("models.update", response_data_model=UpdateResponse)
def update(call, company_id, request: UpdateModelRequest)
⋮----
def set_ready(call: APICall, company_id: str, request: PublishModelRequest)
⋮----
def publish_many(call: APICall, company_id, request: ModelsPublishManyRequest)
⋮----
model_ids = [m.id for m in models]
delete_external_artifacts = delete_external_artifacts and config.get(
⋮----
model_urls = {m.uri for m in models if m.uri}
⋮----
event_urls = delete_task_events_and_collect_urls(
⋮----
@endpoint("models.delete", request_data_model=DeleteModelRequest)
def delete(call: APICall, company_id, request: DeleteModelRequest)
⋮----
user_id = call.identity.user
⋮----
def delete_many(call: APICall, company_id, request: ModelsDeleteManyRequest)
⋮----
succeeded = []
deleted_models = []
⋮----
projects = set(model.project for model in deleted_models)
⋮----
def archive_many(call: APICall, company_id, request: BatchRequest)
⋮----
def unarchive_many(call: APICall, company_id, request: BatchRequest)
⋮----
@endpoint("models.make_public", min_version="2.9", request_data_model=MakePublicRequest)
def make_public(call: APICall, company_id, request: MakePublicRequest)
⋮----
def make_public(call: APICall, company_id, request: MakePublicRequest)
⋮----
@endpoint("models.move", request_data_model=MoveRequest)
def move(call: APICall, company_id: str, request: MoveRequest)
⋮----
@endpoint("models.update_tags")
def update_tags(call: APICall, company_id: str, request: UpdateTagsRequest)
⋮----
@endpoint("models.delete_metadata", min_version="2.13")
def delete_metadata(call: APICall, company_id: str, request: DeleteMetadataRequest)
</file>

<file path="apiserver/services/organization.py">
org_bll = OrgBLL()
project_bll = ProjectBLL()
redis = redman.connection("apiserver")
conf = config.get("services.organization")
⋮----
@endpoint("organization.get_tags", request_data_model=TagsRequest)
def get_tags(call: APICall, company, request: TagsRequest)
⋮----
filter_dict = get_tags_filter_dictionary(request.filter)
ret = defaultdict(set)
⋮----
tags = org_bll.get_tags(
⋮----
@endpoint("organization.get_user_companies")
def get_user_companies(call: APICall, company_id: str, _)
⋮----
users = [
⋮----
@endpoint("organization.get_entities_count")
def get_entities_count(call: APICall, company, request: EntitiesCountRequest)
⋮----
entity_classes: Mapping[str, Type[AttributedDocument]] = {
⋮----
requested_ids = data.get("id")
⋮----
requested_ids = [requested_ids]
⋮----
query = Q()
⋮----
query = entity_cls.get_combined_query(
ids = entity_cls.objects(query).limit(request.limit).scalar("id")
⋮----
count_jobs = [
num_workers = conf.get("max_entities_count_concurrency", 0)
errs = {}
ret = {}
⋮----
def calc_wrapper(input_: tuple) -> Tuple[str, Union[int, str]]
⋮----
result = calc_entities_count(field, data, entity_cls)
⋮----
result = str(ex_)
⋮----
def get_task_data() -> Sequence[dict]
⋮----
tasks = Task.get_many_with_join(
⋮----
def get_model_data() -> Sequence[dict]
⋮----
models = Model.get_many_with_join(
⋮----
call_data = escape_execution_parameters(call_data)
get_fn = get_task_data
⋮----
call_data = Metadata.escape_query_parameters(call_data)
get_fn = get_model_data
⋮----
def getter(page: int, page_size: int) -> Sequence[dict]
⋮----
# validate input params
field_names = set()
⋮----
name = fm.name or fm.field
⋮----
value_keys = set()
⋮----
getter = _get_download_getter_fn(
# retrieve one element just to make sure that there are no issues with the call parameters
⋮----
_dangerous_chars = ("=", "+", "-", "@", "\t", "\r")
⋮----
def _sanitize_csv(value: str) -> str
⋮----
"""
    Prevent csv injection:
    If the string starts with any of the chars that Excel
    interpret as a special char then prepend it with a single quote
    """
⋮----
@endpoint("organization.download_for_get_all")
def download_for_get_all(call: APICall, company, request: DownloadForGetAllRequest)
⋮----
request_data = redis.get(f"get_all_download_{request.prepare_id}")
⋮----
call_data = json.loads(request_data)
request = PrepareDownloadForGetAllRequest(**call_data)
⋮----
class SingleLine
⋮----
@staticmethod
        def write(line: str) -> str
⋮----
def get_field_path(path_str: str) -> Sequence[str]
⋮----
path = path_str.split(".")
⋮----
def generate()
⋮----
field_mappings = {
get_fn = _get_download_getter_fn(
⋮----
val = nested_get(data, field_path, "")
⋮----
val = val.get("id", "")
⋮----
val = values.get(val, val)
⋮----
def get_projected_fields(data: dict) -> Sequence[str]
⋮----
page = 0
page_size = int(conf.get("download.batch_size", 500))
items_left = int(conf.get("download.max_download_items", 1000))
future = pool.submit(get_fn, page, min(page_size, items_left))
⋮----
result = future.result()
⋮----
writer = csv.writer(fp, quoting=csv.QUOTE_NONNUMERIC)
⋮----
fp.write("\ufeff")  # utf-8 signature
⋮----
def get_project_name() -> Optional[str]
⋮----
projects = call_data.get("project")
⋮----
projects = projects[0]
⋮----
project: Project = Project.objects(id=projects).only("basename").first()
⋮----
@endpoint("organization.get_project_usages")
def get_project_usages(call: APICall, company, request: GetProjectUsagesRequest)
</file>

<file path="apiserver/services/pipelines.py">
org_bll = OrgBLL()
project_bll = ProjectBLL()
task_bll = TaskBLL()
queue_bll = QueueBLL()
⋮----
def _update_task_name(task: Task)
⋮----
project = Project.objects(id=task.project).only("name").first()
⋮----
name_mask = re.compile(rf"{re.escape(name_prefix)}( #\d+)?$")
count = Task.objects(
new_name = f"{name_prefix} #{count}" if count > 0 else name_prefix
⋮----
@endpoint("pipelines.delete_runs")
def delete_runs(call: APICall, company_id: str, request: DeleteRunsRequest)
⋮----
existing_runs = set(
⋮----
# make sure that only controller tasks are deleted
ids = existing_runs.intersection(request.ids)
⋮----
succeeded = []
tasks = {}
⋮----
@endpoint("pipelines.start_pipeline")
def start_pipeline(call: APICall, company_id: str, request: StartPipelineRequest)
⋮----
hyperparams = None
⋮----
hyperparams = {
⋮----
extra = {}
⋮----
res_queue = nested_get(res, ("fields", "execution.queue"))
</file>

<file path="apiserver/services/projects.py">
org_bll = OrgBLL()
project_bll = ProjectBLL()
project_queries = ProjectQueries()
⋮----
create_fields = {
⋮----
@endpoint("projects.get_by_id")
def get_by_id(call: APICall, company: str, request: ProjectRequest)
⋮----
project_id = request.project
⋮----
query = Q(id=project_id) & get_company_or_none_constraint(company)
project = Project.objects(query).first()
⋮----
project_dict = project.to_proper_dict()
⋮----
def _hidden_query(search_hidden: bool, ids: Sequence) -> Q
⋮----
"""
    1. Add only non-hidden tasks search condition (unless specifically specified differently)
    """
⋮----
def _adjust_search_parameters(data: dict, shallow_search: bool)
⋮----
"""
    1. Make sure that there is no external query on path
    2. If not shallow_search and parent is provided then parent can be at any place in path
    3. If shallow_search and no parent provided then use a top level parent
    """
⋮----
stats_filter = {"tags": request.children_tags_filter}
⋮----
stats_filter = {"tags": request.children_tags}
⋮----
stats_filter = {}
⋮----
@endpoint("projects.get_all_ex")
def get_all_ex(call: APICall, company_id: str, request: ProjectsGetRequest)
⋮----
data = call.data
⋮----
allow_public = (
⋮----
requested_ids = data.get("id")
⋮----
requested_ids = [requested_ids]
⋮----
selected_project_ids = None
⋮----
ret_params = {}
⋮----
remove_system_tags = False
⋮----
only_fields = data.get("only_fields")
⋮----
remove_system_tags = True
⋮----
projects: Sequence[dict] = Project.get_many_with_join(
⋮----
system_tags = (
⋮----
project_ids = list({project["id"] for project in projects})
⋮----
contents = project_bll.calc_own_datasets(
⋮----
contents = project_bll.calc_own_contents(
⋮----
dataset_stats = project_bll.get_dataset_stats(
⋮----
@endpoint("projects.get_all")
def get_all(call: APICall, company: str, _)
⋮----
projects = Project.get_many(
⋮----
def create(call: APICall, company: str, _)
⋮----
identity = call.identity
⋮----
fields = parse_from_call(call.data, create_fields, Project.get_fields())
⋮----
@endpoint("projects.update", response_data_model=UpdateResponse)
def update(call: APICall, company: str, request: ProjectRequest)
⋮----
"""
    update

    :summary: Update project information.
              See `project.create` for parameters.
    :return: updated - `int` - number of projects updated
             fields - `[string]` - updated fields
    """
fields = parse_from_call(
⋮----
updated = ProjectBLL.update(company=company, project_id=request.project, **fields)
⋮----
def _reset_cached_tags(company: str, projects: Sequence[str])
⋮----
@endpoint("projects.move", request_data_model=MoveRequest)
def move(call: APICall, company: str, request: MoveRequest)
⋮----
@endpoint("projects.merge", request_data_model=MergeRequest)
def merge(call: APICall, company: str, request: MergeRequest)
⋮----
@endpoint("projects.validate_delete")
def validate_delete(call: APICall, company_id: str, request: ProjectRequest)
⋮----
@endpoint("projects.delete", request_data_model=DeleteRequest)
def delete(call: APICall, company_id: str, request: DeleteRequest)
⋮----
# noinspection PyTypeChecker
⋮----
metrics = project_queries.get_unique_metric_variants(
⋮----
@endpoint("projects.get_model_metadata_keys")
def get_model_metadata_keys(call: APICall, company_id: str, request: GetParamsRequest)
⋮----
def get_hyper_parameters(call: APICall, company_id: str, request: GetParamsRequest)
⋮----
@endpoint("projects.get_project_tags")
def get_tags(call: APICall, company, request: ProjectTagsRequest)
⋮----
def get_tags(call: APICall, company, request: ProjectTagsRequest)
⋮----
ret = org_bll.get_tags(
⋮----
def make_public(call: APICall, company_id, request: MakePublicRequest)
⋮----
@endpoint("projects.get_user_names")
def get_user_names(call: APICall, company_id: str, request: ProjectUserNamesRequest)
</file>

<file path="apiserver/services/queues.py">
worker_bll = WorkerBLL()
queue_bll = QueueBLL(worker_bll)
⋮----
def conform_queue_data(call: APICall, queue_data: Union[Sequence[dict], dict])
⋮----
@endpoint("queues.get_by_id", min_version="2.4")
def get_by_id(call: APICall, company_id, request: GetByIdRequest)
⋮----
queue = queue_bll.get_by_id(
queue_dict = queue.to_proper_dict()
⋮----
@endpoint("queues.get_default", min_version="2.4", response_data_model=GetDefaultResp)
def get_by_id(call: APICall)
⋮----
queue = queue_bll.get_default(call.identity.company)
⋮----
def _hidden_query(data: dict) -> Q
⋮----
"""
    1. Add only non-hidden queues search condition (unless specifically specified differently)
    """
hidden_tags = config.get("services.queues.hidden_tags", [])
⋮----
@endpoint("queues.get_all_ex", min_version="2.4")
def get_all_ex(call: APICall, company: str, request: GetAllRequest)
⋮----
ret_params = {}
⋮----
call_data = Metadata.escape_query_parameters(call.data)
queues = queue_bll.get_queue_infos(
⋮----
@endpoint("queues.get_all", min_version="2.4")
def get_all(call: APICall, company: str, request: GetAllRequest)
⋮----
queues = queue_bll.get_all(
⋮----
@endpoint("queues.create", min_version="2.4")
def create(call: APICall, company_id, request: CreateRequest)
⋮----
queue = queue_bll.create(
⋮----
def update(call: APICall, company_id, request: UpdateRequest)
⋮----
data = call.data_model_for_partial_update
⋮----
@endpoint("queues.delete", min_version="2.4")
def delete(call: APICall, company_id, request: DeleteRequest)
⋮----
@endpoint("queues.add_task", min_version="2.4")
def add_task(call: APICall, company_id, request: AddTaskRequest)
⋮----
added = queue_bll.add_task(
⋮----
@endpoint("queues.get_next_task")
def get_next_task(call: APICall, company_id, request: GetNextTaskRequest)
⋮----
entry = queue_bll.get_next_task(
⋮----
data = {"entry": entry.to_proper_dict()}
task = Task.objects(id=entry.task).only("company", "user", "status").first()
⋮----
# fix racing condition that can result in the task being aborted
# by an agent after it was already placed in a queue
⋮----
@endpoint("queues.remove_task", min_version="2.4")
def remove_task(call: APICall, company_id, request: RemoveTaskRequest)
⋮----
@endpoint("queues.clear_queue")
def clear_queue(call: APICall, company_id, request: QueueRequest)
⋮----
def move_task_forward(call: APICall, company_id, request: MoveTaskRequest)
⋮----
def move_task_backward(call: APICall, company_id, request: MoveTaskRequest)
⋮----
def move_task_to_front(call: APICall, company_id, request: TaskRequest)
⋮----
def move_task_to_back(call: APICall, company_id, request: TaskRequest)
⋮----
empty_response = GetMetricsResponse(queues=[])
⋮----
queue_query = dict(company=company_id)
⋮----
metrics_per_queue = True
⋮----
metrics_per_queue = False
⋮----
queue_ids = list(
⋮----
ret = queue_bll.metrics.get_queue_metrics(
⋮----
queue_dicts = {
⋮----
queue_id = request.queue
queue = queue_bll.get_by_id(company_id=company_id, queue_id=queue_id, only=("id",))
⋮----
@endpoint("queues.delete_metadata", min_version="2.13")
def delete_metadata(call: APICall, company_id: str, request: DeleteMetadataRequest)
⋮----
@endpoint("queues.peek_task", min_version="2.15")
def peek_task(call: APICall, company_id: str, request: QueueRequest)
⋮----
@endpoint("queues.get_num_entries", min_version="2.15")
def get_num_entries(call: APICall, company_id: str, request: QueueRequest)
</file>

<file path="apiserver/services/reports.py">
org_bll = OrgBLL()
project_bll = ProjectBLL()
task_bll = TaskBLL()
⋮----
update_fields = {
⋮----
def _assert_report(company_id: str, task_id: str, identity: Identity, only_fields=None)
⋮----
task = get_task_with_write_access(
⋮----
@endpoint("reports.update", response_data_model=UpdateResponse)
def update_report(call: APICall, company_id: str, request: UpdateReportRequest)
⋮----
task = _assert_report(
⋮----
partial_update_dict = {
⋮----
allowed_for_published = set(partial_update_dict.keys()).issubset(
⋮----
now = datetime.utcnow()
more_updates = {
⋮----
updated = task.update(upsert=False, **partial_update_dict, **more_updates)
⋮----
updated_tags = partial_update_dict.get("tags")
⋮----
updated_report = partial_update_dict.get("report")
⋮----
def _ensure_reports_project(company: str, user: str, name: str)
⋮----
name = name.strip("/")
⋮----
name = f"{name}/{reports_project_name}"
⋮----
@endpoint("reports.create")
def create_report(call: APICall, company_id: str, request: CreateReportRequest)
⋮----
user_id = call.identity.user
project_id = request.project
⋮----
project = Project.get_for_writing(
project_name = project.name
⋮----
project_name = ""
⋮----
project_id = _ensure_reports_project(
task = task_bll.create(
⋮----
def _delete_reports_project_if_empty(project_id)
⋮----
project = Project.objects(id=project_id).only("basename").first()
⋮----
@endpoint("reports.get_all_ex")
def get_all_ex(call: APICall, company_id, request: GetAllRequest)
⋮----
call_data = call.data
⋮----
# bring projects one level down in case not the .reports project was passed
⋮----
project_ids = call_data["project"]
⋮----
project_ids = [project_ids]
⋮----
query = Q(parent__in=project_ids) | Q(id__in=project_ids)
project_ids = Project.objects(query & Q(basename=reports_project_name)).scalar(
⋮----
ret_params = {}
tasks = Task.get_many_with_join(
⋮----
task_metrics = {}
⋮----
task_dict = {}
⋮----
@endpoint("reports.get_task_data")
def get_task_data(call: APICall, company_id, request: GetTasksDataRequest)
⋮----
entity_cls = Model
conform_data = conform_model_data
⋮----
entity_cls = Task
conform_data = conform_task_data
⋮----
call_data = escape_execution_parameters(call.data)
⋮----
tasks = entity_cls.get_many_with_join(
⋮----
res = {"tasks": tasks, **ret_params}
⋮----
task_ids = [task["id"] for task in tasks]
companies = _get_task_or_model_index_companies(
⋮----
result = event_bll.debug_images_iterator.get_task_events(
⋮----
@endpoint("reports.move")
def move(call: APICall, company_id: str, request: MoveReportRequest)
⋮----
project_name = request.project_name
⋮----
def publish(call: APICall, company_id, request: PublishReportRequest)
⋮----
updates = ChangeStatusRequest(
⋮----
@endpoint("reports.archive")
def archive(call: APICall, company_id, request: ArchiveReportRequest)
⋮----
archived = task.update(
⋮----
@endpoint("reports.unarchive")
def unarchive(call: APICall, company_id, request: ArchiveReportRequest)
⋮----
unarchived = task.update(
⋮----
# @endpoint("reports.share")
# def share(call: APICall, company_id, request: ShareReportRequest):
#     _assert_report(
#         company_id=company_id, user_id=call.identity.user, task_id=request.task
#     )
#     call.result.data = {
#         "changed": task_bll.share_task(
#             company_id=company_id, task_ids=[request.task], share=request.share
#         )
#     }
⋮----
@endpoint("reports.delete")
def delete(call: APICall, company_id, request: DeleteReportRequest)
⋮----
@endpoint("reports.get_tags")
def get_tags(call: APICall, company_id: str, _)
⋮----
tags = Task.objects(company=company_id, type=TaskType.report).distinct(field="tags")
</file>

<file path="apiserver/services/server/__init__.py">
@endpoint("server.get_stats")
def get_stats(call: APICall)
⋮----
@endpoint("server.config")
def get_config(call: APICall, _, request: GetConfigRequest)
⋮----
path = request.path
⋮----
c = dict(config.get(path))
⋮----
c = config.to_dict()
⋮----
def remove_none_value(x)
⋮----
"""
        Pyhocon bug in Python 3: leaves dummy "NoneValue"s in tree,
        see: https://github.com/chimpler/pyhocon/issues/111
        """
⋮----
@endpoint("server.endpoints")
def get_endpoints(call: APICall)
⋮----
@endpoint("server.info")
def info(call: APICall)
⋮----
@endpoint("server.info", min_version="2.8")
def info_2_8(call: APICall)
⋮----
@endpoint("server.info", min_version="2.12")
def info_2_8(call: APICall)
⋮----
def report_stats(call: APICall, company: str, request: ReportStatsOptionRequest)
⋮----
result = ReportStatsOptionResponse(supported=False)
⋮----
enabled = request.enabled
⋮----
query = Company.objects(id=company)
⋮----
stats_option = query.first().defaults.stats_option
⋮----
stats_option = ReportStatsOption(
updated = query.update(defaults__stats_option=stats_option)
⋮----
data = stats_option.to_mongo()
⋮----
result = ReportStatsOptionResponse(**data)
</file>

<file path="apiserver/services/serving.py">
serving_bll = ServingBLL()
⋮----
@endpoint("serving.register_container")
def register_container(call: APICall, company: str, request: RegisterRequest)
⋮----
@endpoint("serving.unregister_container")
def unregister_container(_: APICall, company: str, request: UnregisterRequest)
⋮----
@endpoint("serving.container_status_report")
def container_status_report(call: APICall, company: str, request: StatusReportRequest)
⋮----
@endpoint("serving.get_endpoints")
def get_endpoints(call: APICall, company: str, _)
⋮----
@endpoint("serving.get_loading_instances")
def get_loading_instances(call: APICall, company: str, _)
</file>

<file path="apiserver/services/storage.py">
storage_bll = StorageBLL()
⋮----
@endpoint("storage.get_settings")
def get_settings(call: APICall, company: str, _)
⋮----
@endpoint("storage.set_settings")
def set_settings(call: APICall, company: str, request: SetSettingsRequest)
⋮----
@endpoint("storage.reset_settings")
def reset_settings(call: APICall, company: str, request: ResetSettingsRequest)
</file>

<file path="apiserver/services/tasks.py">
task_fields = set(Task.get_fields())
task_script_stripped_fields = set(
⋮----
task_bll = TaskBLL()
event_bll = EventBLL()
queue_bll = QueueBLL()
org_bll = OrgBLL()
project_bll = ProjectBLL()
⋮----
tasks = get_many_tasks_for_writing(
missing_ids = set(ids) - {t.id for t in tasks}
⋮----
task = get_task_with_write_access(
⋮----
status_reason = request.status_reason
status_message = request.status_message
force = request.force
⋮----
@endpoint("tasks.get_by_id", request_data_model=TaskRequest)
def get_by_id(call: APICall, company_id, request: TaskRequest)
⋮----
task = TaskBLL.assert_exists(
task_dict = task.to_proper_dict()
⋮----
def escape_execution_parameters(call_data: dict) -> dict
⋮----
keys = list(call_data)
call_data = {
⋮----
projection = Task.get_projection(call_data)
⋮----
ordering = Task.get_ordering(call_data)
⋮----
def _hidden_query(data: dict) -> Q
⋮----
"""
    1. Add only non-hidden tasks search condition (unless specifically specified differently)
    """
⋮----
@endpoint("tasks.get_all_ex")
def get_all_ex(call: APICall, company_id, request: GetAllReq)
⋮----
call_data = escape_execution_parameters(call.data)
⋮----
ret_params = {}
tasks = Task.get_many_with_join(
⋮----
@endpoint("tasks.get_by_id_ex", required_fields=["id"])
def get_by_id_ex(call: APICall, company_id, _)
⋮----
@endpoint("tasks.get_all")
def get_all(call: APICall, company_id, _)
⋮----
tasks = Task.get_many(
⋮----
@endpoint("tasks.get_types", request_data_model=GetTypesRequest)
def get_types(call: APICall, company_id, request: GetTypesRequest)
⋮----
@endpoint("tasks.stop", response_data_model=UpdateResponse)
def stop(call: APICall, company_id, request: StopRequest)
⋮----
"""
    stop
    :summary: Stop a running task. Requires task status 'in_progress' and
              execution_progress 'running', or force=True.
              Development task is stopped immediately. For a non-development task
              only its status message is set to 'stopping'

    """
⋮----
def stop_many(call: APICall, company_id, request: StopManyRequest)
⋮----
def stopped(call: APICall, company_id, req_model: UpdateRequest)
⋮----
def started(call: APICall, company_id, request: UpdateRequest)
⋮----
task = Task.objects(id=request.task).only("id", "status", "started").first()
⋮----
started_update = {}
⋮----
# don't override a previous, smaller "started" field value
⋮----
started_update = {
⋮----
res = StartedResponse(
⋮----
def failed(call: APICall, company_id, req_model: UpdateRequest)
⋮----
def close(call: APICall, company_id, req_model: UpdateRequest)
⋮----
create_fields = {
⋮----
dict_fields_paths = [("execution", "model_labels"), "container"]
⋮----
def prepare_for_save(call: APICall, fields: dict, previous_task: Task = None)
⋮----
# Strip all script fields (remove leading and trailing whitespace chars) to avoid unusable names and paths
script = fields.get("script")
⋮----
value = script.get(field)
⋮----
def conform_task_data(call: APICall, tasks_data: Union[Sequence[dict], dict])
⋮----
tasks_data = [tasks_data]
⋮----
need_legacy_params = call.requested_endpoint_version < PartialVersion("2.9")
⋮----
valid_fields = valid_fields if valid_fields is not None else create_fields
t_fields = task_fields
⋮----
fields = parse_from_call(call.data, valid_fields, t_fields)
⋮----
# Move output_dest to output.destination
output_dest = fields.get("output_dest")
⋮----
output = Output(destination=output_dest)
⋮----
# Add models updated time
models = fields.get("models")
⋮----
now = datetime.now(timezone.utc)
⋮----
field_models = models.get(field)
⋮----
def _validate_and_get_task_from_call(call: APICall, **kwargs) -> Tuple[Task, dict]
⋮----
fields = prepare_create_fields(call, **kwargs)
task = task_bll.create(
⋮----
@endpoint("tasks.validate", request_data_model=CreateRequest)
def validate(call: APICall, _, __: CreateRequest)
⋮----
parent = call.data.get("parent")
⋮----
def _update_cached_tags(company: str, project: str, fields: dict)
⋮----
def _reset_cached_tags(company: str, projects: Sequence[str])
⋮----
def create(call: APICall, company_id, _: CreateRequest)
⋮----
@endpoint("tasks.clone", request_data_model=CloneRequest)
def clone_task(call: APICall, company_id, request: CloneRequest)
⋮----
def prepare_update_fields(call: APICall, call_data)
⋮----
valid_fields = deepcopy(Task.user_set_allowed())
update_fields = {k: v for k, v in create_fields.items() if k in valid_fields}
⋮----
fields = parse_from_call(call_data, update_fields, t_fields)
⋮----
def update(call: APICall, company_id, req_model: UpdateRequest)
⋮----
task_id = req_model.task
⋮----
new_project = updated_fields.get("project", task.project)
⋮----
def set_requirements(call: APICall, company_id, req_model: SetRequirementsRequest)
⋮----
requirements = req_model.requirements
⋮----
res = update_task(
⋮----
@endpoint("tasks.update_batch")
def update_batch(call: APICall, company_id, _)
⋮----
items = call.batched_data
⋮----
items = {i["task"]: i for i in items}
tasks = {
⋮----
missing = tuple(set(items).difference(tasks))
⋮----
bulk_ops = []
updated_projects = set()
⋮----
task = tasks[id]
⋮----
partial_update_dict = Task.get_safe_update_dict(fields)
⋮----
update_op = UpdateOne(
⋮----
new_project = partial_update_dict.get("project", task.project)
⋮----
updated = 0
⋮----
res = Task._get_collection().bulk_write(bulk_ops)
updated = res.modified_count
⋮----
projects = list(updated_projects)
⋮----
def edit(call: APICall, company_id, req_model: UpdateRequest)
⋮----
force = req_model.force
⋮----
edit_fields = create_fields.copy()
⋮----
fields = prepare_create_fields(
⋮----
field = getattr(task, key, None)
value = fields[key]
⋮----
d = field.to_mongo(use_db_field=False).to_dict()
⋮----
# make sure field names do not end in mongoengine comparison operators
fixed_fields = {
⋮----
last_change = dict(last_change=now, last_changed_by=call.identity.user)
⋮----
updated = task.update(upsert=False, **fixed_fields)
⋮----
new_project = fixed_fields.get("project", task.project)
⋮----
def get_hyper_params(call: APICall, company_id, request: GetHyperParamsRequest)
⋮----
tasks_params = HyperParams.get_params(company_id, task_ids=request.tasks)
⋮----
@endpoint("tasks.edit_hyper_params", request_data_model=EditHyperParamsRequest)
def edit_hyper_params(call: APICall, company_id, request: EditHyperParamsRequest)
⋮----
@endpoint("tasks.delete_hyper_params", request_data_model=DeleteHyperParamsRequest)
def delete_hyper_params(call: APICall, company_id, request: DeleteHyperParamsRequest)
⋮----
def get_configurations(call: APICall, company_id, request: GetConfigurationsRequest)
⋮----
tasks_params = HyperParams.get_configurations(
⋮----
tasks_params = HyperParams.get_configuration_names(
⋮----
@endpoint("tasks.edit_configuration", request_data_model=EditConfigurationRequest)
def edit_configuration(call: APICall, company_id, request: EditConfigurationRequest)
⋮----
def enqueue(call: APICall, company_id, request: EnqueueRequest)
⋮----
res_queue = nested_get(res, ("fields", "execution.queue"))
⋮----
def enqueue_many(call: APICall, company_id, request: EnqueueManyRequest)
⋮----
extra = {}
⋮----
def dequeue(call: APICall, company_id, request: DequeueRequest)
⋮----
def dequeue_many(call: APICall, company_id, request: DequeueManyRequest)
⋮----
task_ids = list(tasks)
deleted_model_ids = set(
⋮----
delete_external_artifacts = delete_external_artifacts and config.get(
⋮----
urls = set(cleanup_res.urls.model_urls) | set(
⋮----
event_urls = delete_task_events_and_collect_urls(
⋮----
def reset(call: APICall, company_id, request: ResetRequest)
⋮----
task_id = request.task
⋮----
res = ResetResponse(
⋮----
def reset_many(call: APICall, company_id, request: ResetManyRequest)
⋮----
succeeded = []
tasks = {}
⋮----
def archive(call: APICall, company_id, request: ArchiveRequest)
⋮----
archived = 0
tasks = _assert_writable_tasks(
⋮----
def archive_many(call: APICall, company_id, request: ArchiveManyRequest)
⋮----
def unarchive_many(call: APICall, company_id, request: UnarchiveManyRequest)
⋮----
@endpoint("tasks.delete", request_data_model=DeleteRequest)
def delete(call: APICall, company_id, request: DeleteRequest)
⋮----
@endpoint("tasks.delete_many", request_data_model=DeleteManyRequest)
def delete_many(call: APICall, company_id, request: DeleteManyRequest)
⋮----
projects = set()
⋮----
def publish(call: APICall, company_id, request: PublishRequest)
⋮----
updates = publish_task(
⋮----
def publish_many(call: APICall, company_id, request: PublishManyRequest)
⋮----
def completed(call: APICall, company_id, request: CompletedRequest)
⋮----
res = CompletedResponse(
⋮----
publish_res = publish_task(
⋮----
new_status = nested_get(publish_res, ("fields", "status"))
⋮----
@endpoint("tasks.ping", request_data_model=PingRequest)
def ping(call: APICall, company_id, request: PingRequest)
⋮----
@endpoint("tasks.edit_runtime", request_data_model=EditRuntimeRequest)
def edit_runtime(call: APICall, company_id, request: EditRuntimeRequest)
⋮----
def delete_artifacts(call: APICall, company_id, request: DeleteArtifactsRequest)
⋮----
@endpoint("tasks.make_public", min_version="2.9", request_data_model=MakePublicRequest)
def make_public(call: APICall, company_id, request: MakePublicRequest)
⋮----
@endpoint("tasks.make_private", min_version="2.9", request_data_model=MakePublicRequest)
def make_public(call: APICall, company_id, request: MakePublicRequest)
⋮----
@endpoint("tasks.move", request_data_model=MoveRequest)
def move(call: APICall, company_id: str, request: MoveRequest)
⋮----
updated_projects = set(t.project for t in tasks if t.project)
⋮----
project_id = project_bll.move_under_project(
⋮----
projects = list(updated_projects | {project_id})
⋮----
@endpoint("tasks.update_tags")
def update_tags(call: APICall, company_id: str, request: UpdateTagsRequest)
⋮----
@endpoint("tasks.add_or_update_model", min_version="2.13")
def add_or_update_model(call: APICall, company_id: str, request: AddUpdateModelRequest)
⋮----
models_field = f"models__{request.type}"
model = ModelItem(name=request.name, model=request.model, updated=datetime.now(timezone.utc))
query = {"id": request.task, f"{models_field}__name": request.name}
updated = Task.objects(**query).update_one(**{f"set__{models_field}__S": model})
⋮----
updated = TaskBLL.update_statistics(
⋮----
@endpoint("tasks.delete_models", min_version="2.13")
def delete_models(call: APICall, company_id: str, request: DeleteModelsRequest)
⋮----
task = get_task_for_update(
⋮----
delete_names = {
commands = {
⋮----
updated = task.update(
</file>

<file path="apiserver/services/users.py">
log = config.logger(__file__)
project_bll = ProjectBLL()
⋮----
def get_user(call, company_id, user_id, only=None)
⋮----
"""
    Get user object by the user's ID
    :param call: API call
    :param user_id: user ID
    :param only: fields to include in projection, by default all
    :return: User object
    """
⋮----
# allow system users to get info for all users
query = dict(id=user_id)
⋮----
query = dict(id=user_id, company=company_id)
⋮----
user = User.objects(**query)
⋮----
user = user.only(*only)
res = user.first()
⋮----
@endpoint("users.get_by_id")
def get_by_id(call: APICall, company_id, request: UserRequest)
⋮----
user_id = request.user
⋮----
@endpoint("users.get_all_ex")
def get_all_ex(call: APICall, company_id, _)
⋮----
res = User.get_many_with_join(company=company_id, query_dict=call.data)
⋮----
@endpoint("users.get_all_ex", min_version="2.8")
def get_all_ex2_8(call: APICall, company_id, _)
⋮----
data = call.data
active_in_projects = call.data.get("active_in_projects", None)
⋮----
active_users = project_bll.get_active_users(
⋮----
data = data.copy()
⋮----
res = User.get_many_with_join(company=company_id, query_dict=data)
⋮----
@endpoint("users.get_all")
def get_all(call: APICall, company_id, _)
⋮----
res = User.get_many(
⋮----
@endpoint("users.get_current_user")
def get_current_user(call: APICall, company_id, _)
⋮----
user_id = call.identity.user
⋮----
projection = (
res = User.get_many_with_join(
⋮----
user = res[0]
⋮----
resp = dict(
⋮----
create_fields = {
⋮----
@endpoint("users.create", request_data_model=CreateRequest)
def create(call: APICall)
⋮----
@endpoint("users.delete")
def delete(_: APICall, __, request: UserRequest)
⋮----
def update_user(user_id, company_id, data: dict) -> Tuple[int, dict]
⋮----
"""
    Update user.
    :param user_id: user ID to update
    :param company_id: ID of company user belongs to
    :param data: mapping to update user by
    :return: (updated fields count, updated fields) pair
    """
update_fields = {
auth_user_update_fields = ("name",)
partial_update_dict = parse_from_call(data, update_fields, User.get_fields())
⋮----
ret = User.safe_update(company_id, user_id, partial_update_dict)
auth_update = {
⋮----
@endpoint("users.update", response_data_model=UpdateResponse)
def update(call, company_id, request: UserRequest)
⋮----
def get_user_preferences(call: APICall, company_id)
⋮----
preferences = get_user(call, company_id, user_id, only=["preferences"]).get(
⋮----
preferences = loads(preferences)
⋮----
@endpoint("users.get_preferences")
def get_preferences(call: APICall, company_id, _)
⋮----
@endpoint("users.set_preferences", request_data_model=SetPreferencesRequest)
def set_preferences(call: APICall, company_id, request: SetPreferencesRequest)
⋮----
changes = request.preferences
⋮----
def invalid_key(path, key, __)
⋮----
"""
        key cannot start with "$"
        only top level keys may contain "."
        """
⋮----
base_preferences = get_user_preferences(call, company_id)
new_preferences = deepcopy(base_preferences)
⋮----
updated = User.objects(id=call.identity.user, company=company_id).update(
⋮----
fields = {}
</file>

<file path="apiserver/services/utils.py">
def process_include_subprojects(call_data: dict)
⋮----
include_subprojects = call_data.pop("include_subprojects", False)
project_ids = call_data.get("project")
⋮----
project_ids = [project_ids]
⋮----
def get_tags_filter_dictionary(input_: Filter) -> dict
⋮----
def sort_tags_response(ret: dict) -> dict
⋮----
def conform_output_tags(call: APICall, documents: Union[dict, Sequence[dict]])
⋮----
"""
    Make sure that tags are always returned sorted
    For old clients both tags and system tags are returned in 'tags' field
    """
⋮----
documents = [documents]
⋮----
merge_tags = call.requested_endpoint_version < PartialVersion("2.3")
⋮----
system_tags = doc.get("system_tags")
⋮----
tags = doc.get(field)
⋮----
def conform_tag_fields(call: APICall, document: dict, validate=False)
⋮----
"""
    Upgrade old client tags in place
    """
⋮----
"""
    Make sure that 'tags' from the old SDK clients
    are correctly split into 'tags' and 'system_tags'
    Make sure that there are no duplicate tags
    """
⋮----
def _upgrade_tags(call: APICall, tags: Sequence, system_tags: Sequence)
⋮----
service_name = call.endpoint_name.partition(".")[0]
entity = service_name[:-1] if service_name.endswith("s") else service_name
⋮----
def validate_tags(tags: Sequence[str], system_tags: Sequence[str])
⋮----
unsupported = [
⋮----
def escape_dict(data: dict) -> dict
⋮----
def unescape_dict(data: dict) -> dict
⋮----
def escape_dict_field(fields: dict, path: Union[str, Sequence[str]])
⋮----
path = (path,)
⋮----
data = nested_get(fields, path)
⋮----
def unescape_dict_field(fields: dict, path: Union[str, Sequence[str]])
⋮----
class ModelsBackwardsCompatibility
⋮----
max_version = PartialVersion("2.13")
mode_to_fields = {
models_field = "models"
⋮----
@classmethod
    def prepare_for_save(cls, call: APICall, fields: dict)
⋮----
value = nested_get(fields, field)
⋮----
val = [
⋮----
tasks_data = [tasks_data]
⋮----
models = nested_get(task, (cls.models_field, mode))
⋮----
model = models[0] if mode == TaskModelTypes.input else models[-1]
⋮----
class DockerCmdBackwardsCompatibility
⋮----
field = ("execution", "docker_cmd")
⋮----
docker_cmd = nested_get(fields, cls.field)
⋮----
container = task.get("container")
⋮----
docker_cmd = " ".join(
⋮----
def escape_metadata(document: dict)
⋮----
"""
    Escape special characters in metadata keys
    """
metadata = document.get("metadata")
⋮----
def unescape_metadata(call: APICall, documents: Union[dict, Sequence[dict]])
⋮----
"""
    Unescape special characters in metadata keys
    """
⋮----
old_client = call.requested_endpoint_version <= PartialVersion("2.16")
⋮----
metadata = doc.get("metadata")
</file>

<file path="apiserver/services/workers.py">
log = config.logger(__file__)
⋮----
worker_bll = WorkerBLL()
⋮----
def get_all(call: APICall, company_id: str, request: GetAllRequest)
⋮----
def get_all(call: APICall, company_id: str, request: GetCountRequest)
⋮----
@endpoint("workers.register", min_version="2.4", request_data_model=RegisterRequest)
def register(call: APICall, company_id, request: RegisterRequest)
⋮----
worker = request.worker
timeout = request.timeout
queues = request.queues
⋮----
timeout = config.get("apiserver.workers.default_timeout", 10 * 60)
⋮----
@endpoint("workers.unregister", min_version="2.4", request_data_model=WorkerRequest)
def unregister(call: APICall, company_id, req_model: WorkerRequest)
⋮----
def status_report(call: APICall, company_id, request: StatusReportRequest)
⋮----
ret = worker_bll.stats.get_worker_stats_keys(
⋮----
def get_activity_series(active_only: bool = False) -> ActivityReportSeries
⋮----
ret = worker_bll.stats.get_activity_report(
⋮----
count_by_date = extract_properties_to_lists(["date", "count"], ret)
⋮----
def get_stats(call: APICall, company_id, request: GetStatsRequest)
⋮----
ret = worker_bll.stats.get_worker_stats(company_id, request)
⋮----
resource_series = []
</file>

<file path="apiserver/sync.py">
_redis = redman.connection("apiserver")
⋮----
@contextmanager
def distributed_lock(name: str, timeout: int, max_wait: int = 0)
⋮----
"""
    Context manager that acquires a distributed lock on enter
    and releases it on exit. The has a ttl equal to timeout seconds
    If the lock can not be acquired for wait seconds (defaults to timeout * 2)
    then the exception is thrown
    """
lock_name = f"dist_lock_{name}"
start = time.time()
max_wait = max_wait or timeout * 2
pid = os.getpid()
⋮----
holder = _redis.get(lock_name)
</file>

<file path="apiserver/tests/__init__.py">

</file>

<file path="apiserver/tests/api_client.py">
log = config.logger(__file__)
⋮----
class APICallResult
⋮----
def __init__(self, res)
⋮----
def as_dict(self)
⋮----
class APICallResultMeta
⋮----
def __init__(self, meta_dict)
⋮----
m = meta_dict
⋮----
def format_duration(duration)
⋮----
class APIError(Exception)
⋮----
def __init__(self, result: APICallResult, duration=None)
⋮----
def __str__(self)
⋮----
meta = self.result.meta
message = (
⋮----
header = "\n--- SERVER ERROR {} ---\n"
formatted_traceback = "\n{}{}{}\n".format(header.format("START"), meta.error_stack, header.format("END"))
⋮----
class AttrDict(dict)
⋮----
"""
    ``dict`` which supports attribute lookup syntax.
    Use to implement polymorphism over ``dict``s and database objects, which don't support subscription syntax.
    """
⋮----
def __init__(self, dct=None, **kwargs)
⋮----
def __getattr__(self, item)
⋮----
class APIClient
⋮----
# create http session
⋮----
retries = config.get("apiclient.retries", 7)
backoff_factor = config.get("apiclient.backoff_factor", 0.3)
status_forcelist = config.get("apiclient.status_forcelist", (500, 502, 504))
retry = Retry(
adapter = HTTPAdapter(max_retries=retry)
⋮----
def login(self)
⋮----
def impersonate(self, user_id)
⋮----
headers_overrides = headers_overrides or {}
⋮----
json_lines = (json.dumps(item) for item in items)
data = "\n".join(json_lines)
⋮----
headers_overrides = {}
⋮----
data = {}
headers = {"Content-Type": "application/json"}
⋮----
data = json.dumps(data)
⋮----
auth = HTTPBasicAuth(self.api_key, self.secret_key)
⋮----
auth = None
⋮----
url = "%s/%s" % (self.base_url, endpoint)
start = time.time()
⋮----
http_res = self.http_session.post(url, headers=headers, data=data, auth=auth)
⋮----
msg = "APIClient got non standard response from %s. http_status=%s " % (
⋮----
res = APICallResult(http_res.text)
⋮----
# poll server for async result
got_result = False
call_id = res.meta.call_id
async_res_url = "%s/async.result?id=%s" % (self.base_url, call_id)
async_res_headers = headers.copy()
⋮----
http_res = self.http_session.get(
⋮----
got_result = True
⋮----
duration = time.time() - start
⋮----
error = APIError(res, duration)
⋮----
msg = "APIClient got {} from {}{}".format(
⋮----
class Service(object)
⋮----
def __init__(self, api, name)
⋮----
@staticmethod
@contextmanager
    def raises(codes: Union[Type[BaseError], Tuple[int, int]])
⋮----
codes = codes.codes
</file>

<file path="apiserver/tests/automated/__init__.py">
log = config.logger(__file__)
⋮----
class TestServiceInterface(metaclass=abc.ABCMeta)
⋮----
api = abc.abstractproperty()
⋮----
@abc.abstractmethod
    def defer(self, func, *args, can_fail=False, **kwargs)
⋮----
class TestService(TestCase, TestServiceInterface)
⋮----
@property
    def api(self)
⋮----
@api.setter
    def api(self, value)
⋮----
def defer(self, func, *args, can_fail=False, **kwargs)
⋮----
client = client or self.api
⋮----
object_id = data["id"]
⋮----
@staticmethod
    def update_missing(target: dict, **update)
⋮----
def create_temp(self, service, *, client=None, delete_params=None, object_name="", **kwargs) -> str
⋮----
def setUp(self, version="999.0")
⋮----
def tearDown(self)
⋮----
def assertEqualNoOrder(self, first: Iterable, second: Iterable)
⋮----
"""Compares 2 sequences regardless of their items order"""
⋮----
def header(info, title="=" * 20)
⋮----
def utc_now_tz_aware() -> datetime
⋮----
"""
    Returns utc now with the utc time zone.
    Suitable for subsequent usage with functions that
    make use of tz info like 'timestamp'
    """
</file>

<file path="apiserver/tests/automated/test_batch_operations.py">
class TestBatchOperations(TestService)
⋮----
name = "batch operation test"
comment = "this is a comment"
delete_params = dict(can_fail=True, force=True)
⋮----
def test_tasks(self)
⋮----
tasks = [self._temp_task() for _ in range(2)]
models = [
missing_id = db_id()
ids = [*tasks, missing_id]
⋮----
# enqueue
res = self.api.tasks.enqueue_many(ids=ids, queue_name="test batch")
⋮----
data = self.api.tasks.get_all_ex(id=ids).tasks
⋮----
# stop
⋮----
res = self.api.tasks.stop_many(ids=ids)
⋮----
# publish
res = self.api.tasks.publish_many(ids=ids, publish_model=False)
⋮----
# reset
res = self.api.tasks.reset_many(
⋮----
# archive/unarchive
res = self.api.tasks.archive_many(ids=ids)
⋮----
res = self.api.tasks.unarchive_many(ids=ids)
⋮----
# delete
res = self.api.tasks.delete_many(
⋮----
def test_models(self)
⋮----
uris = [f"file:///{i}" for i in range(2)]
models = [self._temp_model(uri=uri) for uri in uris]
⋮----
ids = [*models, missing_id]
⋮----
task = self._temp_task()
⋮----
res = self.api.models.publish_many(
⋮----
res = self.api.models.archive_many(ids=ids)
⋮----
data = self.api.models.get_all_ex(id=ids).models
⋮----
res = self.api.models.unarchive_many(ids=ids)
⋮----
res = self.api.models.delete_many(ids=[*models, missing_id], force=True)
⋮----
def _assert_succeeded(self, res, succeeded_ids)
⋮----
def _assert_failed(self, res, failed_ids)
⋮----
def _temp_model(self, **kwargs)
⋮----
def _temp_task(self)
⋮----
def _temp_task_model(self, task, **kwargs) -> str
⋮----
model = self._temp_model(ready=False, task=task, **kwargs)
</file>

<file path="apiserver/tests/automated/test_entity_ordering.py">
class TestEntityOrdering(TestService)
⋮----
test_comment = "Entity ordering test"
only_fields = ["id", "started", "comment", "execution.parameters"]
⋮----
def setUp(self, **kwargs)
⋮----
def test_order(self)
⋮----
# test no ordering
⋮----
# sort ascending
⋮----
# sort descending
⋮----
# sort by the same field that we use for the search
⋮----
# sort by parameter which type is not part of db schema
⋮----
def test_order_with_paging(self)
⋮----
order_field = "started"
# all results in one page
⋮----
field_vals = []
page_size = 4
num_pages = 5
⋮----
paged_tasks = self._get_page_tasks(
⋮----
def _get_page_tasks(self, order_by, page: int, page_size: int) -> Sequence
⋮----
def _assertSorted(self, vals: Sequence, ascending=True, is_numeric=False)
⋮----
"""
        Assert that vals are sorted in the ascending or descending order
        with None values are always coming from the end
        """
empty = [None, "", [], {}]
empty_value = None
idx = 0
⋮----
empty_value = val
⋮----
none_tail = vals[idx:]
vals = vals[:idx]
⋮----
vals = list(map(int, vals))
⋮----
cmp = operator.le
⋮----
cmp = operator.ge
⋮----
def _get_value_for_path(self, data: Mapping, field_path: Sequence[str])
⋮----
val = None
⋮----
val = data.get(name)
data = val if isinstance(val, dict) else {}
⋮----
tasks = self.api.tasks.get_all_ex(
⋮----
# test that the output is correctly ordered
field_name = order_by if not order_by.startswith("-") else order_by[1:]
field_vals = [
⋮----
def _create_tasks(self)
⋮----
tasks = [
⋮----
def _temp_task(self, **kwargs)
</file>

<file path="apiserver/tests/automated/test_get_all_ex_filters.py">
class TestGetAllExFilters(TestService)
⋮----
def test_no_tags_filter(self)
⋮----
task = self._temp_task(tags=["test"])
task_no_tags = self._temp_task()
tasks = [task, task_no_tags]
⋮----
res = self.api.tasks.get_all_ex(
⋮----
def test_list_filters(self)
⋮----
tags = ["a", "b", "c", "d"]
tasks = [self._temp_task(tags=tags[:i]) for i in range(len(tags) + 1)]
⋮----
# invalid params check
⋮----
# test any condition
⋮----
# test all condition
⋮----
# test combination
⋮----
def _temp_task(self, **kwargs)
</file>

<file path="apiserver/tests/automated/test_models.py">
MODEL_CANNOT_BE_UPDATED_CODES = (400, 203)
TASK_CANNOT_BE_UPDATED_CODES = (400, 110)
PUBLISHED = "published"
IN_PROGRESS = "in_progress"
⋮----
class TestModelsService(TestService)
⋮----
def setUp(self, version="2.9")
⋮----
def test_delete_model_for_task(self)
⋮----
# non published task
⋮----
task = self.api.tasks.get_by_id(task=task_id).task
⋮----
res = self.api.models.delete(model=model_id)
⋮----
# published task
⋮----
res = self.api.models.delete(model=model_id, force=True)
⋮----
def test_publish_output_model_running_task(self)
⋮----
def test_publish_output_model_running_task_no_task_publish(self)
⋮----
res = self.api.models.set_ready(model=model_id, publish_task=False)
assert res.updated == 1  # model updated
⋮----
def test_publish_output_model_running_task_force_task_publish(self)
⋮----
def test_publish_output_model_published_task(self)
⋮----
res = self.api.models.set_ready(model=model_id)
⋮----
def test_publish_output_model_stopped_task(self)
⋮----
# cannot publish already published model
⋮----
def test_publish_output_model_no_task(self)
⋮----
model_id = self.create_temp(
⋮----
def test_publish_task_with_output_model(self)
⋮----
res = self.api.tasks.publish(task=task_id)
⋮----
def test_publish_task_with_published_model(self)
⋮----
def test_publish_task_no_output_model(self)
⋮----
task_id = self.create_temp(service="tasks", type="testing", name="server-test")
⋮----
def test_get_models_stats(self)
⋮----
model1 = self._create_model(labels={"hello": 1, "world": 2})
model2 = self._create_model(labels={"foo": 1})
model3 = self._create_model()
⋮----
# no stats
res = self.api.models.get_all_ex(id=[model1, model2, model3]).models
⋮----
# stats
res = self.api.models.get_all_ex(
⋮----
stats = {m.id: m.stats.labels_count for m in res}
⋮----
def test_update_model_iteration_with_task(self)
⋮----
task_id = self._create_task()
model_id = self._create_model()
⋮----
def test_update_model_for_task_iteration(self)
⋮----
res = self.api.models.update_for_task(
⋮----
model_id = res.id
⋮----
def test_get_frameworks(self)
⋮----
framework_1 = "Test framework 1"
framework_2 = "Test framework 2"
⋮----
# create model on top level
⋮----
# create model under a project as make it inherit its framework from the task
project = self.create_temp("projects", name="Frameworks test", description="")
task = self._create_task(project=project, execution=dict(framework=framework_2))
⋮----
# get all frameworks
res = self.api.models.get_frameworks()
⋮----
# get frameworks under the project
res = self.api.models.get_frameworks(projects=[project])
⋮----
# empty result
⋮----
def test_delete_many_with_files(self)
⋮----
models = [
⋮----
def test_make_public(self)
⋮----
m1 = self._create_model(name="public model test")
⋮----
# model with company_origin not set to the current company cannot be converted to private
⋮----
# public model can be retrieved but not updated
res = self.api.models.make_public(ids=[m1])
⋮----
res = self.api.models.get_all(id=[m1])
⋮----
# task made private again and can be both retrieved and updated
res = self.api.models.make_private(ids=[m1])
⋮----
def _assert_task_status(self, task_id, status)
⋮----
def _assert_model_ready(self, model_id, ready)
⋮----
model = self.api.models.get_by_id(model=model_id)["model"]
⋮----
def _assert_update_model_failure(self)
⋮----
def _assert_update_task_failure(self)
⋮----
def _create_model(self, **kwargs)
⋮----
def _create_task(self, **kwargs)
⋮----
task_id = self.create_temp(
⋮----
def _create_task_and_model(self)
⋮----
execution_model_id = self.create_temp(
⋮----
output_model_id = self.api.models.update_for_task(
</file>

<file path="apiserver/tests/automated/test_move_under_project.py">
class TestMoveUnderProject(TestService)
⋮----
entity_name = "test move"
⋮----
def setUp(self, version="2.12")
⋮----
def test_move(self)
⋮----
# task move into the new project
task = self._temp_task()
project = self.api.tasks.move(ids=[task], project_name=self.entity_name).project_id
tasks = self.api.tasks.get_all_ex(id=[task]).tasks
⋮----
projects = self.api.projects.get_all_ex(id=[project]).projects
⋮----
# task clone
p2_name = "project_for_clone"
res = self.api.tasks.clone(task=task, new_project_name=p2_name)
task2 = res.id
project_data = res.new_project
⋮----
tasks = self.api.tasks.get_all_ex(id=[task2]).tasks
project2 = tasks[0].project.id
⋮----
projects = self.api.projects.get_all_ex(id=[project2]).projects
⋮----
# move to the root project
⋮----
# model move into existing project referenced by name
model = self._temp_model()
⋮----
models = self.api.models.get_all_ex(id=[model]).models
⋮----
def _temp_task(self)
⋮----
task_input = dict(
⋮----
def _temp_model(self)
⋮----
model_input = dict(name=self.entity_name, uri="file:///a/b", labels={})
</file>

<file path="apiserver/tests/automated/test_organization.py">
class TestOrganization(TestService)
⋮----
def test_get_user_companies(self)
⋮----
company = self.api.organization.get_user_companies().companies[0]
⋮----
users = company.owners
</file>

<file path="apiserver/tests/automated/test_paging_and_scrolling.py">
class TestPagingAndScrolling(TestService)
⋮----
name_prefix = f"Test paging "
⋮----
def setUp(self, **kwargs)
⋮----
def _create_tasks(self)
⋮----
tasks = [
⋮----
def test_paging(self)
⋮----
page_size = 10
⋮----
start = page * page_size
expected_size = min(page_size, len(self.task_ids) - start)
tasks = self._get_tasks(page=page, page_size=page_size,).tasks
⋮----
def test_scrolling(self)
⋮----
scroll_id = None
⋮----
res = self._get_tasks(size=page_size, scroll_id=scroll_id,)
⋮----
scroll_id = res.scroll_id
tasks = res.tasks
⋮----
# no more data in this scroll
tasks = self._get_tasks(size=page_size, scroll_id=scroll_id,).tasks
⋮----
# refresh brings all
tasks = self._get_tasks(
⋮----
def _get_tasks(self, **page_params)
⋮----
def _temp_task(self, name, **kwargs)
</file>

<file path="apiserver/tests/automated/test_pipelines.py">
class TestPipelines(TestService)
⋮----
task_hyperparams = {
⋮----
def test_controller_operations(self)
⋮----
task_name = "pipelines test"
⋮----
steps = [
ids = [task, *steps]
res = self.api.tasks.get_all_ex(id=ids, search_hidden=True)
⋮----
# stop
partial_ids = [task, steps[0]]
⋮----
res = self.api.tasks.get_all_ex(id=partial_ids, search_hidden=True)
⋮----
# archive/unarchive
⋮----
res = self.api.tasks.get_all_ex(id=ids, search_hidden=True, system_tags=["-archived"])
⋮----
# delete
⋮----
def test_delete_runs(self)
⋮----
queue = self.api.queues.get_default().id
⋮----
args = [{"name": "hello", "value": "test"}]
pipeline_tasks = [
tasks = self.api.tasks.get_all_ex(project=project).tasks
⋮----
# cannot delete all runs
⋮----
# successful deletion
res = self.api.pipelines.delete_runs(project=project, ids=pipeline_tasks)
⋮----
def test_start_pipeline(self)
⋮----
res = self.api.pipelines.start_pipeline(task=task, queue=queue, args=args)
pipeline_task = res.pipeline
⋮----
pipeline = self.api.tasks.get_all_ex(id=[pipeline_task]).tasks[0]
⋮----
# watched queue
queue = self._temp_queue("test pipelines")
⋮----
res = self.api.pipelines.start_pipeline(
⋮----
def _temp_project_and_task(self, name) -> Tuple[str, str]
⋮----
project = self.create_temp(
⋮----
def _temp_queue(self, queue_name, **kwargs)
</file>

<file path="apiserver/tests/automated/test_project_delete.py">
class TestProjectsDelete(TestService)
⋮----
def new_task(self, type="testing", **kwargs)
⋮----
def new_model(self, **kwargs)
⋮----
def new_project(self, name=None, **kwargs)
⋮----
def test_delete_fails_with_active_task(self)
⋮----
project = self.new_project()
⋮----
res = self.api.projects.validate_delete(project=project)
⋮----
def test_delete_with_archived_task(self)
⋮----
def test_delete_fails_with_active_model(self)
⋮----
def test_delete_with_archived_model(self)
⋮----
def test_delete_dataset(self)
⋮----
name = "Test datasets delete"
project = self.new_project(name=name)
dataset = self.new_project(f"{name}/.datasets/test dataset", system_tags=["dataset"])
task = self.new_task(project=dataset, system_tags=["dataset"])
⋮----
def test_delete_pipeline(self)
⋮----
name = "Test pipelines delete"
⋮----
pipeline = self.new_project(f"{name}/.pipelines/test pipeline", system_tags=["pipeline"])
task = self.new_task(project=pipeline, type="controller", system_tags=["pipeline"])
</file>

<file path="apiserver/tests/automated/test_project_tags.py">
class TestProjectTags(TestService)
⋮----
def test_project_own_tags(self)
⋮----
p1_tags = ["Tag 1", "Tag 2"]
p1 = self.create_temp(
p2_tags = ["Tag 1", "Tag 3"]
p2 = self.create_temp(
⋮----
res = self.api.projects.get_project_tags(projects=[p1, p2])
⋮----
res = self.api.projects.get_project_tags(
⋮----
def test_project_entities_tags(self)
⋮----
tags_1 = ["Test tag 1", "Test tag 2"]
tags_2 = ["Test tag 3", "Test tag 4"]
⋮----
p1 = self.create_temp("projects", name="Test tags1", description="test")
task1_1 = self.new_task(project=p1, tags=tags_1[:1])
task1_2 = self.new_task(project=p1, tags=tags_1[1:])
⋮----
p2 = self.create_temp("projects", name="Test tasks2", description="test")
task2 = self.new_task(project=p2, tags=tags_2)
⋮----
# test tags per project
data = self.api.projects.get_task_tags(projects=[p1])
⋮----
data = self.api.projects.get_model_tags(projects=[p1])
⋮----
data = self.api.projects.get_task_tags(projects=[p2])
⋮----
# test tags for projects list
data = self.api.projects.get_task_tags(projects=[p1, p2])
⋮----
# test tags for all projects
⋮----
# test move to another project
⋮----
# test tags update
⋮----
def test_organization_tags(self)
⋮----
tag1 = datetime.utcnow().isoformat()
tag2 = "Orgtest tag2"
system_tag = "Orgtest system tag"
⋮----
model = self.new_model(tags=[tag1])
task = self.new_task(tags=[tag1])
data = self.api.organization.get_tags()
⋮----
data = self.api.organization.get_tags(include_system=True)
⋮----
data = self.api.organization.get_tags(
⋮----
def test_tags_api(self)
⋮----
p = self.create_temp("projects", name="Test tags api", description="test")
⋮----
# task
initial_tags = ["Task tag"]
task = self.new_task(project=p, tags=initial_tags)
data = self.api.projects.get_task_tags(projects=[p])
⋮----
new_tags = ["New task tag"]
⋮----
# model
initial_tags = ["Model tag"]
model = self.new_model(project=p, tags=initial_tags)
data = self.api.projects.get_model_tags(projects=[p])
⋮----
new_tags = ["New model tag"]
⋮----
def new_task(self, **kwargs)
⋮----
def new_model(self, **kwargs)
</file>

<file path="apiserver/tests/automated/test_project_usages.py">
class TestProjectUsages(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def test_usages(self)
⋮----
queue = self._temp_queue("Usages test 1")
⋮----
project_name = f"Project Usage {uuid4()}"
project = self._temp_project(project_name)
child_project_name = f"{project_name}/Child1"
child_project = self._temp_project(child_project_name)
task_root_running = self._create_temp_queued_task(
⋮----
task_child_failed = self._create_temp_queued_task(
⋮----
task_child_completed = self._temp_task(
⋮----
task_child_not_running = self._create_temp_queued_task(
⋮----
from_date = (datetime.now().astimezone() - timedelta(days=5)).isoformat()
to_date = datetime.now().astimezone().isoformat()
res = self.api.organization.get_project_usages(
⋮----
delete_params = dict(can_fail=True, force=True)
⋮----
def _temp_project(self, name, **kwargs)
⋮----
def _temp_queue(self, queue_name, **kwargs)
⋮----
def _temp_task(self, task_name, is_development=False, **kwargs)
⋮----
task_input = dict(
⋮----
def _create_temp_queued_task(self, task_name, queue, **kwargs) -> str
⋮----
task_id = self._temp_task(task_name, **kwargs)
⋮----
def _create_temp_worker(self, worker, queue, **more)
</file>

<file path="apiserver/tests/automated/test_projection.py">
log = config.logger(__file__)
⋮----
class TestProjection(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def _temp_task(self, **kwargs)
⋮----
def _temp_project(self)
⋮----
def test_overlapping_fields(self)
⋮----
message = "task started"
task_id = self._temp_task()
⋮----
task = self.api.tasks.get_all_ex(
⋮----
def test_task_projection(self)
⋮----
project = self._temp_project()
task1 = self._temp_task(project=project)
task2 = self._temp_task(project=project)
⋮----
res = self.api.tasks.get_all_ex(
⋮----
def test_exclude_projection(self)
</file>

<file path="apiserver/tests/automated/test_projects_edit.py">
log = config.logger(__file__)
⋮----
class TestProjectsEdit(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def test_make_public(self)
⋮----
p1 = self.create_temp("projects", name="Test public", description="test")
⋮----
# project with company_origin not set to the current company cannot be converted to private
⋮----
# public project can be retrieved but not updated
res = self.api.projects.make_public(ids=[p1])
⋮----
res = self.api.projects.get_all(id=[p1])
⋮----
# task made private again and can be both retrieved and updated
res = self.api.projects.make_private(ids=[p1])
⋮----
def test_project_name_uniqueness(self)
⋮----
name1 = "Test name1"
p1 = self.create_temp("projects", name=name1, description="test")
⋮----
p2 = self.create_temp("projects", name=name1, description="test")
p2 = self.create_temp("projects", name="Test name2", description="test")
</file>

<file path="apiserver/tests/automated/test_queue_model_metadata.py">
class TestQueueAndModelMetadata(TestService)
⋮----
meta1 = {"test_key": {"key": "test_key", "type": "str", "value": "test_value"}}
⋮----
def test_queue_metas(self)
⋮----
queue_id = self._temp_queue("TestMetadata", metadata=self.meta1)
⋮----
def test_models_metas(self)
⋮----
service = self.api.models
entity = "model"
model_id = self._temp_model("TestMetadata", metadata=self.meta1)
⋮----
model_id = self._temp_model("TestMetadata1")
⋮----
def test_project_meta_query(self)
⋮----
project = self.temp_project(name="MetaParent")
test_key = "test_key"
test_key2 = "test_key2"
test_value = "test_value"
test_value2 = "test_value2"
model_id = self._temp_model(
res = self.api.projects.get_model_metadata_keys()
⋮----
res = self.api.projects.get_model_metadata_keys(include_subprojects=False)
⋮----
model = self.api.models.get_all_ex(
⋮----
res = self.api.projects.get_model_metadata_values(key=test_key)
⋮----
assert_meta = partial(self._assertMeta, service=service, entity=entity)
⋮----
meta2 = {
⋮----
updates = [
res = service.add_or_update_metadata(**{entity: _id, "metadata": updates})
⋮----
res = service.delete_metadata(
⋮----
# noinspection PyTypeChecker
⋮----
res = service.get_all_ex(id=[_id])[f"{entity}s"][0]
⋮----
def _temp_queue(self, name, **kwargs)
⋮----
def _temp_model(self, name: str, **kwargs)
⋮----
def temp_project(self, **kwargs) -> str
</file>

<file path="apiserver/tests/automated/test_queues.py">
class TestQueues(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def test_default_queue(self)
⋮----
res = self.api.queues.get_default()
⋮----
def test_create_update_delete(self)
⋮----
queue = self._temp_queue("TempTest", tags=["hello", "world"])
res = self.api.queues.update(queue=queue, tags=["test"])
⋮----
def test_queue_metrics(self)
⋮----
queue_id = self._temp_queue("TestTempQueue")
⋮----
task2 = self._create_temp_queued_task("temp task 2", queue_id)
⋮----
to_date = utc_now_tz_aware()
from_date = to_date - timedelta(hours=1)
res = self.api.queues.get_queue_metrics(
⋮----
def test_add_remove_clear(self)
⋮----
queue1 = self._temp_queue("TestTempQueue1")
queue2 = self._temp_queue("TestTempQueue2")
⋮----
task_names = ["TempDevTask1", "TempDevTask2"]
tasks = [self._temp_task(name) for name in task_names]
⋮----
# remove task with and without status update
res = self.api.queues.remove_task(task=tasks[0], queue=queue1)
⋮----
res = self.api.tasks.get_by_id(task=tasks[0])
⋮----
res = self.api.queues.remove_task(task=tasks[1], queue=queue1, update_task_status=True)
⋮----
res = self.api.tasks.get_by_id(task=tasks[1])
⋮----
res = self.api.queues.get_by_id(queue=queue1)
⋮----
# add task
res = self.api.queues.add_task(queue=queue2, task=tasks[0])
⋮----
res = self.api.queues.get_by_id(queue=queue2)
⋮----
# clear queue
res = self.api.queues.clear_queue(queue=queue1)
⋮----
res = self.api.queues.clear_queue(queue=queue2)
⋮----
def test_hidden_queues(self)
⋮----
hidden_name = "TestHiddenQueue"
hidden_queue = self._temp_queue(hidden_name, system_tags=["k8s-glue"])
non_hidden_queue = self._temp_queue("TestNonHiddenQueue")
⋮----
queues = self.api.queues.get_all_ex().queues
ids = {q.id for q in queues}
⋮----
queues = self.api.queues.get_all_ex(search_hidden=True).queues
⋮----
queues = self.api.queues.get_all_ex(name=f"^{hidden_name}$").queues
⋮----
queues = self.api.queues.get_all_ex(id=[hidden_queue]).queues
⋮----
def test_reset_task(self)
⋮----
queue = self._temp_queue("TestTempQueue")
task = self._temp_task("TempTask", is_development=True)
⋮----
res = self.api.queues.get_by_id(queue=queue)
⋮----
res = self.api.tasks.reset(task=task)
⋮----
def test_enqueue_dev_task(self)
⋮----
task_name = "TempDevTask"
task = self._temp_task(task_name, is_development=True)
⋮----
res = self.api.tasks.get_by_id(task=task)
⋮----
def test_dequeue_not_queued_task(self)
⋮----
# queue = self._temp_queue("TestTempQueue")
⋮----
task = self._temp_task(task_name)
self.api.tasks.edit(task=task, status="queued")  # , execution={"queue": queue})
⋮----
def test_dequeue_from_deleted_queue(self)
⋮----
def test_max_queue_entries(self)
⋮----
tasks = [
⋮----
num = self.api.queues.get_num_entries(queue=queue).num
⋮----
task_id = self.api.queues.peek_task(queue=queue).task
⋮----
res = self.api.queues.get_all(id=[queue]).queues[0]
⋮----
res = self.api.queues.get_all(id=[queue], max_task_entries=2).queues[0]
⋮----
res = self.api.queues.get_all_ex(id=[queue]).queues[0]
⋮----
res = self.api.queues.get_all_ex(id=[queue], max_task_entries=2).queues[0]
⋮----
def test_move_task(self)
⋮----
# no change in position
new_pos = self.api.queues.move_task_to_front(
⋮----
# move backwards in the middle
new_pos = self.api.queues.move_task_backward(
⋮----
changed_tasks = tasks[1:3] + [tasks[0], tasks[3]]
⋮----
# move backwards beyond the end
⋮----
changed_tasks = tasks[1:] + [tasks[0]]
⋮----
# move forwards in the middle
new_pos = self.api.queues.move_task_forward(
⋮----
changed_tasks = [tasks[1], tasks[0]] + tasks[2:]
⋮----
# move forwards beyond the beginning
⋮----
# move to back
new_pos = self.api.queues.move_task_to_back(
⋮----
# move to front
⋮----
def test_get_all_ex(self)
⋮----
queue_name = "TestTempQueue1"
queue_display_name = "Test display name"
queue_tags = ["Test1", "Test2"]
queue = self._temp_queue(queue_name, display_name=queue_display_name, tags=queue_tags)
⋮----
res = self.api.queues.get_all_ex(name="TestTempQueue*").queues
⋮----
workers = [
⋮----
def assertMetricQueues(self, queues_data, queue_id)
⋮----
queue_res = queues_data[0]
⋮----
dates_len = len(queue_res["dates"])
⋮----
dates_in_sec = [d / 1000 for d in queue_res["dates"]]
⋮----
queue = next(q for q in queues if q.id == queue_id)
⋮----
def assertTaskTags(self, task, system_tags)
⋮----
execution = self.api.tasks.get_by_id_ex(
⋮----
def assertGetNextTasks(self, queue, tasks)
⋮----
res = self.api.queues.get_next_task(queue=queue)
⋮----
sort_key = itemgetter("name")
⋮----
res = self.api.workers.get_all()
worker_ids = {w["key"] for w in workers}
found = [w for w in res.workers if w.key in worker_ids]
⋮----
def _temp_queue(self, queue_name, **kwargs)
⋮----
def _temp_task(self, task_name, is_testing=False, is_development=False)
⋮----
task_input = dict(
⋮----
def _create_temp_queued_task(self, task_name, queue) -> dict
⋮----
task_id = self._temp_task(task_name)
⋮----
def _create_temp_running_task(self, task_name) -> dict
⋮----
task_id = self._temp_task(task_name, is_testing=True)
⋮----
def _create_temp_worker(self, worker, queue)
⋮----
task = self._create_temp_running_task(f"temp task for worker {worker}")
</file>

<file path="apiserver/tests/automated/test_reports.py">
class TestReports(TestService)
⋮----
def _delete_project(self, name)
⋮----
existing_project = first(
⋮----
def test_create_update_move(self)
⋮----
task_name = "Rep1"
comment = "My report"
tags = ["hello"]
⋮----
# report creates a hidden task under hidden .reports subproject
⋮----
task_id = self._temp_report(name=task_name, comment=comment, tags=tags)
task = self.api.tasks.get_all_ex(id=[task_id]).tasks[0]
⋮----
projects = self.api.projects.get_all_ex(name=r"^\.reports$", allow_public=False).projects
⋮----
project = self.api.projects.get_all_ex(
⋮----
ret = self.api.reports.get_tags()
⋮----
# update is working on draft reports
new_comment = "My new comment"
res = self.api.reports.update(
⋮----
# update on tags or rename can be done for published report too
⋮----
# move under another project autodeletes the empty project
new_project_name = "Reports Test"
⋮----
task2_id = self._temp_report(name="Rep2")
new_project_id = self.api.reports.move(
new_project = self.api.projects.get_all_ex(id=[new_project_id]).projects[0]
⋮----
tasks = self.api.tasks.get_all_ex(
⋮----
project_id = self.api.reports.move(task=task2_id, project=None).project_id
project = self.api.projects.get_all_ex(id=[project_id]).projects[0]
⋮----
def test_root_reports(self)
⋮----
root_report = self._temp_report(name="Rep1")
project_name = "Test reports"
project = self._temp_project(name=project_name)
project_report = self._temp_report(name="Rep2", project=project)
⋮----
projects = self.api.projects.get_all_ex(
⋮----
p = projects[0]
⋮----
reports = self.api.reports.get_all_ex().tasks
⋮----
reports = self.api.reports.get_all_ex(project=project).tasks
⋮----
reports = self.api.reports.get_all_ex(project=[None]).tasks
⋮----
def test_reports_search(self)
⋮----
report_task = self._temp_report(name="Rep1")
non_report_task = self._temp_task(name="hello")
res = self.api.reports.get_all_ex(
⋮----
def test_reports_task_data(self)
⋮----
non_reports_task_name = "test non-reports"
⋮----
non_report_task = self._temp_model(name=non_reports_task_name)
event_args = {"model_event": True}
⋮----
non_report_task = self._temp_task(name=non_reports_task_name)
event_args = {}
debug_image_events = [
plot_events = [
scalar_events = [
⋮----
res = self.api.reports.get_task_data(
⋮----
task_events = res.debug_images[0]
⋮----
task_metrics = res.single_value_metrics[0]
⋮----
tasks = nested_get(res.scalar_metrics_iter_histogram, (m, v))
⋮----
tasks = nested_get(res.plots, (m, v))
⋮----
task_plots = tasks[non_report_task]
⋮----
iter_plots = task_plots["1"]
⋮----
ev = iter_plots.plots[0]
⋮----
@staticmethod
    def _create_task_event(type_, task, iteration, **kwargs)
⋮----
delete_params = {"force": True}
⋮----
def _temp_project(self, name, **kwargs)
⋮----
def _temp_report(self, name, **kwargs)
⋮----
def _temp_task(self, name, **kwargs)
⋮----
def _temp_model(self, name="test model events", **kwargs)
⋮----
def send_batch(self, events)
</file>

<file path="apiserver/tests/automated/test_serving.py">
class TestServing(TestService)
⋮----
def test_status_report(self)
⋮----
container_id1 = "container_1"
container_id2 = "container_2"
url = "http://test_url"
reference = [
container_infos = [
⋮----
"container_id": container_id,  # required
"endpoint_name": "my endpoint",  # required
"endpoint_url": url,  # can be omitted for register but required for status report
"model_name": "my model",  # required
"model_source": "s3//my_bucket",  # optional right now
"model_version": "3.1.0",  # optional right now
"preprocess_artifact": "some string here",  # optional right now
"input_type": "another string here",  # optional right now
"input_size": 9_000_000,  # optional right now, bytes
"tags": ["tag1", "tag2"],  # optional
"system_tags": None,  # optional
⋮----
# registering instances
⋮----
timeout=100,  # expiration timeout in seconds. Optional, the default value is 600
⋮----
mul = idx + 1
⋮----
requests_min=5 * mul,  # requests per minute
latency_ms=100 * mul,  # average latency
machine_stats={  # the same structure here as used by worker status_reports
⋮----
# getting endpoints and endpoint details
endpoints = self.api.serving.get_endpoints().endpoints
⋮----
details = self.api.serving.get_endpoint_details(endpoint_url=url)
⋮----
# make sure that the first call did not invalidate anything
new_details = self.api.serving.get_endpoint_details(endpoint_url=url)
⋮----
# charts
sleep(5)  # give time to ES to accomodate data
to_date = int(time()) + 40
from_date = to_date - 100
⋮----
res = self.api.serving.get_endpoint_metrics_history(
⋮----
length = len(res.total.dates)
⋮----
# unregistering containers
</file>

<file path="apiserver/tests/automated/test_subprojects.py">
class TestSubProjects(TestService)
⋮----
def test_dataset_stats(self)
⋮----
project = self._temp_project(name="Dataset test", system_tags=["dataset"])
res = self.api.organization.get_entities_count(
⋮----
task = self._temp_task(project=project)
data = self.api.projects.get_all_ex(
⋮----
def test_query_children_system_tags(self)
⋮----
test_root_name = "TestQueryChildrenTags"
test_root = self._temp_project(name=test_root_name)
project1 = self._temp_project(name=f"{test_root_name}/project1")
project2 = self._temp_project(name=f"{test_root_name}/project2")
⋮----
projects = self.api.projects.get_all_ex(
⋮----
p = projects[0]
⋮----
# new filter
⋮----
def test_query_children(self)
⋮----
test_root_name = "TestQueryChildren"
⋮----
dataset_tags = ["hello", "world"]
dataset_project = self._temp_project(
⋮----
pipeline_project = self._temp_project(
⋮----
report_project = self._temp_project(name=f"{test_root_name}/Project3")
⋮----
def test_project_aggregations(self)
⋮----
"""This test requires user with user_auth_only... credentials in db"""
user2_client = APIClient(
⋮----
basename = "Pr1"
child = self._temp_project(name=f"Aggregation/{basename}", client=user2_client)
project = self.api.projects.get_all_ex(name="^Aggregation$").projects[0].id
child_project = self.api.projects.get_all_ex(id=[child]).projects[0]
⋮----
user = self.api.users.get_current_user().user.id
⋮----
# test aggregations on project with empty subprojects
res = self.api.users.get_all_ex(active_in_projects=[project])
⋮----
res = self.api.projects.get_all_ex(id=[project], active_users=[user])
⋮----
res = self.api.models.get_frameworks(projects=[project])
⋮----
res = self.api.tasks.get_types(projects=[project])
⋮----
res = self.api.projects.get_task_parents(projects=[project])
⋮----
res = self.api.projects.get_user_names(projects=[project])
⋮----
# test aggregations with non-empty subprojects
task1 = self._temp_task(project=child)
⋮----
user2_task = self._temp_task(project=child, client=user2_client)
framework = "Test framework"
⋮----
res = self.api.projects.get_all_ex(id=[project], include_stats=True)
⋮----
res = self.api.projects.get_all_ex(
⋮----
def _assert_ids(self, actual: Sequence[dict], expected: Sequence[str])
⋮----
def test_project_operations(self)
⋮----
# create
⋮----
project1 = self._temp_project(name="Root1/Pr1")
project1_child = self._temp_project(name="Root1/Pr1/Pr2")
⋮----
# update
⋮----
new_basename = "Pr2"
res = self.api.projects.update(project=project1, name=f"Root1/{new_basename}")
⋮----
res = self.api.projects.get_by_id(project=project1)
⋮----
res = self.api.projects.get_by_id(project=project1_child)
⋮----
# move
res = self.api.projects.move(project=project1, new_location="Root2")
⋮----
# merge
⋮----
project1_parent = self._getProjectParent(project1)
⋮----
project_with_task_parent = self._getProjectParent(project_with_task)
⋮----
# self._assertTags(project_id=None)
⋮----
merge_source = self.api.projects.get_by_id(
res = self.api.projects.merge(
⋮----
res = self.api.projects.get_by_id(project=project_with_task)
⋮----
# delete
⋮----
res = self.api.projects.delete(project=project1, force=True)
⋮----
res = self.api.tasks.get_by_id(task=active).task
⋮----
# self._assertTags(project_id=None, tags=[], system_tags=[])
⋮----
def _getProjectParent(self, project_id: str)
⋮----
res = self.api.projects.get_task_tags(
⋮----
res = self.api.organization.get_tags(include_system=True)
⋮----
def test_get_all_search_options(self)
⋮----
project1 = self._temp_project(name="project1")
project2 = self._temp_project(name="project1/project2")
⋮----
# local search finds only at the specified level
⋮----
res = self.api.projects.get_all_ex(name="project1", parent=[project1]).projects
⋮----
# basename search
⋮----
# global search finds all or below the specified level
res = self.api.projects.get_all_ex(name="project1").projects
⋮----
project4 = self._temp_project(name="project1/project2/project1")
res = self.api.projects.get_all_ex(name="project1", parent=[project2]).projects
⋮----
res = self.api.projects.get_all_ex(basename="project2").projects
⋮----
def test_include_subprojects(self)
⋮----
# tasks
res = self.api.tasks.get_all_ex(project=project1).tasks
⋮----
res = self.api.tasks.get_all(project=project1).tasks
⋮----
res = self.api.tasks.get_all_ex(
⋮----
res = self.api.tasks.get_all(project=project1, include_subprojects=True).tasks
⋮----
# models
res = self.api.models.get_all_ex(project=project1).models
⋮----
res = self.api.models.get_all(project=project1).models
⋮----
res = self.api.models.get_all_ex(
⋮----
res = self.api.models.get_all(project=project1, include_subprojects=True).models
⋮----
def test_get_all_with_check_own_contents(self)
⋮----
project2 = self._temp_project(name="project2x")
⋮----
res1 = next(p for p in res if p.id == project1)
⋮----
res2 = next(p for p in res if p.id == project2)
⋮----
def test_public_names_clash(self)
⋮----
# cannot create a project with a name that match public existing project
⋮----
project = self._temp_project(name="ClearML Examples")
⋮----
# cannot create a subproject under a public project
⋮----
project = self._temp_project(name="ClearML Examples/my project")
⋮----
def test_get_all_with_stats(self)
⋮----
project2 = self._temp_project(name="project2")
res = self.api.projects.get_all(shallow_search=True).projects
⋮----
project1 = first(p.id for p in res if p.name == "project1")
⋮----
def _run_tasks(self, *tasks)
⋮----
"""Imitate 1 second of running"""
⋮----
def _temp_project_with_tasks(self, name) -> Tuple[str, Tuple[str, str]]
⋮----
pr_id = self._temp_project(name=name)
task_active = self._temp_task(project=pr_id)
task_archived = self._temp_task(
⋮----
delete_params = dict(can_fail=True, force=True)
⋮----
def _temp_project(self, name, client=None, **kwargs)
⋮----
def _temp_report(self, name, **kwargs)
⋮----
def _temp_task(self, client=None, name=None, type=None, **kwargs)
⋮----
def _temp_model(self, **kwargs)
</file>

<file path="apiserver/tests/automated/test_tags.py">
log = config.logger(__file__)
⋮----
class TestTags(TestService)
⋮----
def setUp(self, version="2.4")
⋮----
def testPartition(self)
⋮----
def testBackwardsCompatibility(self)
⋮----
new_api = self.api
⋮----
entity_tags = {
⋮----
create_func = getattr(self, f"_temp_{name}")
_id = create_func(tags=[system_tag, "test"])
names = f"{name}s"
⋮----
# when accessed through the old api all the tags are in the tags field
⋮----
entities = self._send(
⋮----
# when accessed through the new api the tags are in tags and system_tags fields
⋮----
# update operation, remove system tag through the old api
⋮----
def testProjectTags(self)
⋮----
pr_id = self._temp_project(system_tags=["default"])
⋮----
# Test getting project with system tags
projects = self.api.projects.get_all(name="Test tags").projects
⋮----
projects = self.api.projects.get_all(
⋮----
# Test task statistics and delete
task1_id = self._temp_task(
⋮----
task2_id = self._temp_task(
projects = self.api.projects.get_all_ex(name="Test tags").projects
⋮----
projects = self.api.projects.get_all_ex(
project = next(p for p in projects if p.id == pr_id)
⋮----
def testModelTags(self)
⋮----
model_id = self._temp_model(system_tags=["default"])
models = self.api.models.get_all_ex(
⋮----
def testQueueTags(self)
⋮----
q_id = self._temp_queue(system_tags=["default"])
queues = self.api.queues.get_all_ex(
⋮----
# test default queue
queues = self.api.queues.get_all(system_tags=["default"]).queues
⋮----
def testTaskTags(self)
⋮----
task_id = self._temp_task(
tasks = self.api.tasks.get_all_ex(
⋮----
# test development system tag
⋮----
task = self.api.tasks.get_by_id(task=task_id).task
⋮----
def assertProjectStats(self, project: AttrDict)
⋮----
def _run_task(self, task_id)
⋮----
"""Imitate 1 second of running"""
⋮----
def _temp_queue(self, **kwargs)
⋮----
def _temp_project(self, **kwargs)
⋮----
def _temp_model(self, **kwargs)
⋮----
def _temp_task(self, **kwargs)
⋮----
def _send(self, service, action, **kwargs)
⋮----
api = kwargs.pop("api", self.api)
⋮----
def assertGetById(self, service, entity, _id, tags, system_tags=None, **kwargs)
⋮----
entity = self._send(service, "get_by_id", **{entity: _id}, **kwargs)[entity]
⋮----
found = next((r for r in res if _id == r.id), None)
⋮----
def assertTagsEqual(self, tags: Sequence[str], expected_tags: Sequence[str])
</file>

<file path="apiserver/tests/automated/test_task_artifacts.py">
class TestTasksArtifacts(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def new_task(self, **kwargs) -> str
⋮----
def test_artifacts_set_get(self)
⋮----
artifacts = [
⋮----
# test create/get and get_all
task = self.new_task(execution={"artifacts": artifacts})
res = self.api.tasks.get_by_id(task=task).task
⋮----
res = self.api.tasks.get_all_ex(id=[task]).tasks[0]
⋮----
# test edit
⋮----
# test clone
task2 = self.api.tasks.clone(task=task).id
res = self.api.tasks.get_by_id(task=task2).task
⋮----
new_artifacts = [
new_task = self.api.tasks.clone(
res = self.api.tasks.get_by_id(task=new_task).task
⋮----
def test_artifacts_edit_delete(self)
⋮----
# test add_or_update
edit = [
res = self.api.tasks.add_or_update_artifacts(task=task, artifacts=edit)
artifacts = self._update_source(artifacts, edit)
⋮----
# test delete
⋮----
# test edit running task
⋮----
def _update_source(self, source: Sequence[dict], update: Sequence[dict])
⋮----
dict1 = {s["key"]: s for s in source}
dict2 = {u["key"]: u for u in update}
res = {
⋮----
def _assertTaskArtifacts(self, artifacts: Sequence[dict], task)
⋮----
task_artifacts: dict = task.execution.artifacts
</file>

<file path="apiserver/tests/automated/test_task_debug_images.py">
class TestTaskDebugImages(TestService)
⋮----
def setUp(self, version="2.12")
⋮----
def _temp_task(self, name="test task events")
⋮----
task_input = dict(
⋮----
@staticmethod
    def _create_task_event(task, iteration, **kwargs)
⋮----
def test_get_debug_image_sample(self)
⋮----
task = self._temp_task()
metric = "Metric1"
variant = "Variant1"
⋮----
# test empty
res = self.api.events.get_debug_image_sample(
⋮----
# test existing events
iterations = 10
unique_images = 4
events = [
⋮----
# if iteration is not specified then return the event from the last one
⋮----
# else from the specific iteration
iteration = 8
⋮----
def test_next_debug_image_sample(self)
⋮----
variant1 = "Variant1"
variant2 = "Variant2"
⋮----
# init scroll
⋮----
# navigate forwards
res = self.api.events.next_debug_image_sample(
⋮----
# navigate backwards
⋮----
def _assertEqualEvent(self, ev1: dict, ev2: dict)
⋮----
def test_task_debug_images(self)
⋮----
res = self.api.events.debug_images(metrics=[{"task": task}], iters=5)
⋮----
res = self.api.events.debug_images(
⋮----
# test not empty
metrics = {
⋮----
scroll_id = self._assertTaskMetrics(
⋮----
# test refresh
update = {
⋮----
# without refresh the metric states are not updated
⋮----
# with refresh there are new metrics and existing ones are updated
⋮----
expected_variants = set((m, var) for m, vars_ in expected_metrics.items() for var in vars_)
⋮----
def test_get_debug_images_navigation(self)
⋮----
variants = [("Variant1", 7), ("Variant2", 4)]
⋮----
# create events
⋮----
# init testing
unique_images = [unique for (_, unique) in variants]
scroll_id = None
assert_debug_images = partial(
⋮----
# test forward navigation
⋮----
scroll_id = assert_debug_images(scroll_id=scroll_id, expected_page=page)
⋮----
# test backwards navigation
scroll_id = assert_debug_images(
⋮----
# beyond the latest iteration and back
⋮----
# refresh
⋮----
data = res["metrics"][0]
⋮----
left_iterations = max(0, max(unique_images) - expected_page * iters)
⋮----
events_per_iter = sum(
⋮----
def send_batch(self, events)
</file>

<file path="apiserver/tests/automated/test_task_events.py">
class TestTaskEvents(TestService)
⋮----
delete_params = dict(can_fail=True, force=True)
default_task_name = "test task events"
⋮----
def _temp_project(self, name=default_task_name)
⋮----
def _temp_task(self, name=default_task_name, **kwargs)
⋮----
def _temp_model(self, name="test model events", **kwargs)
⋮----
@staticmethod
    def _create_task_event(type_, task, iteration, **kwargs)
⋮----
def test_task_metrics(self)
⋮----
tasks = {
events = [
⋮----
res = self.api.events.get_multi_task_metrics(
⋮----
def _assert_task_metrics(self, tasks: dict, event_type: str)
⋮----
res = self.api.events.get_task_metrics(tasks=list(tasks), event_type=event_type)
⋮----
res_metrics = next(
⋮----
def test_task_single_value_metrics(self)
⋮----
metric = "Metric1"
variant = "Variant1"
iter_count = 10
task = self._temp_task()
special_iteration = -(2 ** 31)
⋮----
# special iteration is present in the events retrieval
metric_param = {"metric": metric, "variants": [variant]}
res = self.api.events.scalar_metrics_iter_raw(
⋮----
# but not in the histogram
data = self.api.events.scalar_metrics_iter_histogram(task=task)
⋮----
# new api
res = self.api.events.get_task_single_value_metrics(tasks=[task]).tasks
⋮----
data = res[0]
⋮----
value = data["values"][0]
⋮----
# test metrics parameter
res = self.api.events.get_task_single_value_metrics(
⋮----
# update is working
task_data = self.api.tasks.get_by_id(task=task).task
last_metrics = first(first(task_data.last_metrics.values()).values())
⋮----
new_value = 1000
new_event = {
⋮----
data = self.api.events.get_task_single_value_metrics(tasks=[task]).tasks[0]
⋮----
def test_last_scalar_metrics(self)
⋮----
iter_count = 100
⋮----
# send 2 batches to check the interaction with already stored db value
# each batch contains multiple iterations
⋮----
metric_data = first(first(task_data.last_metrics.values()).values())
⋮----
res = self.api.events.get_task_latest_scalar_values(task=task)
⋮----
def test_model_events(self)
⋮----
model = self._temp_model(ready=False)
⋮----
# task log events are not allowed
log_event = self._create_task_event(
⋮----
# mixed batch
⋮----
# noinspection PyTypeChecker
⋮----
data = self.api.events.scalar_metrics_iter_histogram(
⋮----
metric_data = data.Metric0
⋮----
variant_data = metric_data.Variant0
⋮----
model_data = self.api.models.get_all_ex(
metric_data = first(first(model_data.last_metrics.values()).values())
⋮----
metrics = self.api.events.get_multi_task_metrics(
⋮----
variants = [f"Variant{i}" for i in range(5)]
⋮----
def test_error_events(self)
⋮----
# failure if no events added
⋮----
# success if at least one event added
res = self.send_batch(events)
⋮----
res = self.api.events.get_task_events(task=task)
⋮----
def test_task_logs(self)
⋮----
timestamp = es_factory.get_timestamp_millis()
⋮----
# test forward navigation
⋮----
# test backwards navigation
⋮----
# test order
⋮----
metric = "metric"
variant = "variant"
⋮----
res = self.api.events.get_task_log(task=task)
⋮----
res = self.api.events.get_task_log(task=task, metrics=[{"metric": metric}])
⋮----
# test clear
⋮----
res = self.api.events.get_task_log(
⋮----
expected_events = max(
⋮----
unique_events = len({ev.iter for ev in res.events})
⋮----
cmp_operator = operator.ge
⋮----
cmp_operator = operator.le
⋮----
def test_task_unique_metric_variants(self)
⋮----
project = self._temp_project()
task1 = self._temp_task(project=project)
task2 = self._temp_task(project=project)
metric1 = "Metric1"
metric2 = "Metric2"
⋮----
metrics = self.api.projects.get_unique_metric_variants(project=project).metrics
⋮----
metrics = self.api.projects.get_unique_metric_variants(ids=[task1, task2]).metrics
⋮----
metrics = self.api.projects.get_unique_metric_variants(ids=[task1]).metrics
⋮----
def test_task_metric_value_intervals_keys(self)
⋮----
def test_multitask_events_many_metrics(self)
⋮----
tasks = [
⋮----
metrics_count = 10
variants_count = 10
⋮----
data = self.api.events.multi_task_scalar_metrics_iter_histogram(tasks=tasks)
⋮----
# test metrics
data = self.api.events.multi_task_scalar_metrics_iter_histogram(
⋮----
metric_data = data[f"Metric{m}"]
⋮----
variant_data = metric_data[f"Variant{v}"]
⋮----
task_data = variant_data[t]
⋮----
def test_task_metric_raw(self)
⋮----
batch_size = 15
⋮----
res_iters = []
res_ys = []
calls = 0
⋮----
scroll_id = res.scroll_id
⋮----
def test_task_metric_value_intervals(self)
⋮----
data = self.api.events.scalar_metrics_iter_histogram(task=task, samples=100)
⋮----
data = self.api.events.scalar_metrics_iter_histogram(task=task, samples=10)
⋮----
def _assert_metrics_histogram(self, data, iters, samples)
⋮----
interval = iters // samples
⋮----
def test_multitask_plots(self)
⋮----
task1 = self._temp_task()
⋮----
task2 = self._temp_task()
⋮----
plots = self.api.events.get_multi_task_plots(tasks=[task1, task2]).plots
⋮----
plots = self.api.events.get_multi_task_plots(
⋮----
def test_task_plots(self)
⋮----
event = self._create_task_event("plot", task, 0)
⋮----
event1 = self._create_task_event("plot", task, 100)
⋮----
plots = self.api.events.get_task_plots(task=task).plots
⋮----
@unittest.skip("this test will run only if 'validate_plot_str' is set to true")
    def test_plots_validation(self)
⋮----
valid_plot_str = json.dumps({"data": []})
invalid_plot_str = "Not a valid json"
⋮----
event = self._create_task_event(
event1 = self._create_task_event(
⋮----
res = self.api.events.get_task_plots(task=task).plots
⋮----
def send_batch(self, events)
⋮----
def send(self, event)
</file>

<file path="apiserver/tests/automated/test_task_hyperparams.py">
class TestTasksHyperparams(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def new_task(self, **kwargs) -> Tuple[str, str]
⋮----
def test_hyperparams(self)
⋮----
legacy_params = {"legacy$1": "val1", "legacy2/name": "val2"}
new_params = [
new_params_dict = self._param_dict_from_list(new_params)
⋮----
# both params and hyper params are set correctly
old_params = self._new_params_from_legacy(legacy_params)
params_dict = new_params_dict.copy()
⋮----
res = self.api.tasks.get_by_id(task=task).task
⋮----
# returned as one list with params in the _legacy section
res = self.api.tasks.get_hyper_params(tasks=[task]).params[0]
⋮----
# replace section
replace_params = [
⋮----
# replace all
⋮----
# add and update
⋮----
# delete
new_to_delete = self._get_param_keys(new_params[1:])
old_to_delete = self._get_param_keys(old_params[:1])
⋮----
# delete section
⋮----
# project hyperparams
res = self.api.projects.get_hyper_parameters(project=project)
⋮----
# clone task
new_task = self.api.tasks.clone(
⋮----
res = self.api.tasks.get_hyper_params(tasks=[new_task]).params[0]
⋮----
# editing of started task
⋮----
# properties section can be edited/deleted in any task state without the flag
⋮----
@staticmethod
    def _get_param_keys(params: Sequence[dict]) -> List[dict]
⋮----
@staticmethod
    def _new_params_from_legacy(legacy: dict) -> List[dict]
⋮----
@staticmethod
    def _param_dict_from_list(params: Sequence[dict]) -> dict
⋮----
@staticmethod
    def _config_dict_from_list(config: Sequence[dict]) -> dict
⋮----
def test_configuration(self)
⋮----
legacy_config = {"design": "hello"}
new_config = [
new_config_dict = self._config_dict_from_list(new_config)
⋮----
old_config = self._new_config_from_legacy(legacy_config)
config_dict = new_config_dict.copy()
⋮----
# returned as one list
res = self.api.tasks.get_configurations(tasks=[task]).configurations[0]
⋮----
# names
res = self.api.tasks.get_configuration_names(tasks=[task]).configurations[0]
⋮----
res = self.api.tasks.get_configuration_names(
⋮----
# returned as one list with names filtering
res = self.api.tasks.get_configurations(
⋮----
replace_configs = [
⋮----
new_to_delete = self._get_config_keys(new_config[1:])
⋮----
res = self.api.tasks.get_configurations(tasks=[new_task]).configurations[0]
⋮----
# edit/delete of running task
⋮----
@staticmethod
    def _get_config_keys(config: Sequence[dict]) -> List[dict]
⋮----
@staticmethod
    def _new_config_from_legacy(legacy: dict) -> List[dict]
⋮----
def test_hyperparams_projection(self)
⋮----
legacy_param = {"legacy.1": "val1"}
new_params1 = [
new_params_dict1 = self._param_dict_from_list(new_params1)
⋮----
new_params2 = [
new_params_dict2 = self._param_dict_from_list(new_params2)
⋮----
old_params = self._new_params_from_legacy(legacy_param)
params_dict = new_params_dict1.copy()
⋮----
res = self.api.tasks.get_all_ex(id=[task1], only_fields=["hyperparams"]).tasks[
⋮----
res = self.api.tasks.get_all_ex(
⋮----
def test_numeric_ordering(self)
⋮----
params = [
tasks = [
⋮----
res = self.api.tasks.get_all_ex(id=tasks, order_by=["hyperparams.section1.param1"]).tasks
⋮----
res = self.api.tasks.get_all_ex(id=tasks, order_by=["-hyperparams.section1.param1"]).tasks
⋮----
def test_old_api(self)
⋮----
legacy_params = {"legacy.1": "val1", "TF_DEFINE/param2": "val2"}
⋮----
config = self._config_dict_from_list(
params = self._param_dict_from_list(self._new_params_from_legacy(legacy_params))
⋮----
old_api = APIClient(base_url="http://localhost:8008/v2.8")
task = old_api.tasks.get_all_ex(id=[task_id]).tasks[0]
⋮----
modified_params = {"legacy.2": "val2"}
modified_config = {"design": "by"}
</file>

<file path="apiserver/tests/automated/test_task_models.py">
class TestTaskModels(TestService)
⋮----
def setUp(self, version="2.13")
⋮----
def test_new_apis(self)
⋮----
# no models
empty_task = self.new_task()
⋮----
input_models = [
output_models = [
⋮----
# task creation with models
task = self.new_task(models={"input": input_models, "output": output_models})
⋮----
# add_or_update existing model
res = self.api.tasks.add_or_update_model(
⋮----
modified_input = deepcopy(input_models)
⋮----
# add_or_update new mode
⋮----
modified_output = deepcopy(output_models)
⋮----
# task editing
⋮----
# delete models
res = self.api.tasks.delete_models(
⋮----
def get_model_id(model: dict) -> Optional[str]
⋮----
id_ = model.get("model")
⋮----
def compare_models(actual: Sequence[dict], expected: Sequence[dict])
⋮----
def new_task(self, **kwargs)
⋮----
def new_model(self, name: str, **kwargs)
</file>

<file path="apiserver/tests/automated/test_task_parents.py">
class TestTaskParent(TestService)
⋮----
def setUp(self, version="2.12")
⋮----
def test_query_by_parents(self)
⋮----
parent = self.new_task()
child = self.new_task(name="Test parent task1", parent=parent)
tasks = self.api.tasks.get_all_ex(parent=[parent]).tasks
⋮----
tasks = self.api.tasks.get_all(parent=parent).tasks
⋮----
def test_query_by_project(self)
⋮----
# stand alone task
parent_sa_name = "Test parent parent standalone"
parent_sa = self.new_task(name=parent_sa_name)
⋮----
# tasks in projects
project_name = "Test parents project"
project = self.create_temp("projects", name=project_name, description="test")
⋮----
parent_name = "Test parent parent"
parent = self.new_task(project=project, name=parent_name)
⋮----
parents = self.api.projects.get_task_parents(projects=[project]).parents
⋮----
res = self.api.projects.get_task_parents()
parents = [p for p in res.parents if p.id in (parent, parent_sa)]
⋮----
def test_query_by_name(self)
⋮----
parent_names = [f"Parent{i}" for i in range(3)]
parents = [self.new_task(project=project, name=name) for name in parent_names]
⋮----
parents = self.api.projects.get_task_parents(
⋮----
res = self.api.projects.get_task_parents(
⋮----
def test_query_by_state(self)
⋮----
parent1_name = "Test parent parent1"
parent1 = self.new_task(project=project, name=parent1_name)
t1 = self.new_task(project=project, name="Test parent task1", parent=parent1)
⋮----
parent2_name = "Test parent parent2"
parent2 = self.new_task(project=project, name=parent2_name)
t2 = self.new_task(project=project, name="Test parent task2", parent=parent2)
⋮----
# No state filter
⋮----
# Active tasks
⋮----
# Archived tasks
⋮----
def new_task(self, **kwargs)
</file>

<file path="apiserver/tests/automated/test_task_plots.py">
class TestTaskPlots(TestService)
⋮----
def _temp_task(self, name="test task events")
⋮----
task_input = dict(
⋮----
@staticmethod
    def _create_task_event(task, iteration, **kwargs)
⋮----
plot_str = kwargs.get("plot_str")
⋮----
plot_str = "http://files.clear.ml/" + plot_str
⋮----
def test_get_plot_sample(self)
⋮----
task = self._temp_task()
metric = "Metric1"
variants = ["Variant1", "Variant2"]
⋮----
# test empty
res = self.api.events.get_plot_sample(task=task, metric=metric)
⋮----
# test existing events
iterations = 5
events = [
⋮----
# if iteration is not specified then return the event from the last one
⋮----
# else from the specific iteration
iteration = 3
res = self.api.events.get_plot_sample(
⋮----
def test_next_plot_sample(self)
⋮----
metric1 = "Metric1"
metric2 = "Metric2"
metrics = [
⋮----
# single metric navigation
# init scroll
res = self.api.events.get_plot_sample(task=task, metric=metric1)
⋮----
# navigate forwards
res = self.api.events.next_plot_sample(
⋮----
# navigate backwards
res = self.api.events.next_plot_sample(task=task, scroll_id=res.scroll_id)
⋮----
# all metrics navigation
⋮----
# next_iteration
⋮----
def compare_event(ev1, ev2)
⋮----
def test_task_plots(self)
⋮----
res = self.api.events.plots(metrics=[{"task": task}], iters=5)
⋮----
res = self.api.events.plots(
⋮----
# test not empty
metrics = {
⋮----
scroll_id = self._assertTaskMetrics(
⋮----
# test refresh
update = {
⋮----
# without refresh the metric states are not updated
⋮----
# with refresh there are new metrics and existing ones are updated
⋮----
expected_variants = set(
⋮----
def test_plots_navigation(self)
⋮----
iterations = 10
⋮----
# create events
⋮----
# init testing
scroll_id = None
assert_plots = partial(
⋮----
# test forward navigation
⋮----
scroll_id = assert_plots(scroll_id=scroll_id, expected_page=page)
⋮----
# test backwards navigation
scroll_id = assert_plots(
⋮----
# beyond the latest iteration and back
res = self.api.events.debug_images(
⋮----
# refresh
⋮----
data = res["metrics"][0]
⋮----
left_iterations = max(0, iterations - expected_page * iters)
⋮----
def send_batch(self, events)
</file>

<file path="apiserver/tests/automated/test_tasks_delete.py">
class TestTasksResetDelete(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def test_delete(self)
⋮----
# draft task can be deleted
task = self.new_task()
res = self.assert_delete_task(task)
⋮----
# published task can be deleted only with force flag
⋮----
# task with published children can only be deleted with force flag
⋮----
child = self.new_task(parent=task)
⋮----
res = self.assert_delete_task(task, force=True)
⋮----
# make sure that the child model is valid after the parent deletion
⋮----
# task with published model can only be deleted with force flag
⋮----
model = self.new_model()
⋮----
def test_return_file_urls(self)
⋮----
# empty task
⋮----
res = self.assert_delete_task(task, return_file_urls=True)
⋮----
artifact_urls = self.send_artifacts(task)
event_urls = self.send_debug_image_events(task)
⋮----
res = self.assert_delete_task(task, force=True, return_file_urls=True)
⋮----
self.assertFalse(set(res.urls.event_urls))  # event urls are not returned anymore
⋮----
def test_reset(self)
⋮----
res = self.api.tasks.reset(task=task)
⋮----
# published task can be reset only with force flag
⋮----
# test urls
⋮----
res = self.api.tasks.reset(task=task, force=True, return_file_urls=True)
⋮----
self.assertFalse(res.urls.event_urls)  # event urls are not returned anymore
⋮----
def test_model_delete(self)
⋮----
model = self.new_model(uri="test")
res = self.api.models.delete(model=model)
⋮----
def test_project_delete(self)
⋮----
# without delete_contents flag
project = self.new_project()
task = self.new_task(project=project)
res = self.api.tasks.get_by_id(task=task)
⋮----
res = self.api.projects.delete(project=project, force=True)
⋮----
# with delete_contents flag
⋮----
res = self.api.projects.delete(
⋮----
task = self.new_task(**kwargs)
⋮----
def assert_delete_task(self, task_id, force=False, return_file_urls=False)
⋮----
tasks = self.api.tasks.get_all_ex(id=[task_id]).tasks
⋮----
res = self.api.tasks.delete(
⋮----
def create_task_models(self, task, **kwargs) -> Tuple
⋮----
"""
        Update models from task and return only non public models
        """
ready_uri = "ready"
not_ready_uri = "not_ready"
model_ready = self.new_model(uri=ready_uri, **kwargs)
model_not_ready = self.new_model(uri=not_ready_uri, ready=False, **kwargs)
⋮----
def send_artifacts(self, task) -> Set[str]
⋮----
"""
        Add input and output artifacts and return output artifact names
        """
artifacts = [
# test create/get and get_all
⋮----
def send_model_events(self, model) -> Set[str]
⋮----
url1 = "http://link1"
url2 = "http://link2"
events = [
⋮----
def send_debug_image_events(self, task) -> Set[str]
⋮----
url_pattern = "url_{num}.txt"
⋮----
def send_plot_events(self, task) -> Set[str]
⋮----
plots = [
⋮----
def create_event(self, task, type_, iteration, **kwargs) -> dict
⋮----
def send_batch(self, events)
⋮----
name = "test task delete"
delete_params = dict(can_fail=True, force=True)
⋮----
def new_task(self, **kwargs)
⋮----
def new_model(self, **kwargs)
⋮----
def new_project(self, **kwargs)
⋮----
def publish_task(self, task_id)
</file>

<file path="apiserver/tests/automated/test_tasks_diff.py">
log = config.logger(__file__)
⋮----
class TestTasksDiff(TestService)
⋮----
def setUp(self, version="2.0")
⋮----
def new_task(self, **kwargs)
⋮----
def _compare_script(self, task_id, script)
⋮----
task = self.api.tasks.get_by_id(task=task_id).task
⋮----
def test_not_deleted(self)
⋮----
task_id = self.new_task()
script = dict(
⋮----
new_reqs = dict()
</file>

<file path="apiserver/tests/automated/test_tasks_edit.py">
log = config.logger(__file__)
⋮----
class TestTasksEdit(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def new_task(self, **kwargs)
⋮----
def new_model(self, **kwargs)
⋮----
def new_queue(self, **kwargs)
⋮----
def test_task_types(self)
⋮----
task = self.new_task(type="Unsupported")
⋮----
types = ["controller", "optimizer"]
p1 = self.create_temp("projects", name="Test tasks1", description="test")
task1 = self.new_task(project=p1, type=types[0])
p2 = self.create_temp("projects", name="Test tasks2", description="test")
task2 = self.new_task(project=p2, type=types[1])
⋮----
# all company types
res = self.api.tasks.get_types()
⋮----
# projects array
res = self.api.tasks.get_types(projects=[p1, p2])
⋮----
# single project
⋮----
res = self.api.tasks.get_types(projects=[p])
⋮----
def test_edit_model_ready(self)
⋮----
task = self.new_task()
model = self.new_model()
⋮----
def test_edit_model_not_ready(self)
⋮----
def test_edit_had_model_model_not_ready(self)
⋮----
ready_model = self.new_model()
⋮----
task = self.new_task(execution=dict(model=ready_model))
not_ready_model = self.new_model()
⋮----
def test_task_with_model_reset(self)
⋮----
# on task reset output model deleted
⋮----
model_id = self.api.models.update_for_task(task=task, uri="file:///b")["id"]
⋮----
# unless it is input of some task
⋮----
task_2 = self.new_task(execution=dict(model=model_id))
⋮----
def test_clone_task(self)
⋮----
script = dict(
execution = dict(parameters=dict(test="Test"))
tags = ["hello"]
system_tags = ["development", "test"]
task = self.new_task(
⋮----
new_name = "new test"
new_tags = ["by"]
execution_overrides = dict(framework="Caffe", model_labels={"test": 1.0})
new_task_id = self._clone_task(
new_task = self.api.tasks.get_by_id(task=new_task_id).task
⋮----
# self.assertEqual(new_task.execution.parameters, execution["parameters"])
⋮----
def test_model_check_in_clone(self)
⋮----
task = self.new_task(execution=dict(model=model))
⋮----
# task with deleted model still can be copied
⋮----
# unless check for refs is done
⋮----
# if the model is overriden then it is always checked
⋮----
def _clone_task(self, task, **kwargs)
⋮----
new_task = self.api.tasks.clone(task=task, **kwargs).id
⋮----
def test_make_public(self)
⋮----
# task is created as private and can be updated
⋮----
# task with company_origin not set to the current company cannot be converted to private
⋮----
# public task can be retrieved but not updated
res = self.api.tasks.make_public(ids=[task])
⋮----
res = self.api.tasks.get_all_ex(id=[task])
⋮----
# task made private again and can be both retrieved and updated
res = self.api.tasks.make_private(ids=[task])
⋮----
def test_archive_task(self)
⋮----
# non-existing task throws an exception
⋮----
system_tag = "existing-system-tag"
status_message = "test-status-message"
status_reason = "test-status-reason"
queue_id = self.new_queue()
⋮----
# Create two tasks with system_tags and enqueue one of them
dequeued_task_id = self.new_task(system_tags=[system_tag])
enqueued_task_id = self.new_task(system_tags=[system_tag])
⋮----
tasks = self.api.tasks.get_all_ex(id=[enqueued_task_id, dequeued_task_id]).tasks
⋮----
# Check that the queue does not contain the enqueued task anymore
queue = self.api.queues.get_by_id(queue=queue_id).queue
task_in_queue = next(
⋮----
def test_stopped_task_enqueue(self)
⋮----
task_id = self.new_task()
⋮----
projection = ["*", "execution.*"]
task = self.api.tasks.get_all_ex(id=task_id, projection=projection).tasks[0]
</file>

<file path="apiserver/tests/automated/test_tasks_filtering.py">
class TestTasksFiltering(TestService)
⋮----
def setUp(self, **kwargs)
⋮----
def test_hyperparam_values(self)
⋮----
project = self.temp_project()
param1 = ("Se$tion1", "pa__ram1", True)
param2 = ("Section2", "param2", False)
task_count = 5
⋮----
t = self.temp_task(project=project)
⋮----
res = self.api.projects.get_hyperparam_values(
⋮----
# search pattern
⋮----
def test_datetime_queries(self)
⋮----
tasks = [self.temp_task() for _ in range(5)]
now = datetime.utcnow()
⋮----
# date time syntax
res = self.api.tasks.get_all_ex(last_update=f">={now.isoformat()}").tasks
⋮----
res = self.api.tasks.get_all_ex(
⋮----
# _any_/_all_ queries
⋮----
# simplified range syntax
res = self.api.tasks.get_all_ex(last_update=[now.isoformat(), None]).tasks
⋮----
def test_range_queries(self)
⋮----
res = self.api.tasks.get_all_ex(started=[now.isoformat(), None]).tasks
⋮----
def temp_project(self, **kwargs) -> str
⋮----
def temp_task(self, **kwargs) -> str
</file>

<file path="apiserver/tests/automated/test_tasks_running.py">
class TestTasksRunning(TestService)
⋮----
STATUS_STOPPED = "stopped"
STATUS_COMPLETED = "completed"
STATUS_PUBLISHED = "published"
STATUS_RUNNING = "in_progress"
⋮----
def test_stop_regular_task(self)
⋮----
task_id = self._create_running_task()
data = self.api.tasks.stop(task=task_id).fields
⋮----
def test_stop_regular_task_with_active_worker(self)
⋮----
worker_id = "worker1"
⋮----
def test_stop_development_task(self)
⋮----
task_id = self._create_running_task(is_development=True)
⋮----
def test_completed_task(self)
⋮----
res = self.api.tasks.completed(task=task_id)
⋮----
res = self.api.tasks.completed(task=task_id, publish=True)
⋮----
def _create_running_task(self, is_development=False)
⋮----
task_input = dict(
⋮----
task_id = self.create_temp("tasks", **task_input)
</file>

<file path="apiserver/tests/automated/test_users.py">
log = config.logger(__file__)
⋮----
class TestUsersService(TestService)
⋮----
def setUp(self, version="2.8")
⋮----
def new_user(self)
⋮----
user_name = uuid4().hex
user_id = self.api.auth.create_user(
⋮----
def test_active_users(self)
⋮----
user_1 = self.new_user()
user_2 = self.new_user()
user_3 = self.new_user()
⋮----
model = (
⋮----
project = self.create_temp("projects", name="users test", description="")
task = (
⋮----
user_ids = [user_1, user_2, user_3]
# no projects filtering
users = self.api.users.get_all_ex(id=user_ids).users
⋮----
# all projects
users = self.api.users.get_all_ex(id=user_ids, active_in_projects=[]).users
⋮----
# specific project
users = self.api.users.get_all_ex(id=user_ids, active_in_projects=[project]).users
⋮----
def _assertUsers(self, expected: Sequence, users: Sequence)
⋮----
def test_no_preferences(self)
⋮----
user = self.new_user()
⋮----
def _test_update(self, user, tests)
⋮----
"""
        Check that all for each (updates, expected_result) pair, ``updates`` yield ``result``.
        """
new_user_client = self.api.impersonate(user)
⋮----
preferences = new_user_client.users.get_preferences(user=user).preferences
⋮----
def test_nested_update(self)
⋮----
tests = [
⋮----
def test_delete(self)
</file>

<file path="apiserver/tests/automated/test_workers.py">
log = config.logger(__file__)
⋮----
class TestWorkersService(TestService)
⋮----
def _check_exists(self, worker: str, exists: bool = True, tags: list = None)
⋮----
workers = self.api.workers.get_all(last_seen=100, tags=tags).workers
found = any(w for w in workers if w.id == worker)
⋮----
def test_workers_register(self)
⋮----
test_worker = f"test_{uuid4().hex}"
⋮----
def test_get_count(self)
⋮----
test_workers = [f"test_{uuid4().hex}" for _ in range(2)]
system_tag = f"tag_{uuid4().hex}"
⋮----
# total workers count include the new ones
count = self.api.workers.get_count().count
⋮----
# filter by system tag and last seen
count = self.api.workers.get_count(system_tags=[system_tag], last_seen=4).count
⋮----
# workers not seen recently
⋮----
# but still visible without the last seen filter
count = self.api.workers.get_count(system_tags=[system_tag]).count
⋮----
def test_workers_timeout(self)
⋮----
def test_system_tags(self)
⋮----
tag = uuid4().hex
system_tag = uuid4().hex
⋮----
# system_tags support
worker = self.api.workers.get_all(tags=[tag], system_tags=[system_tag]).workers[
⋮----
workers = self.api.workers.get_all(
⋮----
def test_filters(self)
⋮----
def _simulate_workers(self, start: int, with_gpu: bool = False) -> dict
⋮----
"""
        Two workers writing the same metrics. One for 4 seconds. Another one for 2
        The first worker reports a task
        :return: worker ids
        """
⋮----
task_id = self._create_running_task(task_name="task-1")
⋮----
workers = [f"test_{uuid4().hex}", f"test_{uuid4().hex}"]
⋮----
gpu_usage = [dict(gpu_usage=[60, 70]), dict(gpu_usage=[40])]
⋮----
gpu_usage = [{}, {}]
⋮----
worker_stats = [
worker_activity = [
timestamp = start * 1000
⋮----
data = dict(
⋮----
def _create_running_task(self, task_name)
⋮----
task_input = dict(name=task_name, type="testing")
⋮----
task_id = self.create_temp("tasks", **task_input)
⋮----
def test_get_keys(self)
⋮----
workers = self._simulate_workers(int(time.time()))
time.sleep(5)  # give to es time to refresh
res = self.api.workers.get_metric_keys(worker_ids=list(workers))
⋮----
def test_get_stats(self)
⋮----
start = int(time.time())
workers = self._simulate_workers(start, with_gpu=True)
⋮----
time.sleep(5)  # give to ES time to refresh
from_date = start
to_date = start + 40*10
# no variants
res = self.api.workers.get_stats(
⋮----
# split_by_variant=True,
⋮----
worker_id = worker.worker
⋮----
metric_name = metric.metric
⋮----
expected = workers[worker_id][metric_name]
⋮----
agg = stat.aggregation
⋮----
val = statistics.mean(expected)
⋮----
val = min(expected)
⋮----
val = max(expected)
⋮----
val = expected
⋮----
# split by resources
⋮----
resource_series = stat.get("resource_series")
⋮----
def assertWorkersInStats(self, workers: Sequence[str], stats: Sequence)
⋮----
def test_get_activity_report(self)
⋮----
# test no workers data
# run on an empty es db since we have no way
# to pass non-existing workers to this api
# res = self.api.workers.get_activity_report(
#     from_timestamp=from_timestamp.timestamp(),
#     to_timestamp=to_timestamp.timestamp(),
#     interval=20,
# )
⋮----
res = self.api.workers.get_activity_report(
⋮----
def assertWorkerSeries(self, series_data: dict, count: int, size: int)
⋮----
# self.assertTrue(any(c == count for c in series_data["counts"]))
# self.assertTrue(all(c <= count for c in series_data["counts"]))
</file>

<file path="apiserver/tests/requirements.txt">
pynose
</file>

<file path="apiserver/tools.py">
""" Command line tools for the API server """
⋮----
def setup()
⋮----
def gen_token(args)
⋮----
resp = AuthBLL.get_token_for_user(
⋮----
def safe_get(obj, glob, default=None, separator="/")
⋮----
top_parser = ArgumentParser(__doc__)
⋮----
subparsers = top_parser.add_subparsers(title="Sections")
⋮----
token = subparsers.add_parser("token")
token_commands = token.add_subparsers(title="Commands")
token_create = token_commands.add_parser(
⋮----
args = top_parser.parse_args()
</file>

<file path="apiserver/updates.py">
log = config.logger(__name__)
⋮----
class CheckUpdatesThread(Thread)
⋮----
_enabled = bool(config.get("apiserver.check_for_updates.enabled", True))
_lock_name = "check_updates"
_redis = redman.connection("apiserver")
⋮----
@attr.s(auto_attribs=True)
    class _VersionResponse
⋮----
version: str
patch_upgrade: bool
description: str = None
⋮----
def __init__(self)
⋮----
@property
    def update_interval(self)
⋮----
def start(self) -> None
⋮----
@property
    def component_name(self) -> str
⋮----
def _check_new_version_available(self) -> Optional[_VersionResponse]
⋮----
url = config.get(
⋮----
uid = Settings.get_by_key("server.uuid")
⋮----
response = requests.get(
⋮----
response = response.json().get(self.component_name)
⋮----
latest_version = response.get("version")
⋮----
cur_version = Version(get_version())
latest_version = Version(latest_version)
⋮----
def _check_updates(self)
⋮----
# noinspection PyBroadException
⋮----
response = self._check_new_version_available()
⋮----
check_updates_thread = CheckUpdatesThread()
</file>

<file path="apiserver/utilities/__init__.py">
def strict_map(*args, **kwargs)
⋮----
"""
    Given a list of dictionaries and names of dictionary keys
    builds a dictionary with the requested keys and values lists
    For the empty list return the dictionary of empty lists
    :param key_names: names of the keys in the resulting dictionary
    :param data: sequence of dictionaries to extract values from
    :param extract_func: the optional callable that extracts properties
    from a dictionary and put them in a tuple in the order corresponding to
    key_names. If not specified then properties are extracted according to key_names
    :param target_keys: optional alternative keys to use in the target dictionary. must be equal in length to key_names.
    """
⋮----
value_sequences = zip(*map(extract_func or itemgetter(*key_names), data))
</file>

<file path="apiserver/utilities/attrs.py">
def typed_attrs(cls)
⋮----
"""
    Created a type-validated attrs class.
    Attributes are set to their default values if they're assigned `None` to.
    """
⋮----
validator = attrib._validator
⋮----
instance_of = attr.validators.instance_of(attrib.type)
⋮----
def converter(default)
⋮----
"""
                Create a converter that interprets `None` as "use default".
                Required in order to create a new lexical scope, see https://stackoverflow.com/a/233835
                """
</file>

<file path="apiserver/utilities/dicts.py">
"""
    iterate through dictionary and return with nested keys flattened into a tuple
    """
next_nesting = None if nesting is None else (nesting - 1)
prefix = prefix or ()
⋮----
path = prefix + (key,)
⋮----
def deep_merge(source: dict, override: dict) -> dict
⋮----
"""
    Merge the override dict into the source in-place
    Contrary to the dpath.merge the sequences are not expanded
    If override contains the sequence with the same name as source
    then the whole sequence in the source is overridden
    """
⋮----
class GetItem(Protocol)
⋮----
def __getitem__(self, key: Any) -> Any
⋮----
node = dictionary
⋮----
node = node.get(key)
⋮----
def nested_delete(dictionary: dict, path: Union[Sequence[str], str]) -> bool
⋮----
"""
    Return 'True' if the element was deleted
    """
⋮----
path = [path]
⋮----
parent = nested_get(dictionary, parent_path)
⋮----
def nested_set(dictionary: dict, path: Union[Sequence[str], str], value: Any)
⋮----
def exclude_fields_from_dict(data: dict, fields: Sequence[str], separator=".")
⋮----
"""
    Performs in place fields exclusion on the passed dict
    """
⋮----
exclude_paths = [e.split(separator) for e in fields]
⋮----
def project_dict(data: dict, projection: Sequence[str], separator=".") -> dict
⋮----
"""
    Project partial data from a dictionary into a new dictionary
    :param data: Input dictionary
    :param projection: List of dictionary paths (each a string with field names separated using a separator)
    :param separator: Separator (default is '.')
    :return: A new dictionary containing only the projected parts from the original dictionary
    """
⋮----
result = {}
⋮----
def copy_path(path_parts, source, destination)
⋮----
src_part = src[path_part]
⋮----
src = src_part
dst = dst.setdefault(path_part, {})
⋮----
last_part = path_parts[-1]
⋮----
# Projection field not in source, no biggie.
</file>

<file path="apiserver/utilities/env.py">
def get_bool(*keys: str, default: bool = None) -> Optional[bool]
⋮----
value = next(env for env in (getenv(key) for key in keys) if env is not None)
</file>

<file path="apiserver/utilities/json.py">
# Dump datetime in ISO format
# treat "native" datetime objects as UTC
DATETIME_MODE = rapidjson.DM_ISO8601 | rapidjson.DM_NAIVE_IS_UTC
⋮----
dumps = rapidjson.Encoder(datetime_mode=DATETIME_MODE)
dumps_notascii = rapidjson.Encoder(datetime_mode=DATETIME_MODE, ensure_ascii=False)
loads = rapidjson.Decoder(datetime_mode=DATETIME_MODE)
</file>

<file path="apiserver/utilities/parameter_key_escaper.py">
class ParameterKeyEscaper
⋮----
"""
    Makes the fields name ready for use with MongoDB and Mongoengine
    . and $ are replaced with their codes
    __ and leading _ are escaped
    Since % is used as an escape character the % is also escaped
    """
⋮----
_mapping = OneToOne({".": "%2E", "$": "%24", "__": "%_%_"})
⋮----
@classmethod
    def escape(cls, value: str)
⋮----
""" Quote a parameter key """
value = value.strip()
⋮----
value = value.replace("%", "%%")
⋮----
value = value.replace(c, r)
⋮----
value = "%_" + value[1:]
⋮----
@classmethod
    def _unescape(cls, value: str)
⋮----
@classmethod
    def unescape(cls, value: str)
⋮----
""" Unquote a quoted parameter key """
value = "%".join(map(cls._unescape, value.split("%%")))
⋮----
value = "_" + value[2:]
⋮----
def mongoengine_safe(field_name)
</file>

<file path="apiserver/utilities/partial_version.py">
class PartialVersion(Version)
⋮----
def __init__(self, version_string: str)
</file>

<file path="apiserver/utilities/stringenum.py">
class StringEnum(str, Enum)
⋮----
def __str__(self)
⋮----
@classmethod
    def values(cls)
⋮----
# noinspection PyMethodParameters
def _generate_next_value_(name, start, count, last_values)
</file>

<file path="apiserver/utilities/threads_manager.py">
class ThreadsManager
⋮----
objects = {}
lock = Lock()
⋮----
def __init__(self, name=None)
⋮----
def register(self, thread_name, daemon=True)
⋮----
def decorator(f)
⋮----
@wraps(f)
            def wrapper(*args, **kwargs)
⋮----
thread = self.objects.get(thread_name)
⋮----
thread = Thread(
⋮----
def __getattr__(self, item)
⋮----
def __getitem__(self, item)
</file>

<file path="apiserver/version.py">
__version__ = "2.3.0"
</file>

<file path="docker/build/Dockerfile">
FROM node:20-bookworm-slim as webapp_builder

ARG CLEARML_WEB_GIT_URL=https://github.com/allegroai/clearml-web.git

USER root
WORKDIR /opt

RUN apt-get update && apt-get install -y git
RUN git clone ${CLEARML_WEB_GIT_URL} clearml-web
RUN mv clearml-web /opt/open-webapp
COPY --chmod=744 docker/build/internal_files/build_webapp.sh /tmp/internal_files/
RUN /bin/bash -c '/tmp/internal_files/build_webapp.sh'

FROM python:3.11-slim-bookworm
COPY --chmod=744 docker/build/internal_files/entrypoint.sh /opt/clearml/
COPY --chmod=744 docker/build/internal_files/update_from_env.py /opt/clearml/utilities/
COPY fileserver /opt/clearml/fileserver/
COPY apiserver /opt/clearml/apiserver/

COPY --chmod=744 docker/build/internal_files/final_image_preparation.sh /tmp/internal_files/
COPY docker/build/internal_files/clearml.conf.template /tmp/internal_files/
COPY docker/build/internal_files/clearml_subpath.conf.template /tmp/internal_files/
RUN /bin/bash -c '/tmp/internal_files/final_image_preparation.sh'

COPY --from=webapp_builder /opt/open-webapp/build /usr/share/nginx/html
COPY --from=webapp_builder /opt/open-webapp/dist/report-widgets /usr/share/nginx/widgets

EXPOSE 8080
EXPOSE 8008
EXPOSE 8081

ARG VERSION
ARG BUILD
ENV CLEARML_SERVER_VERSION=${VERSION}
ENV CLEARML_SERVER_BUILD=${BUILD}

WORKDIR /opt/clearml/
ENTRYPOINT ["/opt/clearml/entrypoint.sh"]
</file>

<file path="docker/build/internal_files/build_webapp.sh">
#!/usr/bin/env bash
set -x
set -e

cd /opt/open-webapp/
npm ci --legacy-peer-deps

cd /opt/open-webapp/
npm run build
npm run build-widgets
</file>

<file path="docker/build/internal_files/clearml_subpath.conf.template">
location /${CLEARML_SERVER_SUB_PATH} {
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $host;
    proxy_pass http://localhost:80;
    rewrite /${CLEARML_SERVER_SUB_PATH}/(.*) /$1  break;
}

location /${CLEARML_SERVER_SUB_PATH}/api {
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $host;
    proxy_pass http://localhost:80/api;
    rewrite /${CLEARML_SERVER_SUB_PATH}/api/(.*) /api/$1  break;
}

location /${CLEARML_SERVER_SUB_PATH}/files {
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $host;
    proxy_pass http://localhost:80/files;
    rewrite /${CLEARML_SERVER_SUB_PATH}/files/(.*) /files/$1  break;
    rewrite /${CLEARML_SERVER_SUB_PATH}/files /files/  break;
}
</file>

<file path="docker/build/internal_files/clearml.conf.template">
server {
    listen       80 default_server;
    ${COMMENT_IPV6_LISTEN}listen       [::]:80 default_server;
    server_name  _;
    root         /usr/share/nginx/html;
    proxy_http_version 1.1;
    client_max_body_size 0;

    # compression
    gzip            on;
    gzip_comp_level 9;
    gzip_http_version 1.0;
    gzip_min_length 512;
    gzip_proxied    expired no-cache no-store private auth;
    gzip_types  text/plain
                text/css
                application/json
                application/javascript
                application/x-javascript
                text/xml application/xml
                application/xml+rss
                text/javascript
                application/x-font-ttf
                font/woff2
                image/svg+xml
                image/x-icon;

    # Load configuration files for the default server block.
    include /etc/nginx/default.d/*.conf;

    location / {
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
        add_header Content-Security-Policy "frame-ancestors 'self'";
        add_header X-XSS-Protection "1; mode=block";
        add_header X-Content-Type-Options "nosniff" always;
        add_header Referrer-Policy "no-referrer-when-downgrade";
        try_files $uri $uri/ /index.html;
    }

    location /version.json {
        add_header Cache-Control 'no-cache';
    }

    location /api {
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $host;
        proxy_pass ${NGINX_APISERVER_ADDR};
        rewrite /api/(.*) /$1  break;
        proxy_set_header Connection "";
    }

    location /files {
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $host;
        proxy_pass ${NGINX_FILESERVER_ADDR};
        rewrite /files/(.*) /$1  break;
    }

    location /widgets {
        alias /usr/share/nginx/widgets;
        try_files $uri $uri/ /widgets/index.html;
        add_header Content-Security-Policy "frame-ancestors *";
    }

    error_page 404 /404.html;
        location = /40x.html {
    }

    error_page 500 502 503 504 /50x.html;
        location = /50x.html {
    }
}
</file>

<file path="docker/build/internal_files/entrypoint.sh">
#!/usr/bin/env bash
set -e

mkdir -p /var/log/clearml

SERVER_TYPE=$1

if (( $# < 1 )) ; then
    echo "The server type was not stated. It should be either apiserver, webserver or fileserver."
    sleep 60
    exit 1

elif [[ ${SERVER_TYPE} == "apiserver" ]]; then
    cd /opt/clearml/
    python3 -m apiserver.apierrors_generator

    if [[ -n $CLEARML_USE_GUNICORN ]]; then
      MAX_REQUESTS=
      if [[ -n $CLEARML_GUNICORN_MAX_REQUESTS ]]; then
        MAX_REQUESTS="--max-requests $CLEARML_GUNICORN_MAX_REQUESTS"
        if [[ -n $CLEARML_GUNICORN_MAX_REQUESTS_JITTER ]]; then
          MAX_REQUESTS="$MAX_REQUESTS --max-requests-jitter $CLEARML_GUNICORN_MAX_REQUESTS_JITTER"
        fi
      fi

      export GUNICORN_CMD_ARGS=${CLEARML_GUNICORN_CMD_ARGS}

      # Note: don't be tempted to "fix" $MAX_REQUESTS with "$MAX_REQUESTS" as this produces an empty arg which fucks up gunicorn
      gunicorn \
        -w "${CLEARML_GUNICORN_WORKERS:-8}" \
        -t "${CLEARML_GUNICORN_TIMEOUT:-600}" --bind="${CLEARML_GUNICORN_BIND:-0.0.0.0:8008}" \
        $MAX_REQUESTS apiserver.server:app
    else
        python3 -m apiserver.server
    fi

elif [[ ${SERVER_TYPE} == "webserver" ]]; then

    if [[ "${USER_KEY}" != "" ]] || [[ "${USER_SECRET}" != "" ]] || [[ "${COMPANY_ID}" != "" ]]; then
      cat << EOF > /usr/share/nginx/html/credentials.json
{
  "userKey": "${USER_KEY}",
  "userSecret": "${USER_SECRET}",
  "companyID": "${COMPANY_ID}"
}
EOF
    fi

    # Create an empty configuration json
    echo "{}" > /tmp/configuration.json
	
    # Copy the external configuration file if it exists
    if test -f "/mnt/external_files/configs/configuration.json"; then
      echo "Copying external configuration"
      cp /mnt/external_files/configs/configuration.json /tmp/configuration.json
    fi

	  # Update from env variables
    echo "Updating configuration from env"
    /opt/clearml/utilities/update_from_env.py \
        --verbose \
        /tmp/configuration.json \
        /usr/share/nginx/html/configuration.json

    export NGINX_APISERVER_ADDR=${NGINX_APISERVER_ADDRESS:-http://apiserver:8008}
    export NGINX_FILESERVER_ADDR=${NGINX_FILESERVER_ADDRESS:-http://fileserver:8081}
    export COMMENT_IPV6_LISTEN=$([ "$DISABLE_NGINX_IPV6" = "true" ] && echo "#" || echo "")
    envsubst '${COMMENT_IPV6_LISTEN} ${NGINX_APISERVER_ADDR} ${NGINX_FILESERVER_ADDR}' < /etc/nginx/clearml.conf.template > /etc/nginx/sites-enabled/default

    if [[ -n "${CLEARML_SERVER_SUB_PATH}" ]]; then
      mkdir -p /etc/nginx/default.d/
      envsubst '${CLEARML_SERVER_SUB_PATH}' < /etc/nginx/clearml_subpath.conf.template > /etc/nginx/default.d/clearml_subpath.conf
      cp /usr/share/nginx/html/env.js /usr/share/nginx/html/env.js.origin
      envsubst '${CLEARML_SERVER_SUB_PATH}' < /usr/share/nginx/html/env.js.origin > /usr/share/nginx/html/env.js
      cp /usr/share/nginx/html/index.html /usr/share/nginx/html/index.html.origin
      sed 's/href="\/"/href="\/'${CLEARML_SERVER_SUB_PATH}'\/"/' /usr/share/nginx/html/index.html.origin > /usr/share/nginx/html/index.html
    fi

    #start the server
    /usr/sbin/nginx -g "daemon off;"

elif [[ ${SERVER_TYPE} == "fileserver" ]]; then
    cd /opt/clearml/fileserver/
    if [ "$FILESERVER_USE_GUNICORN" = true ] ; then
      gunicorn -t 600 --bind=0.0.0.0:8081 fileserver:app
    else
      python3 fileserver.py
    fi

else
    echo "Server type ${SERVER_TYPE} is invalid. Please choose either apiserver, webserver or fileserver."
fi
</file>

<file path="docker/build/internal_files/final_image_preparation.sh">
#!/usr/bin/env bash
set -x
set -o errexit
set -o nounset
set -o pipefail

apt-get update -y
apt-get install -y python3-setuptools python3-dev build-essential nginx gettext vim curl

python3 -m ensurepip
python3 -m pip install --upgrade pip
python3 -m pip install -r /opt/clearml/apiserver/requirements.txt
mkdir -p /opt/clearml/log
mkdir -p /opt/clearml/config
ln -svf /dev/stdout /var/log/nginx/access.log
ln -svf /dev/stderr /var/log/nginx/error.log
mv /tmp/internal_files/clearml.conf.template /etc/nginx/clearml.conf.template
mv /tmp/internal_files/clearml_subpath.conf.template /etc/nginx/clearml_subpath.conf.template

rm -d -r "$(pip cache dir)"
apt-get clean
</file>

<file path="docker/build/internal_files/update_from_env.py">
#!/usr/bin/env python3
""" Update json configuration file from environment variables """
⋮----
class PathConflictError(Exception)
⋮----
def __init__(self, path_: List[str])
⋮----
def set_path(p: List[str], obj: dict, v: Any)
⋮----
parser = ArgumentParser(description=__doc__)
⋮----
args = parser.parse_args()
⋮----
data = None
⋮----
data = json.load(args.input_file)
⋮----
def parse_value(k, v)
⋮----
prefix = args.prefix + args.sep
⋮----
env_vars = {
⋮----
match = next((k for k in env_vars if k.lower() == path), None)
⋮----
replace = env_vars.pop(match)
⋮----
path = k.split(args.sep)
</file>

<file path="docker/docker-compose-win10.yml">
version: "3.6"
services:

  apiserver:
    command:
    - apiserver
    container_name: clearml-apiserver
    image: clearml/server:latest
    restart: unless-stopped
    volumes:
    - c:/opt/clearml/logs:/var/log/clearml
    - c:/opt/clearml/config:/opt/clearml/config
    - c:/opt/clearml/data/fileserver:/mnt/fileserver
    depends_on:
      - redis
      - mongo
      - elasticsearch
      - fileserver
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      CLEARML_SERVER_DEPLOYMENT_TYPE: win10
      CLEARML__apiserver__pre_populate__enabled: "true"
      CLEARML__apiserver__pre_populate__zip_files: "/opt/clearml/db-pre-populate"
      CLEARML__apiserver__pre_populate__artifacts_path: "/mnt/fileserver"
      CLEARML__services__async_urls_delete__enabled: "true"
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    ports:
    - "8008:8008"
    networks:
      - backend
      - frontend

  elasticsearch:
    networks:
      - backend
    container_name: clearml-elastic
    environment:
      bootstrap.memory_lock: "true"
      cluster.name: clearml
      cluster.routing.allocation.node_initial_primaries_recoveries: "500"
      cluster.routing.allocation.disk.watermark.low: 500mb
      cluster.routing.allocation.disk.watermark.high: 500mb
      cluster.routing.allocation.disk.watermark.flood_stage: 500mb
      discovery.type: "single-node"
      http.compression_level: "7"
      node.name: clearml
      reindex.remote.whitelist: "'*.*'"
      xpack.security.enabled: "false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    image: elasticsearch:8.17.0
    restart: unless-stopped
    volumes:
      - c:/opt/clearml/data/elastic_7:/usr/share/elasticsearch/data
      - /usr/share/elasticsearch/logs

  fileserver:
    networks:
      - backend
      - frontend
    command:
    - fileserver
    container_name: clearml-fileserver
    image: clearml/server:latest
    environment:
      CLEARML__fileserver__delete__allow_batch: "true"
    restart: unless-stopped
    volumes:
    - c:/opt/clearml/logs:/var/log/clearml
    - c:/opt/clearml/data/fileserver:/mnt/fileserver
    - c:/opt/clearml/config:/opt/clearml/config

    ports:
    - "8081:8081"

  mongo:
    networks:
      - backend
    container_name: clearml-mongo
    image: mongo:7.0.22
    restart: unless-stopped
    command: --setParameter internalQueryMaxBlockingSortMemoryUsageBytes=196100200
    volumes:
    - c:/opt/clearml/data/mongo_4/db:/data/db
    - c:/opt/clearml/data/mongo_4/configdb:/data/configdb

  redis:
    networks:
      - backend
    container_name: clearml-redis
    image: redis:7.4.1
    restart: unless-stopped
    volumes:
    - c:/opt/clearml/data/redis:/data

  webserver:
    command:
    - webserver
    container_name: clearml-webserver
    image: clearml/server:latest
    restart: unless-stopped
    volumes:
    - c:/clearml/logs:/var/log/clearml
    depends_on:
      - apiserver
    ports:
    - "8080:80"
    networks:
      - backend
      - frontend

  async_delete:
    depends_on:
      - apiserver
      - redis
      - mongo
      - elasticsearch
      - fileserver
    container_name: async_delete
    image: clearml/server:latest
    networks:
      - backend
    restart: unless-stopped
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      PYTHONPATH: /opt/clearml/apiserver
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    entrypoint:
      - python3
      - -m
      - jobs.async_urls_delete
      - --fileserver-host
      - http://fileserver:8081
    volumes:
    - c:/opt/clearml/logs:/var/log/clearml
    - c:/opt/clearml/config:/opt/clearml/config

networks:
  backend:
    driver: bridge
  frontend:
    name: frontend
    driver: bridge
</file>

<file path="docker/docker-compose.yml">
version: "3.6"
services:

  apiserver:
    command:
    - apiserver
    container_name: clearml-apiserver
    image: clearml/server:latest
    restart: unless-stopped
    volumes:
    - /opt/clearml/logs:/var/log/clearml
    - /opt/clearml/config:/opt/clearml/config
    - /opt/clearml/data/fileserver:/mnt/fileserver
    depends_on:
      - redis
      - mongo
      - elasticsearch
      - fileserver
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      CLEARML_SERVER_DEPLOYMENT_TYPE: linux
      CLEARML__apiserver__pre_populate__enabled: "true"
      CLEARML__apiserver__pre_populate__zip_files: "/opt/clearml/db-pre-populate"
      CLEARML__apiserver__pre_populate__artifacts_path: "/mnt/fileserver"
      CLEARML__services__async_urls_delete__enabled: "true"
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
      CLEARML__secure__credentials__services_agent__user_key: ${CLEARML_AGENT_ACCESS_KEY:-}
      CLEARML__secure__credentials__services_agent__user_secret: ${CLEARML_AGENT_SECRET_KEY:-}
    ports:
    - "8008:8008"
    networks:
      - backend
      - frontend

  elasticsearch:
    networks:
      - backend
    container_name: clearml-elastic
    environment:
      bootstrap.memory_lock: "true"
      cluster.name: clearml
      cluster.routing.allocation.node_initial_primaries_recoveries: "500"
      cluster.routing.allocation.disk.watermark.low: 500mb
      cluster.routing.allocation.disk.watermark.high: 500mb
      cluster.routing.allocation.disk.watermark.flood_stage: 500mb
      discovery.type: "single-node"
      http.compression_level: "7"
      node.name: clearml
      reindex.remote.whitelist: "'*.*'"
      xpack.security.enabled: "false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    image: elasticsearch:8.17.0
    restart: unless-stopped
    volumes:
      - /opt/clearml/data/elastic_7:/usr/share/elasticsearch/data
      - /usr/share/elasticsearch/logs

  fileserver:
    networks:
      - backend
      - frontend
    command:
    - fileserver
    container_name: clearml-fileserver
    image: clearml/server:latest
    environment:
      CLEARML__fileserver__delete__allow_batch: "true"
    restart: unless-stopped
    volumes:
    - /opt/clearml/logs:/var/log/clearml
    - /opt/clearml/data/fileserver:/mnt/fileserver
    - /opt/clearml/config:/opt/clearml/config
    ports:
    - "8081:8081"

  mongo:
    networks:
      - backend
    container_name: clearml-mongo
    image: mongo:7.0.22
    restart: unless-stopped
    command: --setParameter internalQueryMaxBlockingSortMemoryUsageBytes=196100200
    volumes:
    - /opt/clearml/data/mongo_4/db:/data/db
    - /opt/clearml/data/mongo_4/configdb:/data/configdb

  redis:
    networks:
      - backend
    container_name: clearml-redis
    image: redis:7.4.1
    restart: unless-stopped
    volumes:
    - /opt/clearml/data/redis:/data

  webserver:
    command:
    - webserver
    container_name: clearml-webserver
    # environment:
    #  CLEARML_SERVER_SUB_PATH : clearml-web # Allow Clearml to be served with a URL path prefix.
    image: clearml/server:latest
    restart: unless-stopped
    depends_on:
      - apiserver
    ports:
    - "8080:80"
    networks:
      - backend
      - frontend

  async_delete:
    depends_on:
      - apiserver
      - redis
      - mongo
      - elasticsearch
      - fileserver
    container_name: async_delete
    image: clearml/server:latest
    networks:
      - backend
    restart: unless-stopped
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      PYTHONPATH: /opt/clearml/apiserver
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    entrypoint:
      - python3
      - -m
      - jobs.async_urls_delete
      - --fileserver-host
      - http://fileserver:8081
    volumes:
      - /opt/clearml/logs:/var/log/clearml
      - /opt/clearml/config:/opt/clearml/config

  agent-services:
    networks:
      - backend
    container_name: clearml-agent-services
    image: clearml/clearml-agent-services:latest
    deploy:
      restart_policy:
        condition: on-failure
    privileged: true
    environment:
      CLEARML_HOST_IP: ${CLEARML_HOST_IP}
      CLEARML_WEB_HOST: ${CLEARML_WEB_HOST:-}
      CLEARML_API_HOST: http://apiserver:8008
      CLEARML_FILES_HOST: ${CLEARML_FILES_HOST:-}
      CLEARML_API_ACCESS_KEY: ${CLEARML_AGENT_ACCESS_KEY:-$CLEARML_API_ACCESS_KEY}
      CLEARML_API_SECRET_KEY: ${CLEARML_AGENT_SECRET_KEY:-$CLEARML_API_SECRET_KEY}
      CLEARML_AGENT_GIT_USER: ${CLEARML_AGENT_GIT_USER}
      CLEARML_AGENT_GIT_PASS: ${CLEARML_AGENT_GIT_PASS}
      CLEARML_AGENT_UPDATE_VERSION: ${CLEARML_AGENT_UPDATE_VERSION:->=0.17.0}
      CLEARML_AGENT_DEFAULT_BASE_DOCKER: "ubuntu:18.04"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-}
      AZURE_STORAGE_ACCOUNT: ${AZURE_STORAGE_ACCOUNT:-}
      AZURE_STORAGE_KEY: ${AZURE_STORAGE_KEY:-}
      GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS:-}
      CLEARML_WORKER_ID: "clearml-services"
      CLEARML_AGENT_DOCKER_HOST_MOUNT: "/opt/clearml/agent:/root/.clearml"
      SHUTDOWN_IF_NO_ACCESS_KEY: 1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/clearml/agent:/root/.clearml
    depends_on:
      - apiserver
    entrypoint: >
      bash -c "curl --retry 10 --retry-delay 10 --retry-connrefused 'http://apiserver:8008/debug.ping' && /usr/agent/entrypoint.sh"

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge
</file>

<file path="docs/apiserver.conf">
auth {
    # Fixed users login credentials
    # No other user will be able to login
    # Note: password may be bcrypt-hashed (generate using `python3 -c 'import bcrypt,base64; print(base64.b64encode(bcrypt.hashpw("password".encode(), bcrypt.gensalt())))'`)
    fixed_users {
        enabled: true
        pass_hashed: false
        users: [
            {
                username: "jane"
                password: "12345678"
                name: "Jane Doe"
            },
            {
                username: "john"
                password: "12345678"
                name: "John Doe"
            },
        ]
    }
}
</file>

<file path="docs/faq.md">
# trains-server FAQ


## **NOTE**: This page's information is deprecated. See the [ClearML documentation](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server) for up-to-date deployment instructions
</file>

<file path="docs/install_aws.md">
# Deploying ClearML Server on AWS

## See the [ClearML documentation](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_aws_ec2_ami/) for up-to-date deployment instructions
</file>

<file path="docs/install_gcp.md">
# Deploying ClearML Server on Google Cloud Platform

# See the [ClearML documentation](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_gcp) for up-to-date deployment instructions
</file>

<file path="docs/install_linux_mac.md">
# Launching ClearML Server Docker in Linux or macOS

## See the [ClearML documentation](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_linux_mac) for up-to-date deployment instructions
</file>

<file path="docs/install_win.md">
# Launching the ClearML Server Docker in Windows 10

## See the [ClearML documentation](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_win) for up-to-date deployment instructions
</file>

<file path="docs/services.conf">
tasks {
    non_responsive_tasks_watchdog {
        # In-progress tasks that haven't been updated for at least 'value' seconds will be stopped by the watchdog
        threshold_sec: 7200

        # Watchdog will sleep for this number of seconds after each cycle
        watch_interval_sec: 900
    }
}
</file>

<file path="fileserver/auth.py">
log = config.logger(__file__)
⋮----
@attr.s(auto_attribs=True)
class TokenInfo
⋮----
company: str
user: str
⋮----
class FileserverSession(Session)
⋮----
@property
    def client(self)
⋮----
@client.setter
    def client(self, _)
⋮----
# do not allow the base class to override the client
⋮----
class AuthHandler
⋮----
enabled = config.get("fileserver.auth.enabled", False)
_instance = None
⋮----
@classmethod
    def instance(cls)
⋮----
def __init__(self)
⋮----
def _validate_and_get_token_info(self, token: str) -> TokenInfo
⋮----
token_hash = sha256(token.encode()).hexdigest() if len(token) > 256 else token
key = f"token_{token_hash}"
token_data = self.redis.get(key)
⋮----
res = self.session.send_request(
⋮----
data = res.json()["data"]
⋮----
info = TokenInfo(
⋮----
timeout_sec = config.get(
⋮----
def validate(self, request: Request)
⋮----
token = self.get_token(request)
⋮----
@staticmethod
    def get_token(request: Request) -> Optional[str]
⋮----
auth_header = request.headers.get("Authorization")
⋮----
token = auth_header.partition(" ")[2]
⋮----
last_ex = None
⋮----
cookie = request.cookies.get(cookie_name)
⋮----
last_ex = ex
</file>

<file path="fileserver/config/__init__.py">
config = BasicConfig(Path(__file__).with_name("default"))
</file>

<file path="fileserver/config/basic.py">
DEFAULT_EXTRA_CONFIG_PATH = "/opt/clearml/config"
PREFIXES = ("CLEARML", "TRAINS")
EXTRA_CONFIG_PATH_SEP = ":"
EXTRA_CONFIG_VALUES_ENV_KEY_SEP = "__"
⋮----
class BasicConfig
⋮----
NotSet = object()
⋮----
def __init__(self, folder)
⋮----
def __getitem__(self, key)
⋮----
def get(self, key, default=NotSet)
⋮----
value = self._config.get(key, default)
⋮----
def logger(self, name)
⋮----
name = Path(name).stem
path = ".".join((self.prefix, Path(name).stem))
⋮----
def _read_extra_env_config_values(self)
⋮----
""" Loads extra configuration from environment-injected values """
result = ConfigTree()
⋮----
keys = sorted(k for k in os.environ if k.startswith(prefix))
⋮----
path = key[len(prefix) :].replace(EXTRA_CONFIG_VALUES_ENV_KEY_SEP, ".").lower()
result = ConfigTree.merge_configs(
⋮----
def _read_env_paths(self)
⋮----
value = first(map(getenv, self.extra_config_path_env_key), DEFAULT_EXTRA_CONFIG_PATH)
⋮----
paths = [
invalid = [
⋮----
def _load(self, verbose=True)
⋮----
extra_config_paths = self._read_env_paths() or []
extra_config_values = self._read_extra_env_config_values()
configs = [
⋮----
def _read_recursive(self, conf_root, verbose=True)
⋮----
conf = ConfigTree()
⋮----
key = ".".join(file.relative_to(conf_root).with_suffix("").parts)
⋮----
@staticmethod
    def _read_single_file(file_path, verbose=True)
⋮----
msg = f"Failed parsing {file_path} ({ex.__class__.__name__}): (at char {ex.loc}, line:{ex.lineno}, col:{ex.column})"
⋮----
msg = f"Failed parsing {file_path} ({ex.__class__.__name__}): {ex}"
⋮----
class ConfigurationError(Exception)
⋮----
def __init__(self, msg, file_path=None, *args)
</file>

<file path="fileserver/config/default/fileserver.conf">
download {
    # Add response headers requesting no caching for served files
    disable_browser_caching: false

    # Cache timeout to be set for downloaded files
    cache_timeout_sec: 300
}

delete {
    allow_batch: true
}

upload {
    # the max size in Mb of the upload contents in one upload call
    max_upload_size_mb: 0
}

cors {
    origins: "*"
}

auth {
    # enable/disable auth validation on upload/download
    enabled: true

    # names of cookies in which authorization token can be found
    cookie_names: ["clearml_token_basic"]

    tokens_cache_threshold_sec: 43200
}
</file>

<file path="fileserver/config/default/hosts.conf">
api_server: "http://apiserver:8008"

redis {
    fileserver {
        host: "redis"
        port: 6379
        db: 8
    }
}
</file>

<file path="fileserver/config/default/logging.conf">
{
    version: 1
    disable_existing_loggers: false
    formatters: {
        default: {
            format: "[%(asctime)s] [%(process)d] [%(levelname)s] [%(name)s] %(message)s"
        }
    }
    handlers {
        console {
            formatter: default
            class: "logging.StreamHandler"
        }
        text_file: {
            formatter: default,
            backupCount: 3
            maxBytes: 10240000,
            class: "logging.handlers.RotatingFileHandler",
            filename: "/var/log/clearml/fileserver.log"
        }
    }
    root {
        handlers: [console, text_file]
        level: INFO
    }
    loggers {
        urllib3 {
            handlers: [console, text_file]
            level: WARN
            propagate: false
        }
        werkzeug {
            handlers: [console, text_file]
            level: WARN
            propagate: false
        }
    }
}
</file>

<file path="fileserver/config/default/secure.conf">
credentials {
    # system credentials as they appear in the auth DB, used for intra-service communications
    fileserver {
        user_key: "GSQWPEKSKNKF354LC9V6BHXKTYFD5I"
        user_secret: "tuBXcGQBECsEhcNiK2kiWi750z9r8Z85XrQ9V0c24huTuCb2xf2X1nKG"
    }
}
</file>

<file path="fileserver/fileserver.py">
""" A Simple file server for uploading and downloading files """
⋮----
log = config.logger(__file__)
DEFAULT_UPLOAD_FOLDER = "/mnt/fileserver"
⋮----
app = Flask(__name__)
⋮----
auth_handler = AuthHandler.instance()
⋮----
@app.route("/", methods=["GET"])
def ping()
⋮----
@app.before_request
def before_request()
⋮----
@app.after_request
def after_request(response)
⋮----
@app.route("/", methods=["POST"])
def upload()
⋮----
results = []
⋮----
file_path = filename.lstrip(os.sep)
safe_path = safe_join(app.config["UPLOAD_FOLDER"], file_path)
⋮----
target = Path(safe_path)
⋮----
@app.route("/<path:path>", methods=["GET"])
def download(path)
⋮----
as_attachment = "download" in request.args
⋮----
mimetype = "application/octet-stream" if encoding == "gzip" else None
⋮----
response = send_from_directory(
⋮----
headers = response.headers
⋮----
def _get_full_path(path: str) -> Optional[Path]
⋮----
path_str = safe_join(os.fspath(app.config["UPLOAD_FOLDER"]), os.fspath(path))
⋮----
@app.route("/<path:path>", methods=["DELETE"])
def delete(path)
⋮----
full_path = _get_full_path(path)
⋮----
def batch_delete()
⋮----
body = request.get_json(force=True, silent=False)
⋮----
files = body.get("files")
⋮----
deleted = {}
errors = defaultdict(list)
log_errors = defaultdict(list)
⋮----
def record_error(msg: str, file_, path_)
⋮----
path = urllib.parse.unquote_plus(file)
⋮----
# empty path may result in deleting all company data. Too dangerous
⋮----
def main()
⋮----
parser = ArgumentParser(description=__doc__)
⋮----
args = parser.parse_args()
</file>

<file path="fileserver/LICENSE">
Server Side Public License
                     VERSION 1, OCTOBER 16, 2018

                    Copyright © 2025 ClearML Inc.

  Everyone is permitted to copy and distribute verbatim copies of this
  license document, but changing it is not allowed.

                       TERMS AND CONDITIONS

  0. Definitions.
  
  “This License” refers to Server Side Public License.

  “Copyright” also means copyright-like laws that apply to other kinds of
  works, such as semiconductor masks.

  “The Program” refers to any copyrightable work licensed under this
  License.  Each licensee is addressed as “you”. “Licensees” and
  “recipients” may be individuals or organizations.

  To “modify” a work means to copy from or adapt all or part of the work in
  a fashion requiring copyright permission, other than the making of an
  exact copy. The resulting work is called a “modified version” of the
  earlier work or a work “based on” the earlier work.

  A “covered work” means either the unmodified Program or a work based on
  the Program.

  To “propagate” a work means to do anything with it that, without
  permission, would make you directly or secondarily liable for
  infringement under applicable copyright law, except executing it on a
  computer or modifying a private copy. Propagation includes copying,
  distribution (with or without modification), making available to the
  public, and in some countries other activities as well.

  To “convey” a work means any kind of propagation that enables other
  parties to make or receive copies. Mere interaction with a user through a
  computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays “Appropriate Legal Notices” to the
  extent that it includes a convenient and prominently visible feature that
  (1) displays an appropriate copyright notice, and (2) tells the user that
  there is no warranty for the work (except to the extent that warranties
  are provided), that licensees may convey the work under this License, and
  how to view a copy of this License. If the interface presents a list of
  user commands or options, such as a menu, a prominent item in the list
  meets this criterion.

  1. Source Code.

  The “source code” for a work means the preferred form of the work for
  making modifications to it. “Object code” means any non-source form of a
  work.

  A “Standard Interface” means an interface that either is an official
  standard defined by a recognized standards body, or, in the case of
  interfaces specified for a particular programming language, one that is
  widely used among developers working in that language.  The “System
  Libraries” of an executable work include anything, other than the work as
  a whole, that (a) is included in the normal form of packaging a Major
  Component, but which is not part of that Major Component, and (b) serves
  only to enable use of the work with that Major Component, or to implement
  a Standard Interface for which an implementation is available to the
  public in source code form. A “Major Component”, in this context, means a
  major essential component (kernel, window system, and so on) of the
  specific operating system (if any) on which the executable work runs, or
  a compiler used to produce the work, or an object code interpreter used
  to run it.

  The “Corresponding Source” for a work in object code form means all the
  source code needed to generate, install, and (for an executable work) run
  the object code and to modify the work, including scripts to control
  those activities. However, it does not include the work's System
  Libraries, or general-purpose tools or generally available free programs
  which are used unmodified in performing those activities but which are
  not part of the work. For example, Corresponding Source includes
  interface definition files associated with source files for the work, and
  the source code for shared libraries and dynamically linked subprograms
  that the work is specifically designed to require, such as by intimate
  data communication or control flow between those subprograms and other
  parts of the work.

  The Corresponding Source need not include anything that users can
  regenerate automatically from other parts of the Corresponding Source.

  The Corresponding Source for a work in source code form is that same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
  copyright on the Program, and are irrevocable provided the stated
  conditions are met. This License explicitly affirms your unlimited
  permission to run the unmodified Program, subject to section 13. The
  output from running a covered work is covered by this License only if the
  output, given its content, constitutes a covered work. This License
  acknowledges your rights of fair use or other equivalent, as provided by
  copyright law.  Subject to section 13, you may make, run and propagate
  covered works that you do not convey, without conditions so long as your
  license otherwise remains in force. You may convey covered works to
  others for the sole purpose of having them make modifications exclusively
  for you, or provide you with facilities for running those works, provided
  that you comply with the terms of this License in conveying all
  material for which you do not control copyright. Those thus making or
  running the covered works for you must do so exclusively on your
  behalf, under your direction and control, on terms that prohibit them
  from making any copies of your copyrighted material outside their
  relationship with you.

  Conveying under any other circumstances is permitted solely under the
  conditions stated below. Sublicensing is not allowed; section 10 makes it
  unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
  measure under any applicable law fulfilling obligations under article 11
  of the WIPO copyright treaty adopted on 20 December 1996, or similar laws
  prohibiting or restricting circumvention of such measures.

  When you convey a covered work, you waive any legal power to forbid
  circumvention of technological measures to the extent such circumvention is
  effected by exercising rights under this License with respect to the
  covered work, and you disclaim any intention to limit operation or
  modification of the work as a means of enforcing, against the work's users,
  your or third parties' legal rights to forbid circumvention of
  technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
  receive it, in any medium, provided that you conspicuously and
  appropriately publish on each copy an appropriate copyright notice; keep
  intact all notices stating that this License and any non-permissive terms
  added in accord with section 7 apply to the code; keep intact all notices
  of the absence of any warranty; and give all recipients a copy of this
  License along with the Program.  You may charge any price or no price for
  each copy that you convey, and you may offer support or warranty
  protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
  produce it from the Program, in the form of source code under the terms
  of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified it,
    and giving a relevant date.

    b) The work must carry prominent notices stating that it is released
    under this License and any conditions added under section 7. This
    requirement modifies the requirement in section 4 to “keep intact all
    notices”.

    c) You must license the entire work, as a whole, under this License to
    anyone who comes into possession of a copy. This License will therefore
    apply, along with any applicable section 7 additional terms, to the
    whole of the work, and all its parts, regardless of how they are
    packaged. This License gives no permission to license the work in any
    other way, but it does not invalidate such permission if you have
    separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your work
    need not make them do so.

  A compilation of a covered work with other separate and independent
  works, which are not by their nature extensions of the covered work, and
  which are not combined with it such as to form a larger program, in or on
  a volume of a storage or distribution medium, is called an “aggregate” if
  the compilation and its resulting copyright are not used to limit the
  access or legal rights of the compilation's users beyond what the
  individual works permit. Inclusion of a covered work in an aggregate does
  not cause this License to apply to the other parts of the aggregate.
  
  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms of
  sections 4 and 5, provided that you also convey the machine-readable
  Corresponding Source under the terms of this License, in one of these
  ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium customarily
    used for software interchange.
   
    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a written
    offer, valid for at least three years and valid for as long as you
    offer spare parts or customer support for that product model, to give
    anyone who possesses the object code either (1) a copy of the
    Corresponding Source for all the software in the product that is
    covered by this License, on a durable physical medium customarily used
    for software interchange, for a price no more than your reasonable cost
    of physically performing this conveying of source, or (2) access to
    copy the Corresponding Source from a network server at no charge.
   
    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source. This alternative is
    allowed only occasionally and noncommercially, and only if you received
    the object code with such an offer, in accord with subsection 6b.
   
    d) Convey the object code by offering access from a designated place
    (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge. You need not require recipients to copy the
    Corresponding Source along with the object code. If the place to copy
    the object code is a network server, the Corresponding Source may be on
    a different server (operated by you or a third party) that supports
    equivalent copying facilities, provided you maintain clear directions
    next to the object code saying where to find the Corresponding Source.
    Regardless of what server hosts the Corresponding Source, you remain
    obligated to ensure that it is available for as long as needed to
    satisfy these requirements.
   
    e) Convey the object code using peer-to-peer transmission, provided you
    inform other peers where the object code and Corresponding Source of
    the work are being offered to the general public at no charge under
    subsection 6d.

  A separable portion of the object code, whose source code is excluded
  from the Corresponding Source as a System Library, need not be included
  in conveying the object code work.

  A “User Product” is either (1) a “consumer product”, which means any
  tangible personal property which is normally used for personal, family,
  or household purposes, or (2) anything designed or sold for incorporation
  into a dwelling. In determining whether a product is a consumer product,
  doubtful cases shall be resolved in favor of coverage. For a particular
  product received by a particular user, “normally used” refers to a
  typical or common use of that class of product, regardless of the status
  of the particular user or of the way in which the particular user
  actually uses, or expects or is expected to use, the product. A product
  is a consumer product regardless of whether the product has substantial
  commercial, industrial or non-consumer uses, unless such uses represent
  the only significant mode of use of the product.

  “Installation Information” for a User Product means any methods,
  procedures, authorization keys, or other information required to install
  and execute modified versions of a covered work in that User Product from
  a modified version of its Corresponding Source. The information must
  suffice to ensure that the continued functioning of the modified object
  code is in no case prevented or interfered with solely because
  modification has been made.

  If you convey an object code work under this section in, or with, or
  specifically for use in, a User Product, and the conveying occurs as part
  of a transaction in which the right of possession and use of the User
  Product is transferred to the recipient in perpetuity or for a fixed term
  (regardless of how the transaction is characterized), the Corresponding
  Source conveyed under this section must be accompanied by the
  Installation Information. But this requirement does not apply if neither
  you nor any third party retains the ability to install modified object
  code on the User Product (for example, the work has been installed in
  ROM).

  The requirement to provide Installation Information does not include a
  requirement to continue to provide support service, warranty, or updates
  for a work that has been modified or installed by the recipient, or for
  the User Product in which it has been modified or installed. Access
  to a network may be denied when the modification itself materially
  and adversely affects the operation of the network or violates the
  rules and protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided, in
  accord with this section must be in a format that is publicly documented
  (and with an implementation available to the public in source code form),
  and must require no special password or key for unpacking, reading or
  copying.

  7. Additional Terms.

  “Additional permissions” are terms that supplement the terms of this
  License by making exceptions from one or more of its conditions.
  Additional permissions that are applicable to the entire Program shall be
  treated as though they were included in this License, to the extent that
  they are valid under applicable law. If additional permissions apply only
  to part of the Program, that part may be used separately under those
  permissions, but the entire Program remains governed by this License
  without regard to the additional permissions.  When you convey a copy of
  a covered work, you may at your option remove any additional permissions
  from that copy, or from any part of it. (Additional permissions may be
  written to require their own removal in certain cases when you modify the
  work.) You may place additional permissions on material, added by you to
  a covered work, for which you have or can give appropriate copyright
  permission.

  Notwithstanding any other provision of this License, for material you add
  to a covered work, you may (if authorized by the copyright holders of
  that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some trade
    names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that material
    by anyone who conveys the material (or modified versions of it) with
    contractual assumptions of liability to the recipient, for any
    liability that these contractual assumptions directly impose on those
    licensors and authors.

  All other non-permissive additional terms are considered “further
  restrictions” within the meaning of section 10. If the Program as you
  received it, or any part of it, contains a notice stating that it is
  governed by this License along with a term that is a further restriction,
  you may remove that term. If a license document contains a further
  restriction but permits relicensing or conveying under this License, you
  may add to a covered work material governed by the terms of that license
  document, provided that the further restriction does not survive such
  relicensing or conveying.

  If you add terms to a covered work in accord with this section, you must
  place, in the relevant source files, a statement of the additional terms
  that apply to those files, or a notice indicating where to find the
  applicable terms.  Additional terms, permissive or non-permissive, may be
  stated in the form of a separately written license, or stated as
  exceptions; the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
  provided under this License. Any attempt otherwise to propagate or modify
  it is void, and will automatically terminate your rights under this
  License (including any patent licenses granted under the third paragraph
  of section 11).

  However, if you cease all violation of this License, then your license
  from a particular copyright holder is reinstated (a) provisionally,
  unless and until the copyright holder explicitly and finally terminates
  your license, and (b) permanently, if the copyright holder fails to
  notify you of the violation by some reasonable means prior to 60 days
  after the cessation.

  Moreover, your license from a particular copyright holder is reinstated
  permanently if the copyright holder notifies you of the violation by some
  reasonable means, this is the first time you have received notice of
  violation of this License (for any work) from that copyright holder, and
  you cure the violation prior to 30 days after your receipt of the notice.

  Termination of your rights under this section does not terminate the
  licenses of parties who have received copies or rights from you under
  this License. If your rights have been terminated and not permanently
  reinstated, you do not qualify to receive new licenses for the same
  material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or run a
  copy of the Program. Ancillary propagation of a covered work occurring
  solely as a consequence of using peer-to-peer transmission to receive a
  copy likewise does not require acceptance. However, nothing other than
  this License grants you permission to propagate or modify any covered
  work. These actions infringe copyright if you do not accept this License.
  Therefore, by modifying or propagating a covered work, you indicate your
  acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically receives
  a license from the original licensors, to run, modify and propagate that
  work, subject to this License. You are not responsible for enforcing
  compliance by third parties with this License.

  An “entity transaction” is a transaction transferring control of an
  organization, or substantially all assets of one, or subdividing an
  organization, or merging organizations. If propagation of a covered work
  results from an entity transaction, each party to that transaction who
  receives a copy of the work also receives whatever licenses to the work
  the party's predecessor in interest had or could give under the previous
  paragraph, plus a right to possession of the Corresponding Source of the
  work from the predecessor in interest, if the predecessor has it or can
  get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the rights
  granted or affirmed under this License. For example, you may not impose a
  license fee, royalty, or other charge for exercise of rights granted
  under this License, and you may not initiate litigation (including a
  cross-claim or counterclaim in a lawsuit) alleging that any patent claim
  is infringed by making, using, selling, offering for sale, or importing
  the Program or any portion of it.

  11. Patents.

  A “contributor” is a copyright holder who authorizes use under this
  License of the Program or a work on which the Program is based. The work
  thus licensed is called the contributor's “contributor version”.

  A contributor's “essential patent claims” are all patent claims owned or
  controlled by the contributor, whether already acquired or hereafter
  acquired, that would be infringed by some manner, permitted by this
  License, of making, using, or selling its contributor version, but do not
  include claims that would be infringed only as a consequence of further
  modification of the contributor version. For purposes of this definition,
  “control” includes the right to grant patent sublicenses in a manner
  consistent with the requirements of this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
  patent license under the contributor's essential patent claims, to make,
  use, sell, offer for sale, import and otherwise run, modify and propagate
  the contents of its contributor version.

  In the following three paragraphs, a “patent license” is any express
  agreement or commitment, however denominated, not to enforce a patent
  (such as an express permission to practice a patent or covenant not to
  sue for patent infringement). To “grant” such a patent license to a party
  means to make such an agreement or commitment not to enforce a patent
  against the party.

  If you convey a covered work, knowingly relying on a patent license, and
  the Corresponding Source of the work is not available for anyone to copy,
  free of charge and under the terms of this License, through a publicly
  available network server or other readily accessible means, then you must
  either (1) cause the Corresponding Source to be so available, or (2)
  arrange to deprive yourself of the benefit of the patent license for this
  particular work, or (3) arrange, in a manner consistent with the
  requirements of this License, to extend the patent license to downstream
  recipients. “Knowingly relying” means you have actual knowledge that, but
  for the patent license, your conveying the covered work in a country, or
  your recipient's use of the covered work in a country, would infringe
  one or more identifiable patents in that country that you have reason
  to believe are valid.

  If, pursuant to or in connection with a single transaction or
  arrangement, you convey, or propagate by procuring conveyance of, a
  covered work, and grant a patent license to some of the parties receiving
  the covered work authorizing them to use, propagate, modify or convey a
  specific copy of the covered work, then the patent license you grant is
  automatically extended to all recipients of the covered work and works
  based on it.

  A patent license is “discriminatory” if it does not include within the
  scope of its coverage, prohibits the exercise of, or is conditioned on
  the non-exercise of one or more of the rights that are specifically
  granted under this License. You may not convey a covered work if you are
  a party to an arrangement with a third party that is in the business of
  distributing software, under which you make payment to the third party
  based on the extent of your activity of conveying the work, and under
  which the third party grants, to any of the parties who would receive the
  covered work from you, a discriminatory patent license (a) in connection
  with copies of the covered work conveyed by you (or copies made from
  those copies), or (b) primarily for and in connection with specific
  products or compilations that contain the covered work, unless you
  entered into that arrangement, or that patent license was granted, prior
  to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting any
  implied license or other defenses to infringement that may otherwise be
  available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
  otherwise) that contradict the conditions of this License, they do not
  excuse you from the conditions of this License. If you cannot use,
  propagate or convey a covered work so as to satisfy simultaneously your
  obligations under this License and any other pertinent obligations, then
  as a consequence you may not use, propagate or convey it at all. For
  example, if you agree to terms that obligate you to collect a royalty for
  further conveying from those to whom you convey the Program, the only way
  you could satisfy both those terms and this License would be to refrain
  entirely from conveying the Program.

  13. Offering the Program as a Service.

  If you make the functionality of the Program or a modified version
  available to third parties as a service, you must make the Service Source
  Code available via network download to everyone at no charge, under the
  terms of this License. Making the functionality of the Program or
  modified version available to third parties as a service includes,
  without limitation, enabling third parties to interact with the
  functionality of the Program or modified version remotely through a
  computer network, offering a service the value of which entirely or
  primarily derives from the value of the Program or modified version, or
  offering a service that accomplishes for users the primary purpose of the
  Program or modified version.

  “Service Source Code” means the Corresponding Source for the Program or
  the modified version, and the Corresponding Source for all programs that
  you use to make the Program or modified version available as a service,
  including, without limitation, management software, user interfaces,
  application program interfaces, automation software, monitoring software,
  backup software, storage software and hosting software, all such that a
  user could run an instance of the service using the Service Source Code
  you make available.  

  14. Revised Versions of this License.

  MongoDB, Inc. may publish revised and/or new versions of the Server Side
  Public License from time to time. Such new versions will be similar in
  spirit to the present version, but may differ in detail to address new
  problems or concerns.

  Each version is given a distinguishing version number. If the Program
  specifies that a certain numbered version of the Server Side Public
  License “or any later version” applies to it, you have the option of
  following the terms and conditions either of that numbered version or of
  any later version published by MongoDB, Inc. If the Program does not
  specify a version number of the Server Side Public License, you may
  choose any version ever published by MongoDB, Inc.

  If the Program specifies that a proxy can decide which future versions of
  the Server Side Public License can be used, that proxy's public statement
  of acceptance of a version permanently authorizes you to choose that
  version for the Program.

  Later license versions may give you additional or different permissions.
  However, no additional obligations are imposed on any author or copyright
  holder as a result of your choosing to follow a later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
  APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
  HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY
  OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
  IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
  ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
  
  16. Limitation of Liability.
  
  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
  WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
  THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING
  ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF
  THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO
  LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU
  OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
  PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
  POSSIBILITY OF SUCH DAMAGES.
  
  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided above
  cannot be given local legal effect according to their terms, reviewing
  courts shall apply local law that most closely approximates an absolute
  waiver of all civil liability in connection with the Program, unless a
  warranty or assumption of liability accompanies a copy of the Program in
  return for a fee.
  
                        END OF TERMS AND CONDITIONS
</file>

<file path="fileserver/redis_manager.py">
log = config.logger(__file__)
⋮----
OVERRIDE_HOST_ENV_KEY = (
OVERRIDE_PORT_ENV_KEY = (
OVERRIDE_PASSWORD_ENV_KEY = (
⋮----
OVERRIDE_HOST = first(filter(None, map(getenv, OVERRIDE_HOST_ENV_KEY)))
⋮----
OVERRIDE_PORT = first(filter(None, map(getenv, OVERRIDE_PORT_ENV_KEY)))
⋮----
OVERRIDE_PASSWORD = first(filter(None, map(getenv, OVERRIDE_PASSWORD_ENV_KEY)))
⋮----
class ConfigError(Exception)
⋮----
class GeneralError(Exception)
⋮----
class RedisManager(object)
⋮----
def __init__(self, redis_config_dict)
⋮----
alias_config = alias_config.as_plain_ordered_dict()
⋮----
is_cluster = alias_config.get("cluster", False)
⋮----
host = OVERRIDE_HOST or alias_config.get("host", None)
⋮----
port = OVERRIDE_PORT or alias_config.get("port", None)
⋮----
password = OVERRIDE_PASSWORD or alias_config.get("password", None)
⋮----
def connection(self, alias) -> StrictRedis
⋮----
obj = self.aliases.get(alias)
⋮----
def host(self, alias)
⋮----
r = self.connection(alias)
⋮----
connections = r.get_default_node().redis_connection.connection_pool._available_connections
⋮----
connections = r.connection_pool._available_connections
⋮----
redman = RedisManager(config.get("hosts.redis"))
</file>

<file path="fileserver/utils.py">
def get_env_bool(*keys: str, default: bool = None) -> Optional[bool]
⋮----
value = next(env for env in (getenv(key) for key in keys) if env is not None)
</file>

<file path="LICENSE">
Server Side Public License
                     VERSION 1, OCTOBER 16, 2018

                    Copyright © 2025 ClearML Inc.

  Everyone is permitted to copy and distribute verbatim copies of this
  license document, but changing it is not allowed.

                       TERMS AND CONDITIONS

  0. Definitions.
  
  “This License” refers to Server Side Public License.

  “Copyright” also means copyright-like laws that apply to other kinds of
  works, such as semiconductor masks.

  “The Program” refers to any copyrightable work licensed under this
  License.  Each licensee is addressed as “you”. “Licensees” and
  “recipients” may be individuals or organizations.

  To “modify” a work means to copy from or adapt all or part of the work in
  a fashion requiring copyright permission, other than the making of an
  exact copy. The resulting work is called a “modified version” of the
  earlier work or a work “based on” the earlier work.

  A “covered work” means either the unmodified Program or a work based on
  the Program.

  To “propagate” a work means to do anything with it that, without
  permission, would make you directly or secondarily liable for
  infringement under applicable copyright law, except executing it on a
  computer or modifying a private copy. Propagation includes copying,
  distribution (with or without modification), making available to the
  public, and in some countries other activities as well.

  To “convey” a work means any kind of propagation that enables other
  parties to make or receive copies. Mere interaction with a user through a
  computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays “Appropriate Legal Notices” to the
  extent that it includes a convenient and prominently visible feature that
  (1) displays an appropriate copyright notice, and (2) tells the user that
  there is no warranty for the work (except to the extent that warranties
  are provided), that licensees may convey the work under this License, and
  how to view a copy of this License. If the interface presents a list of
  user commands or options, such as a menu, a prominent item in the list
  meets this criterion.

  1. Source Code.

  The “source code” for a work means the preferred form of the work for
  making modifications to it. “Object code” means any non-source form of a
  work.

  A “Standard Interface” means an interface that either is an official
  standard defined by a recognized standards body, or, in the case of
  interfaces specified for a particular programming language, one that is
  widely used among developers working in that language.  The “System
  Libraries” of an executable work include anything, other than the work as
  a whole, that (a) is included in the normal form of packaging a Major
  Component, but which is not part of that Major Component, and (b) serves
  only to enable use of the work with that Major Component, or to implement
  a Standard Interface for which an implementation is available to the
  public in source code form. A “Major Component”, in this context, means a
  major essential component (kernel, window system, and so on) of the
  specific operating system (if any) on which the executable work runs, or
  a compiler used to produce the work, or an object code interpreter used
  to run it.

  The “Corresponding Source” for a work in object code form means all the
  source code needed to generate, install, and (for an executable work) run
  the object code and to modify the work, including scripts to control
  those activities. However, it does not include the work's System
  Libraries, or general-purpose tools or generally available free programs
  which are used unmodified in performing those activities but which are
  not part of the work. For example, Corresponding Source includes
  interface definition files associated with source files for the work, and
  the source code for shared libraries and dynamically linked subprograms
  that the work is specifically designed to require, such as by intimate
  data communication or control flow between those subprograms and other
  parts of the work.

  The Corresponding Source need not include anything that users can
  regenerate automatically from other parts of the Corresponding Source.

  The Corresponding Source for a work in source code form is that same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
  copyright on the Program, and are irrevocable provided the stated
  conditions are met. This License explicitly affirms your unlimited
  permission to run the unmodified Program, subject to section 13. The
  output from running a covered work is covered by this License only if the
  output, given its content, constitutes a covered work. This License
  acknowledges your rights of fair use or other equivalent, as provided by
  copyright law.  Subject to section 13, you may make, run and propagate
  covered works that you do not convey, without conditions so long as your
  license otherwise remains in force. You may convey covered works to
  others for the sole purpose of having them make modifications exclusively
  for you, or provide you with facilities for running those works, provided
  that you comply with the terms of this License in conveying all
  material for which you do not control copyright. Those thus making or
  running the covered works for you must do so exclusively on your
  behalf, under your direction and control, on terms that prohibit them
  from making any copies of your copyrighted material outside their
  relationship with you.

  Conveying under any other circumstances is permitted solely under the
  conditions stated below. Sublicensing is not allowed; section 10 makes it
  unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
  measure under any applicable law fulfilling obligations under article 11
  of the WIPO copyright treaty adopted on 20 December 1996, or similar laws
  prohibiting or restricting circumvention of such measures.

  When you convey a covered work, you waive any legal power to forbid
  circumvention of technological measures to the extent such circumvention is
  effected by exercising rights under this License with respect to the
  covered work, and you disclaim any intention to limit operation or
  modification of the work as a means of enforcing, against the work's users,
  your or third parties' legal rights to forbid circumvention of
  technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
  receive it, in any medium, provided that you conspicuously and
  appropriately publish on each copy an appropriate copyright notice; keep
  intact all notices stating that this License and any non-permissive terms
  added in accord with section 7 apply to the code; keep intact all notices
  of the absence of any warranty; and give all recipients a copy of this
  License along with the Program.  You may charge any price or no price for
  each copy that you convey, and you may offer support or warranty
  protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
  produce it from the Program, in the form of source code under the terms
  of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified it,
    and giving a relevant date.

    b) The work must carry prominent notices stating that it is released
    under this License and any conditions added under section 7. This
    requirement modifies the requirement in section 4 to “keep intact all
    notices”.

    c) You must license the entire work, as a whole, under this License to
    anyone who comes into possession of a copy. This License will therefore
    apply, along with any applicable section 7 additional terms, to the
    whole of the work, and all its parts, regardless of how they are
    packaged. This License gives no permission to license the work in any
    other way, but it does not invalidate such permission if you have
    separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your work
    need not make them do so.

  A compilation of a covered work with other separate and independent
  works, which are not by their nature extensions of the covered work, and
  which are not combined with it such as to form a larger program, in or on
  a volume of a storage or distribution medium, is called an “aggregate” if
  the compilation and its resulting copyright are not used to limit the
  access or legal rights of the compilation's users beyond what the
  individual works permit. Inclusion of a covered work in an aggregate does
  not cause this License to apply to the other parts of the aggregate.
  
  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms of
  sections 4 and 5, provided that you also convey the machine-readable
  Corresponding Source under the terms of this License, in one of these
  ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium customarily
    used for software interchange.
   
    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a written
    offer, valid for at least three years and valid for as long as you
    offer spare parts or customer support for that product model, to give
    anyone who possesses the object code either (1) a copy of the
    Corresponding Source for all the software in the product that is
    covered by this License, on a durable physical medium customarily used
    for software interchange, for a price no more than your reasonable cost
    of physically performing this conveying of source, or (2) access to
    copy the Corresponding Source from a network server at no charge.
   
    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source. This alternative is
    allowed only occasionally and noncommercially, and only if you received
    the object code with such an offer, in accord with subsection 6b.
   
    d) Convey the object code by offering access from a designated place
    (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge. You need not require recipients to copy the
    Corresponding Source along with the object code. If the place to copy
    the object code is a network server, the Corresponding Source may be on
    a different server (operated by you or a third party) that supports
    equivalent copying facilities, provided you maintain clear directions
    next to the object code saying where to find the Corresponding Source.
    Regardless of what server hosts the Corresponding Source, you remain
    obligated to ensure that it is available for as long as needed to
    satisfy these requirements.
   
    e) Convey the object code using peer-to-peer transmission, provided you
    inform other peers where the object code and Corresponding Source of
    the work are being offered to the general public at no charge under
    subsection 6d.

  A separable portion of the object code, whose source code is excluded
  from the Corresponding Source as a System Library, need not be included
  in conveying the object code work.

  A “User Product” is either (1) a “consumer product”, which means any
  tangible personal property which is normally used for personal, family,
  or household purposes, or (2) anything designed or sold for incorporation
  into a dwelling. In determining whether a product is a consumer product,
  doubtful cases shall be resolved in favor of coverage. For a particular
  product received by a particular user, “normally used” refers to a
  typical or common use of that class of product, regardless of the status
  of the particular user or of the way in which the particular user
  actually uses, or expects or is expected to use, the product. A product
  is a consumer product regardless of whether the product has substantial
  commercial, industrial or non-consumer uses, unless such uses represent
  the only significant mode of use of the product.

  “Installation Information” for a User Product means any methods,
  procedures, authorization keys, or other information required to install
  and execute modified versions of a covered work in that User Product from
  a modified version of its Corresponding Source. The information must
  suffice to ensure that the continued functioning of the modified object
  code is in no case prevented or interfered with solely because
  modification has been made.

  If you convey an object code work under this section in, or with, or
  specifically for use in, a User Product, and the conveying occurs as part
  of a transaction in which the right of possession and use of the User
  Product is transferred to the recipient in perpetuity or for a fixed term
  (regardless of how the transaction is characterized), the Corresponding
  Source conveyed under this section must be accompanied by the
  Installation Information. But this requirement does not apply if neither
  you nor any third party retains the ability to install modified object
  code on the User Product (for example, the work has been installed in
  ROM).

  The requirement to provide Installation Information does not include a
  requirement to continue to provide support service, warranty, or updates
  for a work that has been modified or installed by the recipient, or for
  the User Product in which it has been modified or installed. Access
  to a network may be denied when the modification itself materially
  and adversely affects the operation of the network or violates the
  rules and protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided, in
  accord with this section must be in a format that is publicly documented
  (and with an implementation available to the public in source code form),
  and must require no special password or key for unpacking, reading or
  copying.

  7. Additional Terms.

  “Additional permissions” are terms that supplement the terms of this
  License by making exceptions from one or more of its conditions.
  Additional permissions that are applicable to the entire Program shall be
  treated as though they were included in this License, to the extent that
  they are valid under applicable law. If additional permissions apply only
  to part of the Program, that part may be used separately under those
  permissions, but the entire Program remains governed by this License
  without regard to the additional permissions.  When you convey a copy of
  a covered work, you may at your option remove any additional permissions
  from that copy, or from any part of it. (Additional permissions may be
  written to require their own removal in certain cases when you modify the
  work.) You may place additional permissions on material, added by you to
  a covered work, for which you have or can give appropriate copyright
  permission.

  Notwithstanding any other provision of this License, for material you add
  to a covered work, you may (if authorized by the copyright holders of
  that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some trade
    names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that material
    by anyone who conveys the material (or modified versions of it) with
    contractual assumptions of liability to the recipient, for any
    liability that these contractual assumptions directly impose on those
    licensors and authors.

  All other non-permissive additional terms are considered “further
  restrictions” within the meaning of section 10. If the Program as you
  received it, or any part of it, contains a notice stating that it is
  governed by this License along with a term that is a further restriction,
  you may remove that term. If a license document contains a further
  restriction but permits relicensing or conveying under this License, you
  may add to a covered work material governed by the terms of that license
  document, provided that the further restriction does not survive such
  relicensing or conveying.

  If you add terms to a covered work in accord with this section, you must
  place, in the relevant source files, a statement of the additional terms
  that apply to those files, or a notice indicating where to find the
  applicable terms.  Additional terms, permissive or non-permissive, may be
  stated in the form of a separately written license, or stated as
  exceptions; the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
  provided under this License. Any attempt otherwise to propagate or modify
  it is void, and will automatically terminate your rights under this
  License (including any patent licenses granted under the third paragraph
  of section 11).

  However, if you cease all violation of this License, then your license
  from a particular copyright holder is reinstated (a) provisionally,
  unless and until the copyright holder explicitly and finally terminates
  your license, and (b) permanently, if the copyright holder fails to
  notify you of the violation by some reasonable means prior to 60 days
  after the cessation.

  Moreover, your license from a particular copyright holder is reinstated
  permanently if the copyright holder notifies you of the violation by some
  reasonable means, this is the first time you have received notice of
  violation of this License (for any work) from that copyright holder, and
  you cure the violation prior to 30 days after your receipt of the notice.

  Termination of your rights under this section does not terminate the
  licenses of parties who have received copies or rights from you under
  this License. If your rights have been terminated and not permanently
  reinstated, you do not qualify to receive new licenses for the same
  material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or run a
  copy of the Program. Ancillary propagation of a covered work occurring
  solely as a consequence of using peer-to-peer transmission to receive a
  copy likewise does not require acceptance. However, nothing other than
  this License grants you permission to propagate or modify any covered
  work. These actions infringe copyright if you do not accept this License.
  Therefore, by modifying or propagating a covered work, you indicate your
  acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically receives
  a license from the original licensors, to run, modify and propagate that
  work, subject to this License. You are not responsible for enforcing
  compliance by third parties with this License.

  An “entity transaction” is a transaction transferring control of an
  organization, or substantially all assets of one, or subdividing an
  organization, or merging organizations. If propagation of a covered work
  results from an entity transaction, each party to that transaction who
  receives a copy of the work also receives whatever licenses to the work
  the party's predecessor in interest had or could give under the previous
  paragraph, plus a right to possession of the Corresponding Source of the
  work from the predecessor in interest, if the predecessor has it or can
  get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the rights
  granted or affirmed under this License. For example, you may not impose a
  license fee, royalty, or other charge for exercise of rights granted
  under this License, and you may not initiate litigation (including a
  cross-claim or counterclaim in a lawsuit) alleging that any patent claim
  is infringed by making, using, selling, offering for sale, or importing
  the Program or any portion of it.

  11. Patents.

  A “contributor” is a copyright holder who authorizes use under this
  License of the Program or a work on which the Program is based. The work
  thus licensed is called the contributor's “contributor version”.

  A contributor's “essential patent claims” are all patent claims owned or
  controlled by the contributor, whether already acquired or hereafter
  acquired, that would be infringed by some manner, permitted by this
  License, of making, using, or selling its contributor version, but do not
  include claims that would be infringed only as a consequence of further
  modification of the contributor version. For purposes of this definition,
  “control” includes the right to grant patent sublicenses in a manner
  consistent with the requirements of this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
  patent license under the contributor's essential patent claims, to make,
  use, sell, offer for sale, import and otherwise run, modify and propagate
  the contents of its contributor version.

  In the following three paragraphs, a “patent license” is any express
  agreement or commitment, however denominated, not to enforce a patent
  (such as an express permission to practice a patent or covenant not to
  sue for patent infringement). To “grant” such a patent license to a party
  means to make such an agreement or commitment not to enforce a patent
  against the party.

  If you convey a covered work, knowingly relying on a patent license, and
  the Corresponding Source of the work is not available for anyone to copy,
  free of charge and under the terms of this License, through a publicly
  available network server or other readily accessible means, then you must
  either (1) cause the Corresponding Source to be so available, or (2)
  arrange to deprive yourself of the benefit of the patent license for this
  particular work, or (3) arrange, in a manner consistent with the
  requirements of this License, to extend the patent license to downstream
  recipients. “Knowingly relying” means you have actual knowledge that, but
  for the patent license, your conveying the covered work in a country, or
  your recipient's use of the covered work in a country, would infringe
  one or more identifiable patents in that country that you have reason
  to believe are valid.

  If, pursuant to or in connection with a single transaction or
  arrangement, you convey, or propagate by procuring conveyance of, a
  covered work, and grant a patent license to some of the parties receiving
  the covered work authorizing them to use, propagate, modify or convey a
  specific copy of the covered work, then the patent license you grant is
  automatically extended to all recipients of the covered work and works
  based on it.

  A patent license is “discriminatory” if it does not include within the
  scope of its coverage, prohibits the exercise of, or is conditioned on
  the non-exercise of one or more of the rights that are specifically
  granted under this License. You may not convey a covered work if you are
  a party to an arrangement with a third party that is in the business of
  distributing software, under which you make payment to the third party
  based on the extent of your activity of conveying the work, and under
  which the third party grants, to any of the parties who would receive the
  covered work from you, a discriminatory patent license (a) in connection
  with copies of the covered work conveyed by you (or copies made from
  those copies), or (b) primarily for and in connection with specific
  products or compilations that contain the covered work, unless you
  entered into that arrangement, or that patent license was granted, prior
  to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting any
  implied license or other defenses to infringement that may otherwise be
  available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
  otherwise) that contradict the conditions of this License, they do not
  excuse you from the conditions of this License. If you cannot use,
  propagate or convey a covered work so as to satisfy simultaneously your
  obligations under this License and any other pertinent obligations, then
  as a consequence you may not use, propagate or convey it at all. For
  example, if you agree to terms that obligate you to collect a royalty for
  further conveying from those to whom you convey the Program, the only way
  you could satisfy both those terms and this License would be to refrain
  entirely from conveying the Program.

  13. Offering the Program as a Service.

  If you make the functionality of the Program or a modified version
  available to third parties as a service, you must make the Service Source
  Code available via network download to everyone at no charge, under the
  terms of this License. Making the functionality of the Program or
  modified version available to third parties as a service includes,
  without limitation, enabling third parties to interact with the
  functionality of the Program or modified version remotely through a
  computer network, offering a service the value of which entirely or
  primarily derives from the value of the Program or modified version, or
  offering a service that accomplishes for users the primary purpose of the
  Program or modified version.

  “Service Source Code” means the Corresponding Source for the Program or
  the modified version, and the Corresponding Source for all programs that
  you use to make the Program or modified version available as a service,
  including, without limitation, management software, user interfaces,
  application program interfaces, automation software, monitoring software,
  backup software, storage software and hosting software, all such that a
  user could run an instance of the service using the Service Source Code
  you make available.  

  14. Revised Versions of this License.

  MongoDB, Inc. may publish revised and/or new versions of the Server Side
  Public License from time to time. Such new versions will be similar in
  spirit to the present version, but may differ in detail to address new
  problems or concerns.

  Each version is given a distinguishing version number. If the Program
  specifies that a certain numbered version of the Server Side Public
  License “or any later version” applies to it, you have the option of
  following the terms and conditions either of that numbered version or of
  any later version published by MongoDB, Inc. If the Program does not
  specify a version number of the Server Side Public License, you may
  choose any version ever published by MongoDB, Inc.

  If the Program specifies that a proxy can decide which future versions of
  the Server Side Public License can be used, that proxy's public statement
  of acceptance of a version permanently authorizes you to choose that
  version for the Program.

  Later license versions may give you additional or different permissions.
  However, no additional obligations are imposed on any author or copyright
  holder as a result of your choosing to follow a later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
  APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
  HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY
  OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
  IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
  ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
  
  16. Limitation of Liability.
  
  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
  WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
  THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING
  ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF
  THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO
  LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU
  OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
  PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
  POSSIBILITY OF SUCH DAMAGES.
  
  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided above
  cannot be given local legal effect according to their terms, reviewing
  courts shall apply local law that most closely approximates an absolute
  waiver of all civil liability in connection with the Program, unless a
  warranty or assumption of liability accompanies a copy of the Program in
  return for a fee.
  
                        END OF TERMS AND CONDITIONS
</file>

<file path="README.md">
<div align="center">

<img src="docs/clearml_server_logo.png" width="250px">

**ClearML - Auto-Magical Suite of tools to streamline your ML workflow 
</br>Experiment Manager, ML-Ops and Data-Management**

[![GitHub license](https://img.shields.io/badge/license-SSPL-green.svg)](https://img.shields.io/badge/license-SSPL-green.svg)
[![Python versions](https://img.shields.io/badge/python-3.11-blue.svg)](https://img.shields.io/badge/python-3.11-blue.svg)
[![GitHub version](https://img.shields.io/github/release-pre/clearml/clearml-server.svg)](https://img.shields.io/github/release-pre/clearml/clearml-server.svg)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/clearml)](https://artifacthub.io/packages/search?repo=clearml)

</div>

## ClearML Server

The **ClearML Server** is the backend service infrastructure for [ClearML](https://github.com/clearml/clearml).
It allows multiple users to collaborate and manage their experiments.
**ClearML** offers a [free hosted service](https://app.clear.ml/), which is maintained by **ClearML** and open to anyone.
In order to host your own server, you will need to launch the **ClearML Server** and point **ClearML** to it.

The **ClearML Server** contains the following components:

* The **ClearML** Web-App, a single-page UI for experiment management and browsing
* RESTful API for:
    * Documenting and logging experiment information, statistics and results
    * Querying experiments history, logs and results
* Locally-hosted file server for storing images and models making them easily accessible using the Web-App

You can quickly [deploy](#launching-the-clearml-server)  your **ClearML Server** using Docker, AWS EC2 AMI, or Kubernetes. 

## System design


![Alt Text](docs/ClearML_Server_Diagram.png)

The **ClearML Server** has two supported configurations:
- Single IP (domain) with the following open ports
    - Web application on port 8080
    - API service on port 8008
    - File storage service on port 8081

- Sub-Domain configuration with default http/s ports (80 or 443)
    - Web application on sub-domain: app.\*.\*
    - API service on sub-domain: api.\*.\*
    - File storage service on sub-domain: files.\*.\*
    
## Launching The ClearML Server

### Prerequisites

The ports 8080/8081/8008 must be available for the **ClearML Server** services.
   
For example, to see if port `8080` is in use:

* Linux or macOS: 
   
        sudo lsof -Pn -i4 | grep :8080 | grep LISTEN

* Windows:

        netstat -an |find /i "8080"
   
### Launching   
    
Launch The **ClearML Server** in any of the following formats:

- Pre-built [AWS EC2 AMI](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_aws_ec2_ami)
- Pre-built [GCP Custom Image](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_gcp)
- Pre-built Docker Image
    - [Linux](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_linux_mac)
    - [macOS](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_linux_mac)
    - [Windows 10](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_win)
- Kubernetes    
    - [Kubernetes Helm](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_kubernetes_helm)
    - Manual [Kubernetes installation](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_kubernetes)

## Connecting ClearML to your ClearML Server

In order to set up the **ClearML** client to work with your **ClearML Server**:
- Run the `clearml-init` command for an interactive setup.
- Or manually edit `~/clearml.conf` file, making sure the server settings (`api_server`, `web_server`, `file_server`) are configured correctly, for example:

        api {
            # API server on port 8008
            api_server: "http://localhost:8008"

            # web_server on port 8080
            web_server: "http://localhost:8080"

            # file server on port 8081
            files_server: "http://localhost:8081"
        }

**Note**: If you have set up your **ClearML Server** in a sub-domain configuration, then there is no need to specify a port number,
it will be inferred from the http/s scheme.

After launching the **ClearML Server** and configuring the **ClearML** client to use the **ClearML Server**,
you can [use](https://github.com/clearml/clearml) **ClearML** in your experiments and view them in your **ClearML Server** web server,
for example http://localhost:8080.  
For more information about the ClearML client, see [**ClearML**](https://github.com/clearml/clearml).

## ClearML-Agent Services  <a name="services"></a> 

As of version 0.15 of **ClearML Server**, dockerized deployment includes a **ClearML-Agent Services** container running as 
part of the docker container collection.

ClearML-Agent Services is an extension of ClearML-Agent that provides the ability to launch long-lasting jobs 
that previously had to be executed on local / dedicated machines. It allows a single agent to 
launch multiple dockers (Tasks) for different use cases. To name a few use cases, auto-scaler service (spinning instances 
when the need arises and the budget allows), Controllers (Implementing pipelines and more sophisticated DevOps logic),
Optimizer (such as Hyper-parameter Optimization or sweeping), and Application (such as interactive Bokeh apps for 
increased data transparency)

ClearML-Agent Services container will spin **any** task enqueued into the dedicated `services` queue. 
Every task launched by ClearML-Agent Services  will be registered as a new node in the system, 
providing tracking and transparency capabilities.  
You can also run the ClearML-Agent Services manually, see details in [ClearML-agent services mode](https://github.com/clearml/clearml-agent#clearml-agent-services-mode-)

**Note**: It is the user's responsibility to make sure the proper tasks are pushed into the `services` queue. 
Do not enqueue training / inference tasks into the `services` queue, as it will put unnecessary load on the server.

## Advanced Functionality

The **ClearML Server** provides a few additional useful features, which can be manually enabled:
 
* [Web login authentication](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_config#web-login-authentication)
* [Non-responsive experiments watchdog](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_config#non-responsive-task-watchdog)  

## Restarting ClearML Server

To restart the **ClearML Server**, you must first stop the containers, and then restart them.

   ```bash
   docker-compose down
   docker-compose -f docker-compose.yml up
   ```

## Upgrading <a name="upgrade"></a>

**ClearML Server** releases are also reflected in the [docker compose configuration file](https://github.com/clearml/clearml-server/blob/master/docker/docker-compose.yml).  
We strongly encourage you to keep your **ClearML Server** up to date, by keeping up with the current release.

**Note**: The following upgrade instructions use the Linux OS as an example.

To upgrade your existing **ClearML Server** deployment:

1. Shut down the docker containers
   ```bash
   docker-compose down
   ```

1. We highly recommend backing up your data directory before upgrading.

   Assuming your data directory is `/opt/clearml`, to archive all data into `~/clearml_backup.tgz` execute:

   ```bash
   sudo tar czvf ~/clearml_backup.tgz /opt/clearml/data
   ```    

   <details>
   <summary>Restore instructions:</summary>

   To restore this example backup, execute:
   ```bash
   sudo rm -R /opt/clearml/data
   sudo tar -xzf ~/clearml_backup.tgz -C /opt/clearml/data
   ```
   </details>

1. Download the latest `docker-compose.yml` file.

   ```bash
   curl https://raw.githubusercontent.com/clearml/clearml-server/master/docker/docker-compose.yml -o docker-compose.yml 
   ```

1. Configure the ClearML-Agent Services (not supported on Windows installation). 
   If `CLEARML_HOST_IP` is not provided, ClearML-Agent Services will use the external 
   public address of the **ClearML Server**. If `CLEARML_AGENT_GIT_USER` / `CLEARML_AGENT_GIT_PASS` are not provided, 
   the ClearML-Agent Services will not be able to access any private repositories for running service tasks.
   
   ```bash
   export CLEARML_HOST_IP=server_host_ip_here
   export CLEARML_AGENT_GIT_USER=git_username_here
   export CLEARML_AGENT_GIT_PASS=git_password_here
   ```

1. Spin up the docker containers, it will automatically pull the latest **ClearML Server** build    
   ```bash
   docker-compose -f docker-compose.yml pull
   docker-compose -f docker-compose.yml up
   ```

**\* If something went wrong along the way, check our FAQ: [Common Docker Upgrade Errors](https://clear.ml/docs/latest/docs/faq/).**


## Community & Support

If you have any questions, look to the ClearML [FAQ](https://clear.ml/docs/latest/docs/faq), or
tag your questions on [stackoverflow](https://stackoverflow.com/questions/tagged/clearml) with '**clearml**' tag.

For feature requests or bug reports, please use [GitHub issues](https://github.com/clearml/clearml-server/issues).

Additionally, you can always find us at *clearml@allegro.ai*

## License

[Server Side Public License v1.0](https://github.com/mongodb/mongo/blob/master/LICENSE-Community.txt)

The **ClearML Server** relies on both [MongoDB](https://github.com/mongodb/mongo) and [ElasticSearch](https://github.com/elastic/elasticsearch).
With the recent changes in both MongoDB's and ElasticSearch's OSS license, we feel it is our responsibility as a
member of the community to support the projects we love and cherish.
We believe the cause for the license change in both cases is more than just,
and chose [SSPL](https://www.mongodb.com/licensing/server-side-public-license) because it is the more general and flexible of the two licenses.

This is our way to say - we support you guys!
</file>

<file path="upgrade/1_17_to_2_0/docker-compose-win10.yml">
version: "3.6"
services:

  apiserver:
    command:
    - apiserver
    container_name: clearml-apiserver
    image: allegroai/clearml:1.17.1-554
    restart: unless-stopped
    volumes:
    - c:/opt/clearml/logs:/var/log/clearml
    - c:/opt/clearml/config:/opt/clearml/config
    - c:/opt/clearml/data/fileserver:/mnt/fileserver
    depends_on:
      - redis
      - mongo
      - elasticsearch
      - fileserver
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      CLEARML_SERVER_DEPLOYMENT_TYPE: win10
      CLEARML__apiserver__pre_populate__enabled: "true"
      CLEARML__apiserver__pre_populate__zip_files: "/opt/clearml/db-pre-populate"
      CLEARML__apiserver__pre_populate__artifacts_path: "/mnt/fileserver"
      CLEARML__services__async_urls_delete__enabled: "true"
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    ports:
    - "8008:8008"
    networks:
      - backend
      - frontend

  elasticsearch:
    networks:
      - backend
    container_name: clearml-elastic
    environment:
      bootstrap.memory_lock: "true"
      cluster.name: clearml
      cluster.routing.allocation.node_initial_primaries_recoveries: "500"
      cluster.routing.allocation.disk.watermark.low: 500mb
      cluster.routing.allocation.disk.watermark.high: 500mb
      cluster.routing.allocation.disk.watermark.flood_stage: 500mb
      discovery.type: "single-node"
      http.compression_level: "7"
      node.name: clearml
      reindex.remote.whitelist: "'*.*'"
      xpack.security.enabled: "false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.3
    restart: unless-stopped
    volumes:
      - c:/opt/clearml/data/elastic_7:/usr/share/elasticsearch/data
      - /usr/share/elasticsearch/logs

  fileserver:
    networks:
      - backend
      - frontend
    command:
    - fileserver
    container_name: clearml-fileserver
    image: allegroai/clearml:1.17.1-554
    environment:
      CLEARML__fileserver__delete__allow_batch: "true"
    restart: unless-stopped
    volumes:
    - c:/opt/clearml/logs:/var/log/clearml
    - c:/opt/clearml/data/fileserver:/mnt/fileserver
    - c:/opt/clearml/config:/opt/clearml/config

    ports:
    - "8081:8081"

  mongo:
    networks:
      - backend
    container_name: clearml-mongo
    image: mongo:5.0.26
    restart: unless-stopped
    command: --setParameter internalQueryMaxBlockingSortMemoryUsageBytes=196100200
    volumes:
    - c:/opt/clearml/data/mongo_4/db:/data/db
    - c:/opt/clearml/data/mongo_4/configdb:/data/configdb

  redis:
    networks:
      - backend
    container_name: clearml-redis
    image: redis:6.2
    restart: unless-stopped
    volumes:
    - c:/opt/clearml/data/redis:/data

  webserver:
    command:
    - webserver
    container_name: clearml-webserver
    image: allegroai/clearml:1.17.1-554
    restart: unless-stopped
    volumes:
    - c:/clearml/logs:/var/log/clearml
    depends_on:
      - apiserver
    ports:
    - "8080:80"
    networks:
      - backend
      - frontend

  async_delete:
    depends_on:
      - apiserver
      - redis
      - mongo
      - elasticsearch
      - fileserver
    container_name: async_delete
    image: allegroai/clearml:1.17.1-554
    networks:
      - backend
    restart: unless-stopped
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      PYTHONPATH: /opt/clearml/apiserver
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    entrypoint:
      - python3
      - -m
      - jobs.async_urls_delete
      - --fileserver-host
      - http://fileserver:8081
    volumes:
    - c:/opt/clearml/logs:/var/log/clearml
    - c:/opt/clearml/config:/opt/clearml/config

networks:
  backend:
    driver: bridge
  frontend:
    name: frontend
    driver: bridge
</file>

<file path="upgrade/1_17_to_2_0/docker-compose.yml">
version: "3.6"
services:

  apiserver:
    command:
    - apiserver
    container_name: clearml-apiserver
    image: allegroai/clearml:1.17.1-554
    restart: unless-stopped
    volumes:
    - /opt/clearml/logs:/var/log/clearml
    - /opt/clearml/config:/opt/clearml/config
    - /opt/clearml/data/fileserver:/mnt/fileserver
    depends_on:
      - redis
      - mongo
      - elasticsearch
      - fileserver
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      CLEARML_SERVER_DEPLOYMENT_TYPE: linux
      CLEARML__apiserver__pre_populate__enabled: "true"
      CLEARML__apiserver__pre_populate__zip_files: "/opt/clearml/db-pre-populate"
      CLEARML__apiserver__pre_populate__artifacts_path: "/mnt/fileserver"
      CLEARML__services__async_urls_delete__enabled: "true"
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
      CLEARML__secure__credentials__services_agent__user_key: ${CLEARML_AGENT_ACCESS_KEY:-}
      CLEARML__secure__credentials__services_agent__user_secret: ${CLEARML_AGENT_SECRET_KEY:-}
    ports:
    - "8008:8008"
    networks:
      - backend
      - frontend

  elasticsearch:
    networks:
      - backend
    container_name: clearml-elastic
    environment:
      bootstrap.memory_lock: "true"
      cluster.name: clearml
      cluster.routing.allocation.node_initial_primaries_recoveries: "500"
      cluster.routing.allocation.disk.watermark.low: 500mb
      cluster.routing.allocation.disk.watermark.high: 500mb
      cluster.routing.allocation.disk.watermark.flood_stage: 500mb
      discovery.type: "single-node"
      http.compression_level: "7"
      node.name: clearml
      reindex.remote.whitelist: "'*.*'"
      xpack.security.enabled: "false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.3
    restart: unless-stopped
    volumes:
      - /opt/clearml/data/elastic_7:/usr/share/elasticsearch/data
      - /usr/share/elasticsearch/logs

  fileserver:
    networks:
      - backend
      - frontend
    command:
    - fileserver
    container_name: clearml-fileserver
    image: allegroai/clearml:1.17.1-554
    environment:
      CLEARML__fileserver__delete__allow_batch: "true"
    restart: unless-stopped
    volumes:
    - /opt/clearml/logs:/var/log/clearml
    - /opt/clearml/data/fileserver:/mnt/fileserver
    - /opt/clearml/config:/opt/clearml/config
    ports:
    - "8081:8081"

  mongo:
    networks:
      - backend
    container_name: clearml-mongo
    image: mongo:5.0.26
    restart: unless-stopped
    command: --setParameter internalQueryMaxBlockingSortMemoryUsageBytes=196100200
    volumes:
    - /opt/clearml/data/mongo_4/db:/data/db
    - /opt/clearml/data/mongo_4/configdb:/data/configdb

  redis:
    networks:
      - backend
    container_name: clearml-redis
    image: redis:6.2
    restart: unless-stopped
    volumes:
    - /opt/clearml/data/redis:/data

  webserver:
    command:
    - webserver
    container_name: clearml-webserver
    # environment:
    #  CLEARML_SERVER_SUB_PATH : clearml-web # Allow Clearml to be served with a URL path prefix.
    image: allegroai/clearml:1.17.1-554
    restart: unless-stopped
    depends_on:
      - apiserver
    ports:
    - "8080:80"
    networks:
      - backend
      - frontend

  async_delete:
    depends_on:
      - apiserver
      - redis
      - mongo
      - elasticsearch
      - fileserver
    container_name: async_delete
    image: allegroai/clearml:1.17.1-554
    networks:
      - backend
    restart: unless-stopped
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      PYTHONPATH: /opt/clearml/apiserver
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    entrypoint:
      - python3
      - -m
      - jobs.async_urls_delete
      - --fileserver-host
      - http://fileserver:8081
    volumes:
      - /opt/clearml/logs:/var/log/clearml
      - /opt/clearml/config:/opt/clearml/config

  agent-services:
    networks:
      - backend
    container_name: clearml-agent-services
    image: allegroai/clearml-agent-services:latest
    deploy:
      restart_policy:
        condition: on-failure
    privileged: true
    environment:
      CLEARML_HOST_IP: ${CLEARML_HOST_IP}
      CLEARML_WEB_HOST: ${CLEARML_WEB_HOST:-}
      CLEARML_API_HOST: http://apiserver:8008
      CLEARML_FILES_HOST: ${CLEARML_FILES_HOST:-}
      CLEARML_API_ACCESS_KEY: ${CLEARML_AGENT_ACCESS_KEY:-$CLEARML_API_ACCESS_KEY}
      CLEARML_API_SECRET_KEY: ${CLEARML_AGENT_SECRET_KEY:-$CLEARML_API_SECRET_KEY}
      CLEARML_AGENT_GIT_USER: ${CLEARML_AGENT_GIT_USER}
      CLEARML_AGENT_GIT_PASS: ${CLEARML_AGENT_GIT_PASS}
      CLEARML_AGENT_UPDATE_VERSION: ${CLEARML_AGENT_UPDATE_VERSION:->=0.17.0}
      CLEARML_AGENT_DEFAULT_BASE_DOCKER: "ubuntu:18.04"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-}
      AZURE_STORAGE_ACCOUNT: ${AZURE_STORAGE_ACCOUNT:-}
      AZURE_STORAGE_KEY: ${AZURE_STORAGE_KEY:-}
      GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS:-}
      CLEARML_WORKER_ID: "clearml-services"
      CLEARML_AGENT_DOCKER_HOST_MOUNT: "/opt/clearml/agent:/root/.clearml"
      SHUTDOWN_IF_NO_ACCESS_KEY: 1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/clearml/agent:/root/.clearml
    depends_on:
      - apiserver
    entrypoint: >
      bash -c "curl --retry 10 --retry-delay 10 --retry-connrefused 'http://apiserver:8008/debug.ping' && /usr/agent/entrypoint.sh"

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge
</file>

</files>
